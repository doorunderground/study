{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 과제\n",
    "# 2차방정식 풀이 해보기\n",
    "# 현재까지 만들어진 소스를 가지고\n",
    "# 2차방정식을 풀어본다\n",
    "# 2차 방정식 artificial 샘플을 만들고\n",
    "# 2차 방정식을 위한 model (kernel 생성,  weight 크기 수정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45925509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "a[[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093c83be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da7f3ce590>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 100\n",
    "x = torch.rand(m, 1) * 2\n",
    "y = x * 3 + 4 + torch.randn(m, 1)\n",
    "\n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b7a532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1]), torch.Size([20, 1]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(x, y))\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size = 20, shuffle = True)\n",
    "\n",
    "for xi , yi in loader: # DataLoader에서 첫 번째 배치 하나만 꺼내서 확인\n",
    "    break\n",
    "\n",
    "xi.shape, yi.shape, xi.dtype, yi.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d58bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1627],\n",
       "        [3.8577]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.pinv(torch.cat([x, x**0], axis = 1)) @ y\n",
    "\n",
    "# [[1],\n",
    "# [2],\n",
    "# [3]]\n",
    "# -> \n",
    "# [[1, 1],\n",
    "# [2, 1],\n",
    "# [3, 1]]\n",
    "\n",
    "# shape = (100, 2)\n",
    "# a(기울기) / b(절편)을 구한거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135e7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((1, 5), requires_grad= True) # 이 파라미터는 학습 대상이다.\n",
    "B1 = torch.randn((5,), requires_grad= True)\n",
    "W2 = torch.randn((5, 1), requires_grad= True)\n",
    "B2 = torch.randn((1,), requires_grad= True)\n",
    "\n",
    "# x.shape = (batch_size, 1)\n",
    "# (batch, 1) @ (1, 5)\n",
    "#  = (batch, 5)\n",
    "\n",
    "# (batch, 5) @ (5, 1)\n",
    "#  = (batch, 1)\n",
    "\n",
    "def model(x):\n",
    "    h = torch.relu(x @ W1 + B1)\n",
    "    y = (h @ W2 + B2)\n",
    "    return y\n",
    "\n",
    "# optimizer(최적화)\n",
    "opt = torch.optim.Adam([W1, B1, W2, B2], lr=0.0001)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db6ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(12.8785, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(18.0724, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(16.2836, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(14.0841, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(15.5683, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(17.5049, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(12.2009, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(17.5845, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(17.6413, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(16.1682, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad\n",
    "\n",
    "# 100개 데이터\n",
    "# 20개씩 나눔\n",
    "# 한 epoch당 5번 update 발생\n",
    "\n",
    "# pred = model(bx)\n",
    "# W1, B1, W2, B2로 예측값 계산\n",
    "\n",
    "for epoch in range(10):\n",
    "    for bx, by in loader:\n",
    "        # 현재 파라미터를 이용해서 예측값을 구하고 f(x) = ax+b\n",
    "        pred = model(bx)\n",
    "\n",
    "        # 에러를 구하고\n",
    "        loss = loss_fn(pred, by)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        opt.step()\n",
    "    print(epoch, loss)         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b8063",
   "metadata": {},
   "source": [
    "23 tensor(58.8977, grad_fn=<MseLossBackward0>)\n",
    "\n",
    "24 tensor(62.9800, grad_fn=<MseLossBackward0>)\n",
    "\n",
    "-> 예측과 실제값 차이의 제곱 평균\n",
    "\n",
    "숫자가 클수록 예측이 많이 틀린 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6609cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2576, -0.0177,  0.1657,  2.8885, -0.0877]], requires_grad=True) tensor([-0.8647,  0.8362,  0.7735,  0.2636,  0.6314], requires_grad=True) tensor([[-2.0342],\n",
      "        [ 0.5230],\n",
      "        [ 0.7603],\n",
      "        [ 0.5918],\n",
      "        [-0.8804]], requires_grad=True) tensor([0.7458], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W1, B1, W2, B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24c153d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29802247010>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "\n",
    "plt.plot(x, pred, '.')\n",
    "\n",
    "((pred-y) ** 2).mean()\n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe62cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74974d7c",
   "metadata": {},
   "source": [
    "# 과제\n",
    "1. 2차방정식 풀이 해보기\n",
    "2. 현재까지 만들어진 소스를 가지고\n",
    "3. 2차방정식을 풀어본다\n",
    "4. 2차 방정식 artificial 샘플을 만들고\n",
    "5. 2차 방정식을 위한 model (kernel 생성,  weight 크기 수정)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c405a",
   "metadata": {},
   "source": [
    "y= 2X^2 + 3X + 5 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe51651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da7f618850>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 200\n",
    "x = torch.randn(m, 1) * 2\n",
    "y = 2 * x**2 + 3*x + 5 + torch.randn(m, 1)\n",
    "\n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80db0c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1]), torch.Size([20, 1]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(x, y))\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size= 20 , shuffle=True)\n",
    "\n",
    "for xi, yi in loader:\n",
    "    break\n",
    "xi.shape, yi.shape, xi.dtype, yi.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00049903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.cat([x**2, x, x**0], axis = 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a25d34d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9894],\n",
      "        [2.9914],\n",
      "        [5.2100]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.linalg.pinv(X) @ y\n",
    "\n",
    "print(w)\n",
    "\n",
    "# tensor([[2.0xxx],   ← a ≈ 2\n",
    "#         [3.0xxx],   ← b ≈ 3\n",
    "#         [5.0xxx]])  ← c ≈ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb220252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망\n",
    "\n",
    "W1 = torch.randn((1,32), requires_grad=True) #입력->은닉1\n",
    "B1 = torch.randn((32, ), requires_grad=True)\n",
    "W2 = torch.randn((32,32), requires_grad=True) #은닉1->출력\n",
    "B2 = torch.randn((1, ), requires_grad=True)\n",
    "\n",
    "\n",
    "# (batch,1)  @ (1,32)  = (batch,32)\n",
    "# (batch,32) @ (32,1)  = (batch,1)\n",
    "\n",
    "def model(x):\n",
    "    h = torch.relu(x @ W1 + B1)\n",
    "    y = (h @ W2 + B2)\n",
    "    return y \n",
    "\n",
    "# optimize(최적화)\n",
    "opt = torch.optim.Adam([W1, B1, W2, B2], lr = 0.0001)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e80034fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDT51\\miniconda3\\envs\\moon\\lib\\site-packages\\torch\\nn\\modules\\loss.py:626: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([20, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(159.8633, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(225.7429, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(405.9037, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(265.6236, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(513.5771, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(373.6646, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(493.5350, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(273.0512, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(409.3324, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(507.8710, grad_fn=<MseLossBackward0>)\n",
      "10 tensor(445.2067, grad_fn=<MseLossBackward0>)\n",
      "11 tensor(319.9407, grad_fn=<MseLossBackward0>)\n",
      "12 tensor(378.9164, grad_fn=<MseLossBackward0>)\n",
      "13 tensor(584.9786, grad_fn=<MseLossBackward0>)\n",
      "14 tensor(334.4023, grad_fn=<MseLossBackward0>)\n",
      "15 tensor(275.2917, grad_fn=<MseLossBackward0>)\n",
      "16 tensor(264.3896, grad_fn=<MseLossBackward0>)\n",
      "17 tensor(364.2672, grad_fn=<MseLossBackward0>)\n",
      "18 tensor(323.6830, grad_fn=<MseLossBackward0>)\n",
      "19 tensor(141.7050, grad_fn=<MseLossBackward0>)\n",
      "20 tensor(230.6791, grad_fn=<MseLossBackward0>)\n",
      "21 tensor(361.6403, grad_fn=<MseLossBackward0>)\n",
      "22 tensor(346.0215, grad_fn=<MseLossBackward0>)\n",
      "23 tensor(306.9950, grad_fn=<MseLossBackward0>)\n",
      "24 tensor(504.9699, grad_fn=<MseLossBackward0>)\n",
      "25 tensor(140.5726, grad_fn=<MseLossBackward0>)\n",
      "26 tensor(568.4370, grad_fn=<MseLossBackward0>)\n",
      "27 tensor(487.4422, grad_fn=<MseLossBackward0>)\n",
      "28 tensor(474.4138, grad_fn=<MseLossBackward0>)\n",
      "29 tensor(270.4909, grad_fn=<MseLossBackward0>)\n",
      "30 tensor(326.1838, grad_fn=<MseLossBackward0>)\n",
      "31 tensor(489.3214, grad_fn=<MseLossBackward0>)\n",
      "32 tensor(258.1354, grad_fn=<MseLossBackward0>)\n",
      "33 tensor(135.2311, grad_fn=<MseLossBackward0>)\n",
      "34 tensor(173.4480, grad_fn=<MseLossBackward0>)\n",
      "35 tensor(336.4246, grad_fn=<MseLossBackward0>)\n",
      "36 tensor(144.1569, grad_fn=<MseLossBackward0>)\n",
      "37 tensor(504.0706, grad_fn=<MseLossBackward0>)\n",
      "38 tensor(416.8076, grad_fn=<MseLossBackward0>)\n",
      "39 tensor(118.5533, grad_fn=<MseLossBackward0>)\n",
      "40 tensor(293.4386, grad_fn=<MseLossBackward0>)\n",
      "41 tensor(470.9819, grad_fn=<MseLossBackward0>)\n",
      "42 tensor(400.7259, grad_fn=<MseLossBackward0>)\n",
      "43 tensor(522.4101, grad_fn=<MseLossBackward0>)\n",
      "44 tensor(215.5579, grad_fn=<MseLossBackward0>)\n",
      "45 tensor(144.4626, grad_fn=<MseLossBackward0>)\n",
      "46 tensor(212.9926, grad_fn=<MseLossBackward0>)\n",
      "47 tensor(296.7896, grad_fn=<MseLossBackward0>)\n",
      "48 tensor(398.1804, grad_fn=<MseLossBackward0>)\n",
      "49 tensor(279.9752, grad_fn=<MseLossBackward0>)\n",
      "50 tensor(355.1298, grad_fn=<MseLossBackward0>)\n",
      "51 tensor(303.2737, grad_fn=<MseLossBackward0>)\n",
      "52 tensor(448.2037, grad_fn=<MseLossBackward0>)\n",
      "53 tensor(202.7839, grad_fn=<MseLossBackward0>)\n",
      "54 tensor(400.9154, grad_fn=<MseLossBackward0>)\n",
      "55 tensor(175.8501, grad_fn=<MseLossBackward0>)\n",
      "56 tensor(374.1929, grad_fn=<MseLossBackward0>)\n",
      "57 tensor(405.5321, grad_fn=<MseLossBackward0>)\n",
      "58 tensor(461.9906, grad_fn=<MseLossBackward0>)\n",
      "59 tensor(229.5952, grad_fn=<MseLossBackward0>)\n",
      "60 tensor(273.4194, grad_fn=<MseLossBackward0>)\n",
      "61 tensor(342.7252, grad_fn=<MseLossBackward0>)\n",
      "62 tensor(395.0661, grad_fn=<MseLossBackward0>)\n",
      "63 tensor(523.9440, grad_fn=<MseLossBackward0>)\n",
      "64 tensor(173.3445, grad_fn=<MseLossBackward0>)\n",
      "65 tensor(535.0289, grad_fn=<MseLossBackward0>)\n",
      "66 tensor(441.8198, grad_fn=<MseLossBackward0>)\n",
      "67 tensor(452.8788, grad_fn=<MseLossBackward0>)\n",
      "68 tensor(116.2421, grad_fn=<MseLossBackward0>)\n",
      "69 tensor(159.5999, grad_fn=<MseLossBackward0>)\n",
      "70 tensor(228.4346, grad_fn=<MseLossBackward0>)\n",
      "71 tensor(362.2018, grad_fn=<MseLossBackward0>)\n",
      "72 tensor(236.1355, grad_fn=<MseLossBackward0>)\n",
      "73 tensor(429.1939, grad_fn=<MseLossBackward0>)\n",
      "74 tensor(186.4037, grad_fn=<MseLossBackward0>)\n",
      "75 tensor(191.8879, grad_fn=<MseLossBackward0>)\n",
      "76 tensor(240.6029, grad_fn=<MseLossBackward0>)\n",
      "77 tensor(215.6767, grad_fn=<MseLossBackward0>)\n",
      "78 tensor(254.8136, grad_fn=<MseLossBackward0>)\n",
      "79 tensor(446.1479, grad_fn=<MseLossBackward0>)\n",
      "80 tensor(382.8278, grad_fn=<MseLossBackward0>)\n",
      "81 tensor(216.3571, grad_fn=<MseLossBackward0>)\n",
      "82 tensor(286.1380, grad_fn=<MseLossBackward0>)\n",
      "83 tensor(245.7432, grad_fn=<MseLossBackward0>)\n",
      "84 tensor(109.7710, grad_fn=<MseLossBackward0>)\n",
      "85 tensor(288.4870, grad_fn=<MseLossBackward0>)\n",
      "86 tensor(180.9588, grad_fn=<MseLossBackward0>)\n",
      "87 tensor(129.2649, grad_fn=<MseLossBackward0>)\n",
      "88 tensor(306.7770, grad_fn=<MseLossBackward0>)\n",
      "89 tensor(209.8712, grad_fn=<MseLossBackward0>)\n",
      "90 tensor(260.0761, grad_fn=<MseLossBackward0>)\n",
      "91 tensor(481.8729, grad_fn=<MseLossBackward0>)\n",
      "92 tensor(289.8534, grad_fn=<MseLossBackward0>)\n",
      "93 tensor(185.0872, grad_fn=<MseLossBackward0>)\n",
      "94 tensor(214.6519, grad_fn=<MseLossBackward0>)\n",
      "95 tensor(243.6483, grad_fn=<MseLossBackward0>)\n",
      "96 tensor(216.1580, grad_fn=<MseLossBackward0>)\n",
      "97 tensor(255.6605, grad_fn=<MseLossBackward0>)\n",
      "98 tensor(337.0236, grad_fn=<MseLossBackward0>)\n",
      "99 tensor(199.9483, grad_fn=<MseLossBackward0>)\n",
      "100 tensor(305.1436, grad_fn=<MseLossBackward0>)\n",
      "101 tensor(289.9029, grad_fn=<MseLossBackward0>)\n",
      "102 tensor(212.7048, grad_fn=<MseLossBackward0>)\n",
      "103 tensor(394.5938, grad_fn=<MseLossBackward0>)\n",
      "104 tensor(251.3124, grad_fn=<MseLossBackward0>)\n",
      "105 tensor(112.2563, grad_fn=<MseLossBackward0>)\n",
      "106 tensor(362.6255, grad_fn=<MseLossBackward0>)\n",
      "107 tensor(313.3993, grad_fn=<MseLossBackward0>)\n",
      "108 tensor(299.3083, grad_fn=<MseLossBackward0>)\n",
      "109 tensor(304.1499, grad_fn=<MseLossBackward0>)\n",
      "110 tensor(172.1000, grad_fn=<MseLossBackward0>)\n",
      "111 tensor(194.9966, grad_fn=<MseLossBackward0>)\n",
      "112 tensor(180.8006, grad_fn=<MseLossBackward0>)\n",
      "113 tensor(399.6437, grad_fn=<MseLossBackward0>)\n",
      "114 tensor(254.6055, grad_fn=<MseLossBackward0>)\n",
      "115 tensor(213.2996, grad_fn=<MseLossBackward0>)\n",
      "116 tensor(202.4278, grad_fn=<MseLossBackward0>)\n",
      "117 tensor(630.4985, grad_fn=<MseLossBackward0>)\n",
      "118 tensor(376.7485, grad_fn=<MseLossBackward0>)\n",
      "119 tensor(196.8888, grad_fn=<MseLossBackward0>)\n",
      "120 tensor(202.2651, grad_fn=<MseLossBackward0>)\n",
      "121 tensor(161.2877, grad_fn=<MseLossBackward0>)\n",
      "122 tensor(284.8020, grad_fn=<MseLossBackward0>)\n",
      "123 tensor(291.0156, grad_fn=<MseLossBackward0>)\n",
      "124 tensor(126.2492, grad_fn=<MseLossBackward0>)\n",
      "125 tensor(270.7277, grad_fn=<MseLossBackward0>)\n",
      "126 tensor(367.4806, grad_fn=<MseLossBackward0>)\n",
      "127 tensor(269.2270, grad_fn=<MseLossBackward0>)\n",
      "128 tensor(211.8541, grad_fn=<MseLossBackward0>)\n",
      "129 tensor(255.0637, grad_fn=<MseLossBackward0>)\n",
      "130 tensor(133.4438, grad_fn=<MseLossBackward0>)\n",
      "131 tensor(332.0706, grad_fn=<MseLossBackward0>)\n",
      "132 tensor(179.2284, grad_fn=<MseLossBackward0>)\n",
      "133 tensor(174.0468, grad_fn=<MseLossBackward0>)\n",
      "134 tensor(179.2738, grad_fn=<MseLossBackward0>)\n",
      "135 tensor(356.4254, grad_fn=<MseLossBackward0>)\n",
      "136 tensor(296.7890, grad_fn=<MseLossBackward0>)\n",
      "137 tensor(424.7956, grad_fn=<MseLossBackward0>)\n",
      "138 tensor(248.9203, grad_fn=<MseLossBackward0>)\n",
      "139 tensor(225.1371, grad_fn=<MseLossBackward0>)\n",
      "140 tensor(79.0153, grad_fn=<MseLossBackward0>)\n",
      "141 tensor(254.0921, grad_fn=<MseLossBackward0>)\n",
      "142 tensor(180.3404, grad_fn=<MseLossBackward0>)\n",
      "143 tensor(133.6495, grad_fn=<MseLossBackward0>)\n",
      "144 tensor(78.7325, grad_fn=<MseLossBackward0>)\n",
      "145 tensor(220.6366, grad_fn=<MseLossBackward0>)\n",
      "146 tensor(158.9303, grad_fn=<MseLossBackward0>)\n",
      "147 tensor(347.9914, grad_fn=<MseLossBackward0>)\n",
      "148 tensor(489.5652, grad_fn=<MseLossBackward0>)\n",
      "149 tensor(147.7173, grad_fn=<MseLossBackward0>)\n",
      "150 tensor(204.6081, grad_fn=<MseLossBackward0>)\n",
      "151 tensor(246.5137, grad_fn=<MseLossBackward0>)\n",
      "152 tensor(336.5424, grad_fn=<MseLossBackward0>)\n",
      "153 tensor(229.1743, grad_fn=<MseLossBackward0>)\n",
      "154 tensor(241.1823, grad_fn=<MseLossBackward0>)\n",
      "155 tensor(269.7007, grad_fn=<MseLossBackward0>)\n",
      "156 tensor(163.7519, grad_fn=<MseLossBackward0>)\n",
      "157 tensor(231.6215, grad_fn=<MseLossBackward0>)\n",
      "158 tensor(166.8899, grad_fn=<MseLossBackward0>)\n",
      "159 tensor(284.8796, grad_fn=<MseLossBackward0>)\n",
      "160 tensor(160.0746, grad_fn=<MseLossBackward0>)\n",
      "161 tensor(229.0053, grad_fn=<MseLossBackward0>)\n",
      "162 tensor(318.0815, grad_fn=<MseLossBackward0>)\n",
      "163 tensor(157.3331, grad_fn=<MseLossBackward0>)\n",
      "164 tensor(225.1349, grad_fn=<MseLossBackward0>)\n",
      "165 tensor(158.8357, grad_fn=<MseLossBackward0>)\n",
      "166 tensor(265.2667, grad_fn=<MseLossBackward0>)\n",
      "167 tensor(357.2647, grad_fn=<MseLossBackward0>)\n",
      "168 tensor(320.4623, grad_fn=<MseLossBackward0>)\n",
      "169 tensor(257.2383, grad_fn=<MseLossBackward0>)\n",
      "170 tensor(213.7318, grad_fn=<MseLossBackward0>)\n",
      "171 tensor(180.5951, grad_fn=<MseLossBackward0>)\n",
      "172 tensor(239.0069, grad_fn=<MseLossBackward0>)\n",
      "173 tensor(109.7111, grad_fn=<MseLossBackward0>)\n",
      "174 tensor(388.5861, grad_fn=<MseLossBackward0>)\n",
      "175 tensor(225.9179, grad_fn=<MseLossBackward0>)\n",
      "176 tensor(139.8395, grad_fn=<MseLossBackward0>)\n",
      "177 tensor(183.4027, grad_fn=<MseLossBackward0>)\n",
      "178 tensor(200.5078, grad_fn=<MseLossBackward0>)\n",
      "179 tensor(338.4193, grad_fn=<MseLossBackward0>)\n",
      "180 tensor(235.8886, grad_fn=<MseLossBackward0>)\n",
      "181 tensor(313.9565, grad_fn=<MseLossBackward0>)\n",
      "182 tensor(191.2004, grad_fn=<MseLossBackward0>)\n",
      "183 tensor(265.1758, grad_fn=<MseLossBackward0>)\n",
      "184 tensor(293.2675, grad_fn=<MseLossBackward0>)\n",
      "185 tensor(136.0267, grad_fn=<MseLossBackward0>)\n",
      "186 tensor(175.6796, grad_fn=<MseLossBackward0>)\n",
      "187 tensor(167.8038, grad_fn=<MseLossBackward0>)\n",
      "188 tensor(184.0998, grad_fn=<MseLossBackward0>)\n",
      "189 tensor(178.0746, grad_fn=<MseLossBackward0>)\n",
      "190 tensor(142.5568, grad_fn=<MseLossBackward0>)\n",
      "191 tensor(195.9057, grad_fn=<MseLossBackward0>)\n",
      "192 tensor(141.1049, grad_fn=<MseLossBackward0>)\n",
      "193 tensor(203.2199, grad_fn=<MseLossBackward0>)\n",
      "194 tensor(217.3542, grad_fn=<MseLossBackward0>)\n",
      "195 tensor(161.9352, grad_fn=<MseLossBackward0>)\n",
      "196 tensor(120.9382, grad_fn=<MseLossBackward0>)\n",
      "197 tensor(173.8179, grad_fn=<MseLossBackward0>)\n",
      "198 tensor(239.6579, grad_fn=<MseLossBackward0>)\n",
      "199 tensor(223.1725, grad_fn=<MseLossBackward0>)\n",
      "200 tensor(202.0988, grad_fn=<MseLossBackward0>)\n",
      "201 tensor(279.8074, grad_fn=<MseLossBackward0>)\n",
      "202 tensor(176.2932, grad_fn=<MseLossBackward0>)\n",
      "203 tensor(206.2132, grad_fn=<MseLossBackward0>)\n",
      "204 tensor(141.5295, grad_fn=<MseLossBackward0>)\n",
      "205 tensor(213.2526, grad_fn=<MseLossBackward0>)\n",
      "206 tensor(259.0805, grad_fn=<MseLossBackward0>)\n",
      "207 tensor(186.1093, grad_fn=<MseLossBackward0>)\n",
      "208 tensor(136.2296, grad_fn=<MseLossBackward0>)\n",
      "209 tensor(266.7818, grad_fn=<MseLossBackward0>)\n",
      "210 tensor(145.0786, grad_fn=<MseLossBackward0>)\n",
      "211 tensor(172.3204, grad_fn=<MseLossBackward0>)\n",
      "212 tensor(159.5603, grad_fn=<MseLossBackward0>)\n",
      "213 tensor(165.8422, grad_fn=<MseLossBackward0>)\n",
      "214 tensor(196.3789, grad_fn=<MseLossBackward0>)\n",
      "215 tensor(147.1528, grad_fn=<MseLossBackward0>)\n",
      "216 tensor(105.2019, grad_fn=<MseLossBackward0>)\n",
      "217 tensor(244.0712, grad_fn=<MseLossBackward0>)\n",
      "218 tensor(107.3563, grad_fn=<MseLossBackward0>)\n",
      "219 tensor(306.2866, grad_fn=<MseLossBackward0>)\n",
      "220 tensor(171.5331, grad_fn=<MseLossBackward0>)\n",
      "221 tensor(135.3054, grad_fn=<MseLossBackward0>)\n",
      "222 tensor(117.7062, grad_fn=<MseLossBackward0>)\n",
      "223 tensor(221.4708, grad_fn=<MseLossBackward0>)\n",
      "224 tensor(180.2803, grad_fn=<MseLossBackward0>)\n",
      "225 tensor(161.7996, grad_fn=<MseLossBackward0>)\n",
      "226 tensor(118.1518, grad_fn=<MseLossBackward0>)\n",
      "227 tensor(174.6844, grad_fn=<MseLossBackward0>)\n",
      "228 tensor(172.9862, grad_fn=<MseLossBackward0>)\n",
      "229 tensor(98.1832, grad_fn=<MseLossBackward0>)\n",
      "230 tensor(206.6633, grad_fn=<MseLossBackward0>)\n",
      "231 tensor(291.5222, grad_fn=<MseLossBackward0>)\n",
      "232 tensor(96.8597, grad_fn=<MseLossBackward0>)\n",
      "233 tensor(212.6492, grad_fn=<MseLossBackward0>)\n",
      "234 tensor(123.1378, grad_fn=<MseLossBackward0>)\n",
      "235 tensor(183.0459, grad_fn=<MseLossBackward0>)\n",
      "236 tensor(185.5237, grad_fn=<MseLossBackward0>)\n",
      "237 tensor(237.3530, grad_fn=<MseLossBackward0>)\n",
      "238 tensor(251.2628, grad_fn=<MseLossBackward0>)\n",
      "239 tensor(73.2077, grad_fn=<MseLossBackward0>)\n",
      "240 tensor(315.7596, grad_fn=<MseLossBackward0>)\n",
      "241 tensor(185.8801, grad_fn=<MseLossBackward0>)\n",
      "242 tensor(134.4484, grad_fn=<MseLossBackward0>)\n",
      "243 tensor(202.4912, grad_fn=<MseLossBackward0>)\n",
      "244 tensor(348.1045, grad_fn=<MseLossBackward0>)\n",
      "245 tensor(153.6668, grad_fn=<MseLossBackward0>)\n",
      "246 tensor(138.5133, grad_fn=<MseLossBackward0>)\n",
      "247 tensor(171.7310, grad_fn=<MseLossBackward0>)\n",
      "248 tensor(119.6935, grad_fn=<MseLossBackward0>)\n",
      "249 tensor(95.2302, grad_fn=<MseLossBackward0>)\n",
      "250 tensor(132.7823, grad_fn=<MseLossBackward0>)\n",
      "251 tensor(120.9891, grad_fn=<MseLossBackward0>)\n",
      "252 tensor(140.9764, grad_fn=<MseLossBackward0>)\n",
      "253 tensor(98.6921, grad_fn=<MseLossBackward0>)\n",
      "254 tensor(159.2210, grad_fn=<MseLossBackward0>)\n",
      "255 tensor(186.0581, grad_fn=<MseLossBackward0>)\n",
      "256 tensor(172.4414, grad_fn=<MseLossBackward0>)\n",
      "257 tensor(163.9377, grad_fn=<MseLossBackward0>)\n",
      "258 tensor(151.4012, grad_fn=<MseLossBackward0>)\n",
      "259 tensor(258.3296, grad_fn=<MseLossBackward0>)\n",
      "260 tensor(175.9262, grad_fn=<MseLossBackward0>)\n",
      "261 tensor(99.9189, grad_fn=<MseLossBackward0>)\n",
      "262 tensor(278.6443, grad_fn=<MseLossBackward0>)\n",
      "263 tensor(461.0530, grad_fn=<MseLossBackward0>)\n",
      "264 tensor(61.3660, grad_fn=<MseLossBackward0>)\n",
      "265 tensor(188.4467, grad_fn=<MseLossBackward0>)\n",
      "266 tensor(63.2362, grad_fn=<MseLossBackward0>)\n",
      "267 tensor(145.4913, grad_fn=<MseLossBackward0>)\n",
      "268 tensor(134.7240, grad_fn=<MseLossBackward0>)\n",
      "269 tensor(41.0696, grad_fn=<MseLossBackward0>)\n",
      "270 tensor(150.9648, grad_fn=<MseLossBackward0>)\n",
      "271 tensor(148.0974, grad_fn=<MseLossBackward0>)\n",
      "272 tensor(56.2681, grad_fn=<MseLossBackward0>)\n",
      "273 tensor(138.4582, grad_fn=<MseLossBackward0>)\n",
      "274 tensor(85.8890, grad_fn=<MseLossBackward0>)\n",
      "275 tensor(160.8362, grad_fn=<MseLossBackward0>)\n",
      "276 tensor(97.9691, grad_fn=<MseLossBackward0>)\n",
      "277 tensor(63.4975, grad_fn=<MseLossBackward0>)\n",
      "278 tensor(158.6130, grad_fn=<MseLossBackward0>)\n",
      "279 tensor(153.9650, grad_fn=<MseLossBackward0>)\n",
      "280 tensor(153.7264, grad_fn=<MseLossBackward0>)\n",
      "281 tensor(97.0047, grad_fn=<MseLossBackward0>)\n",
      "282 tensor(203.7325, grad_fn=<MseLossBackward0>)\n",
      "283 tensor(129.2691, grad_fn=<MseLossBackward0>)\n",
      "284 tensor(235.6238, grad_fn=<MseLossBackward0>)\n",
      "285 tensor(180.8007, grad_fn=<MseLossBackward0>)\n",
      "286 tensor(110.2018, grad_fn=<MseLossBackward0>)\n",
      "287 tensor(127.7204, grad_fn=<MseLossBackward0>)\n",
      "288 tensor(263.2141, grad_fn=<MseLossBackward0>)\n",
      "289 tensor(93.6392, grad_fn=<MseLossBackward0>)\n",
      "290 tensor(132.1728, grad_fn=<MseLossBackward0>)\n",
      "291 tensor(104.6532, grad_fn=<MseLossBackward0>)\n",
      "292 tensor(238.9076, grad_fn=<MseLossBackward0>)\n",
      "293 tensor(118.2604, grad_fn=<MseLossBackward0>)\n",
      "294 tensor(133.9663, grad_fn=<MseLossBackward0>)\n",
      "295 tensor(124.1551, grad_fn=<MseLossBackward0>)\n",
      "296 tensor(189.3423, grad_fn=<MseLossBackward0>)\n",
      "297 tensor(140.8491, grad_fn=<MseLossBackward0>)\n",
      "298 tensor(231.4681, grad_fn=<MseLossBackward0>)\n",
      "299 tensor(63.6586, grad_fn=<MseLossBackward0>)\n",
      "300 tensor(102.7123, grad_fn=<MseLossBackward0>)\n",
      "301 tensor(198.0994, grad_fn=<MseLossBackward0>)\n",
      "302 tensor(192.6239, grad_fn=<MseLossBackward0>)\n",
      "303 tensor(122.2049, grad_fn=<MseLossBackward0>)\n",
      "304 tensor(278.2890, grad_fn=<MseLossBackward0>)\n",
      "305 tensor(95.9224, grad_fn=<MseLossBackward0>)\n",
      "306 tensor(196.2214, grad_fn=<MseLossBackward0>)\n",
      "307 tensor(131.0800, grad_fn=<MseLossBackward0>)\n",
      "308 tensor(254.9454, grad_fn=<MseLossBackward0>)\n",
      "309 tensor(75.2850, grad_fn=<MseLossBackward0>)\n",
      "310 tensor(108.7569, grad_fn=<MseLossBackward0>)\n",
      "311 tensor(115.5586, grad_fn=<MseLossBackward0>)\n",
      "312 tensor(84.4216, grad_fn=<MseLossBackward0>)\n",
      "313 tensor(107.8046, grad_fn=<MseLossBackward0>)\n",
      "314 tensor(210.1089, grad_fn=<MseLossBackward0>)\n",
      "315 tensor(71.6924, grad_fn=<MseLossBackward0>)\n",
      "316 tensor(143.3268, grad_fn=<MseLossBackward0>)\n",
      "317 tensor(81.9564, grad_fn=<MseLossBackward0>)\n",
      "318 tensor(71.0997, grad_fn=<MseLossBackward0>)\n",
      "319 tensor(278.4553, grad_fn=<MseLossBackward0>)\n",
      "320 tensor(227.8168, grad_fn=<MseLossBackward0>)\n",
      "321 tensor(103.0699, grad_fn=<MseLossBackward0>)\n",
      "322 tensor(142.2698, grad_fn=<MseLossBackward0>)\n",
      "323 tensor(65.0320, grad_fn=<MseLossBackward0>)\n",
      "324 tensor(114.8351, grad_fn=<MseLossBackward0>)\n",
      "325 tensor(142.3510, grad_fn=<MseLossBackward0>)\n",
      "326 tensor(220.5798, grad_fn=<MseLossBackward0>)\n",
      "327 tensor(121.5752, grad_fn=<MseLossBackward0>)\n",
      "328 tensor(193.1829, grad_fn=<MseLossBackward0>)\n",
      "329 tensor(149.7221, grad_fn=<MseLossBackward0>)\n",
      "330 tensor(56.3213, grad_fn=<MseLossBackward0>)\n",
      "331 tensor(104.9181, grad_fn=<MseLossBackward0>)\n",
      "332 tensor(114.0513, grad_fn=<MseLossBackward0>)\n",
      "333 tensor(42.7874, grad_fn=<MseLossBackward0>)\n",
      "334 tensor(186.2362, grad_fn=<MseLossBackward0>)\n",
      "335 tensor(136.8756, grad_fn=<MseLossBackward0>)\n",
      "336 tensor(193.6875, grad_fn=<MseLossBackward0>)\n",
      "337 tensor(86.6052, grad_fn=<MseLossBackward0>)\n",
      "338 tensor(61.2281, grad_fn=<MseLossBackward0>)\n",
      "339 tensor(101.8815, grad_fn=<MseLossBackward0>)\n",
      "340 tensor(77.9106, grad_fn=<MseLossBackward0>)\n",
      "341 tensor(105.2700, grad_fn=<MseLossBackward0>)\n",
      "342 tensor(145.5687, grad_fn=<MseLossBackward0>)\n",
      "343 tensor(160.3701, grad_fn=<MseLossBackward0>)\n",
      "344 tensor(68.5261, grad_fn=<MseLossBackward0>)\n",
      "345 tensor(159.4154, grad_fn=<MseLossBackward0>)\n",
      "346 tensor(81.9577, grad_fn=<MseLossBackward0>)\n",
      "347 tensor(225.1619, grad_fn=<MseLossBackward0>)\n",
      "348 tensor(102.4395, grad_fn=<MseLossBackward0>)\n",
      "349 tensor(85.9261, grad_fn=<MseLossBackward0>)\n",
      "350 tensor(173.7122, grad_fn=<MseLossBackward0>)\n",
      "351 tensor(101.3913, grad_fn=<MseLossBackward0>)\n",
      "352 tensor(62.4711, grad_fn=<MseLossBackward0>)\n",
      "353 tensor(82.7421, grad_fn=<MseLossBackward0>)\n",
      "354 tensor(180.6920, grad_fn=<MseLossBackward0>)\n",
      "355 tensor(56.4678, grad_fn=<MseLossBackward0>)\n",
      "356 tensor(78.5808, grad_fn=<MseLossBackward0>)\n",
      "357 tensor(75.2456, grad_fn=<MseLossBackward0>)\n",
      "358 tensor(243.3181, grad_fn=<MseLossBackward0>)\n",
      "359 tensor(85.9625, grad_fn=<MseLossBackward0>)\n",
      "360 tensor(229.2207, grad_fn=<MseLossBackward0>)\n",
      "361 tensor(77.9021, grad_fn=<MseLossBackward0>)\n",
      "362 tensor(142.6427, grad_fn=<MseLossBackward0>)\n",
      "363 tensor(49.7346, grad_fn=<MseLossBackward0>)\n",
      "364 tensor(197.6947, grad_fn=<MseLossBackward0>)\n",
      "365 tensor(128.8337, grad_fn=<MseLossBackward0>)\n",
      "366 tensor(67.4286, grad_fn=<MseLossBackward0>)\n",
      "367 tensor(169.3453, grad_fn=<MseLossBackward0>)\n",
      "368 tensor(143.8246, grad_fn=<MseLossBackward0>)\n",
      "369 tensor(34.9182, grad_fn=<MseLossBackward0>)\n",
      "370 tensor(79.6219, grad_fn=<MseLossBackward0>)\n",
      "371 tensor(112.6905, grad_fn=<MseLossBackward0>)\n",
      "372 tensor(108.4394, grad_fn=<MseLossBackward0>)\n",
      "373 tensor(138.5672, grad_fn=<MseLossBackward0>)\n",
      "374 tensor(161.5943, grad_fn=<MseLossBackward0>)\n",
      "375 tensor(104.5643, grad_fn=<MseLossBackward0>)\n",
      "376 tensor(58.5009, grad_fn=<MseLossBackward0>)\n",
      "377 tensor(123.3281, grad_fn=<MseLossBackward0>)\n",
      "378 tensor(107.1350, grad_fn=<MseLossBackward0>)\n",
      "379 tensor(87.7579, grad_fn=<MseLossBackward0>)\n",
      "380 tensor(67.4699, grad_fn=<MseLossBackward0>)\n",
      "381 tensor(117.3697, grad_fn=<MseLossBackward0>)\n",
      "382 tensor(104.4902, grad_fn=<MseLossBackward0>)\n",
      "383 tensor(100.7351, grad_fn=<MseLossBackward0>)\n",
      "384 tensor(92.7178, grad_fn=<MseLossBackward0>)\n",
      "385 tensor(177.7793, grad_fn=<MseLossBackward0>)\n",
      "386 tensor(101.7113, grad_fn=<MseLossBackward0>)\n",
      "387 tensor(57.1795, grad_fn=<MseLossBackward0>)\n",
      "388 tensor(182.1409, grad_fn=<MseLossBackward0>)\n",
      "389 tensor(259.4318, grad_fn=<MseLossBackward0>)\n",
      "390 tensor(124.4940, grad_fn=<MseLossBackward0>)\n",
      "391 tensor(102.6101, grad_fn=<MseLossBackward0>)\n",
      "392 tensor(99.1364, grad_fn=<MseLossBackward0>)\n",
      "393 tensor(62.8861, grad_fn=<MseLossBackward0>)\n",
      "394 tensor(46.5469, grad_fn=<MseLossBackward0>)\n",
      "395 tensor(63.3310, grad_fn=<MseLossBackward0>)\n",
      "396 tensor(131.9115, grad_fn=<MseLossBackward0>)\n",
      "397 tensor(148.7453, grad_fn=<MseLossBackward0>)\n",
      "398 tensor(39.1399, grad_fn=<MseLossBackward0>)\n",
      "399 tensor(69.7543, grad_fn=<MseLossBackward0>)\n",
      "400 tensor(110.7692, grad_fn=<MseLossBackward0>)\n",
      "401 tensor(129.1744, grad_fn=<MseLossBackward0>)\n",
      "402 tensor(127.2374, grad_fn=<MseLossBackward0>)\n",
      "403 tensor(102.8845, grad_fn=<MseLossBackward0>)\n",
      "404 tensor(73.1623, grad_fn=<MseLossBackward0>)\n",
      "405 tensor(92.6409, grad_fn=<MseLossBackward0>)\n",
      "406 tensor(48.1526, grad_fn=<MseLossBackward0>)\n",
      "407 tensor(62.0985, grad_fn=<MseLossBackward0>)\n",
      "408 tensor(97.0410, grad_fn=<MseLossBackward0>)\n",
      "409 tensor(135.1332, grad_fn=<MseLossBackward0>)\n",
      "410 tensor(97.5789, grad_fn=<MseLossBackward0>)\n",
      "411 tensor(107.7170, grad_fn=<MseLossBackward0>)\n",
      "412 tensor(75.3741, grad_fn=<MseLossBackward0>)\n",
      "413 tensor(129.4338, grad_fn=<MseLossBackward0>)\n",
      "414 tensor(143.1841, grad_fn=<MseLossBackward0>)\n",
      "415 tensor(91.4098, grad_fn=<MseLossBackward0>)\n",
      "416 tensor(48.8992, grad_fn=<MseLossBackward0>)\n",
      "417 tensor(51.3617, grad_fn=<MseLossBackward0>)\n",
      "418 tensor(80.2187, grad_fn=<MseLossBackward0>)\n",
      "419 tensor(136.1431, grad_fn=<MseLossBackward0>)\n",
      "420 tensor(90.6373, grad_fn=<MseLossBackward0>)\n",
      "421 tensor(89.7457, grad_fn=<MseLossBackward0>)\n",
      "422 tensor(63.4095, grad_fn=<MseLossBackward0>)\n",
      "423 tensor(72.1613, grad_fn=<MseLossBackward0>)\n",
      "424 tensor(66.0463, grad_fn=<MseLossBackward0>)\n",
      "425 tensor(117.4719, grad_fn=<MseLossBackward0>)\n",
      "426 tensor(61.1212, grad_fn=<MseLossBackward0>)\n",
      "427 tensor(104.6658, grad_fn=<MseLossBackward0>)\n",
      "428 tensor(67.9376, grad_fn=<MseLossBackward0>)\n",
      "429 tensor(107.3221, grad_fn=<MseLossBackward0>)\n",
      "430 tensor(45.5248, grad_fn=<MseLossBackward0>)\n",
      "431 tensor(165.8359, grad_fn=<MseLossBackward0>)\n",
      "432 tensor(64.4160, grad_fn=<MseLossBackward0>)\n",
      "433 tensor(132.5221, grad_fn=<MseLossBackward0>)\n",
      "434 tensor(85.9530, grad_fn=<MseLossBackward0>)\n",
      "435 tensor(25.1440, grad_fn=<MseLossBackward0>)\n",
      "436 tensor(39.6026, grad_fn=<MseLossBackward0>)\n",
      "437 tensor(56.3572, grad_fn=<MseLossBackward0>)\n",
      "438 tensor(67.5774, grad_fn=<MseLossBackward0>)\n",
      "439 tensor(113.1311, grad_fn=<MseLossBackward0>)\n",
      "440 tensor(111.0181, grad_fn=<MseLossBackward0>)\n",
      "441 tensor(56.1247, grad_fn=<MseLossBackward0>)\n",
      "442 tensor(107.4672, grad_fn=<MseLossBackward0>)\n",
      "443 tensor(109.9844, grad_fn=<MseLossBackward0>)\n",
      "444 tensor(124.0126, grad_fn=<MseLossBackward0>)\n",
      "445 tensor(139.6963, grad_fn=<MseLossBackward0>)\n",
      "446 tensor(139.4740, grad_fn=<MseLossBackward0>)\n",
      "447 tensor(52.5656, grad_fn=<MseLossBackward0>)\n",
      "448 tensor(72.7741, grad_fn=<MseLossBackward0>)\n",
      "449 tensor(62.5593, grad_fn=<MseLossBackward0>)\n",
      "450 tensor(58.9453, grad_fn=<MseLossBackward0>)\n",
      "451 tensor(77.2391, grad_fn=<MseLossBackward0>)\n",
      "452 tensor(60.2933, grad_fn=<MseLossBackward0>)\n",
      "453 tensor(98.4043, grad_fn=<MseLossBackward0>)\n",
      "454 tensor(44.6064, grad_fn=<MseLossBackward0>)\n",
      "455 tensor(82.5210, grad_fn=<MseLossBackward0>)\n",
      "456 tensor(58.3272, grad_fn=<MseLossBackward0>)\n",
      "457 tensor(126.0931, grad_fn=<MseLossBackward0>)\n",
      "458 tensor(73.6319, grad_fn=<MseLossBackward0>)\n",
      "459 tensor(87.0811, grad_fn=<MseLossBackward0>)\n",
      "460 tensor(95.5681, grad_fn=<MseLossBackward0>)\n",
      "461 tensor(47.8228, grad_fn=<MseLossBackward0>)\n",
      "462 tensor(107.8494, grad_fn=<MseLossBackward0>)\n",
      "463 tensor(90.9703, grad_fn=<MseLossBackward0>)\n",
      "464 tensor(100.1015, grad_fn=<MseLossBackward0>)\n",
      "465 tensor(29.4902, grad_fn=<MseLossBackward0>)\n",
      "466 tensor(59.8804, grad_fn=<MseLossBackward0>)\n",
      "467 tensor(42.1020, grad_fn=<MseLossBackward0>)\n",
      "468 tensor(94.6465, grad_fn=<MseLossBackward0>)\n",
      "469 tensor(64.4116, grad_fn=<MseLossBackward0>)\n",
      "470 tensor(94.9264, grad_fn=<MseLossBackward0>)\n",
      "471 tensor(68.6156, grad_fn=<MseLossBackward0>)\n",
      "472 tensor(54.4917, grad_fn=<MseLossBackward0>)\n",
      "473 tensor(37.3937, grad_fn=<MseLossBackward0>)\n",
      "474 tensor(40.4455, grad_fn=<MseLossBackward0>)\n",
      "475 tensor(66.5967, grad_fn=<MseLossBackward0>)\n",
      "476 tensor(36.5243, grad_fn=<MseLossBackward0>)\n",
      "477 tensor(75.6576, grad_fn=<MseLossBackward0>)\n",
      "478 tensor(55.9835, grad_fn=<MseLossBackward0>)\n",
      "479 tensor(110.7767, grad_fn=<MseLossBackward0>)\n",
      "480 tensor(198.0446, grad_fn=<MseLossBackward0>)\n",
      "481 tensor(55.6991, grad_fn=<MseLossBackward0>)\n",
      "482 tensor(111.5717, grad_fn=<MseLossBackward0>)\n",
      "483 tensor(37.7686, grad_fn=<MseLossBackward0>)\n",
      "484 tensor(31.6724, grad_fn=<MseLossBackward0>)\n",
      "485 tensor(59.4642, grad_fn=<MseLossBackward0>)\n",
      "486 tensor(54.1028, grad_fn=<MseLossBackward0>)\n",
      "487 tensor(92.1800, grad_fn=<MseLossBackward0>)\n",
      "488 tensor(166.6212, grad_fn=<MseLossBackward0>)\n",
      "489 tensor(64.4004, grad_fn=<MseLossBackward0>)\n",
      "490 tensor(39.7484, grad_fn=<MseLossBackward0>)\n",
      "491 tensor(46.9475, grad_fn=<MseLossBackward0>)\n",
      "492 tensor(71.4697, grad_fn=<MseLossBackward0>)\n",
      "493 tensor(65.1071, grad_fn=<MseLossBackward0>)\n",
      "494 tensor(54.3496, grad_fn=<MseLossBackward0>)\n",
      "495 tensor(42.2045, grad_fn=<MseLossBackward0>)\n",
      "496 tensor(146.0904, grad_fn=<MseLossBackward0>)\n",
      "497 tensor(56.7767, grad_fn=<MseLossBackward0>)\n",
      "498 tensor(42.1760, grad_fn=<MseLossBackward0>)\n",
      "499 tensor(119.9234, grad_fn=<MseLossBackward0>)\n",
      "500 tensor(41.8959, grad_fn=<MseLossBackward0>)\n",
      "501 tensor(58.9275, grad_fn=<MseLossBackward0>)\n",
      "502 tensor(85.2485, grad_fn=<MseLossBackward0>)\n",
      "503 tensor(43.3605, grad_fn=<MseLossBackward0>)\n",
      "504 tensor(38.6535, grad_fn=<MseLossBackward0>)\n",
      "505 tensor(24.7762, grad_fn=<MseLossBackward0>)\n",
      "506 tensor(109.7705, grad_fn=<MseLossBackward0>)\n",
      "507 tensor(26.9406, grad_fn=<MseLossBackward0>)\n",
      "508 tensor(31.7477, grad_fn=<MseLossBackward0>)\n",
      "509 tensor(112.6745, grad_fn=<MseLossBackward0>)\n",
      "510 tensor(43.6209, grad_fn=<MseLossBackward0>)\n",
      "511 tensor(43.1800, grad_fn=<MseLossBackward0>)\n",
      "512 tensor(83.8224, grad_fn=<MseLossBackward0>)\n",
      "513 tensor(26.9499, grad_fn=<MseLossBackward0>)\n",
      "514 tensor(58.6385, grad_fn=<MseLossBackward0>)\n",
      "515 tensor(82.2122, grad_fn=<MseLossBackward0>)\n",
      "516 tensor(48.2529, grad_fn=<MseLossBackward0>)\n",
      "517 tensor(28.6959, grad_fn=<MseLossBackward0>)\n",
      "518 tensor(40.4724, grad_fn=<MseLossBackward0>)\n",
      "519 tensor(38.7245, grad_fn=<MseLossBackward0>)\n",
      "520 tensor(59.0237, grad_fn=<MseLossBackward0>)\n",
      "521 tensor(64.7315, grad_fn=<MseLossBackward0>)\n",
      "522 tensor(70.6213, grad_fn=<MseLossBackward0>)\n",
      "523 tensor(80.0966, grad_fn=<MseLossBackward0>)\n",
      "524 tensor(31.5300, grad_fn=<MseLossBackward0>)\n",
      "525 tensor(65.8084, grad_fn=<MseLossBackward0>)\n",
      "526 tensor(74.9362, grad_fn=<MseLossBackward0>)\n",
      "527 tensor(99.1286, grad_fn=<MseLossBackward0>)\n",
      "528 tensor(33.7774, grad_fn=<MseLossBackward0>)\n",
      "529 tensor(91.6856, grad_fn=<MseLossBackward0>)\n",
      "530 tensor(70.2978, grad_fn=<MseLossBackward0>)\n",
      "531 tensor(35.3567, grad_fn=<MseLossBackward0>)\n",
      "532 tensor(56.0965, grad_fn=<MseLossBackward0>)\n",
      "533 tensor(57.7958, grad_fn=<MseLossBackward0>)\n",
      "534 tensor(48.7349, grad_fn=<MseLossBackward0>)\n",
      "535 tensor(123.3855, grad_fn=<MseLossBackward0>)\n",
      "536 tensor(39.4522, grad_fn=<MseLossBackward0>)\n",
      "537 tensor(23.3404, grad_fn=<MseLossBackward0>)\n",
      "538 tensor(43.6398, grad_fn=<MseLossBackward0>)\n",
      "539 tensor(64.9563, grad_fn=<MseLossBackward0>)\n",
      "540 tensor(24.2450, grad_fn=<MseLossBackward0>)\n",
      "541 tensor(30.8972, grad_fn=<MseLossBackward0>)\n",
      "542 tensor(37.0378, grad_fn=<MseLossBackward0>)\n",
      "543 tensor(48.1818, grad_fn=<MseLossBackward0>)\n",
      "544 tensor(12.3896, grad_fn=<MseLossBackward0>)\n",
      "545 tensor(31.1981, grad_fn=<MseLossBackward0>)\n",
      "546 tensor(52.7630, grad_fn=<MseLossBackward0>)\n",
      "547 tensor(39.5118, grad_fn=<MseLossBackward0>)\n",
      "548 tensor(36.7759, grad_fn=<MseLossBackward0>)\n",
      "549 tensor(36.2087, grad_fn=<MseLossBackward0>)\n",
      "550 tensor(31.8336, grad_fn=<MseLossBackward0>)\n",
      "551 tensor(26.5027, grad_fn=<MseLossBackward0>)\n",
      "552 tensor(32.7467, grad_fn=<MseLossBackward0>)\n",
      "553 tensor(28.4570, grad_fn=<MseLossBackward0>)\n",
      "554 tensor(100.6070, grad_fn=<MseLossBackward0>)\n",
      "555 tensor(52.0309, grad_fn=<MseLossBackward0>)\n",
      "556 tensor(75.2187, grad_fn=<MseLossBackward0>)\n",
      "557 tensor(65.6185, grad_fn=<MseLossBackward0>)\n",
      "558 tensor(28.1306, grad_fn=<MseLossBackward0>)\n",
      "559 tensor(82.3065, grad_fn=<MseLossBackward0>)\n",
      "560 tensor(85.2954, grad_fn=<MseLossBackward0>)\n",
      "561 tensor(35.4234, grad_fn=<MseLossBackward0>)\n",
      "562 tensor(25.2259, grad_fn=<MseLossBackward0>)\n",
      "563 tensor(17.4072, grad_fn=<MseLossBackward0>)\n",
      "564 tensor(58.9625, grad_fn=<MseLossBackward0>)\n",
      "565 tensor(104.1881, grad_fn=<MseLossBackward0>)\n",
      "566 tensor(77.6731, grad_fn=<MseLossBackward0>)\n",
      "567 tensor(92.1509, grad_fn=<MseLossBackward0>)\n",
      "568 tensor(24.7466, grad_fn=<MseLossBackward0>)\n",
      "569 tensor(52.5184, grad_fn=<MseLossBackward0>)\n",
      "570 tensor(28.5305, grad_fn=<MseLossBackward0>)\n",
      "571 tensor(38.6439, grad_fn=<MseLossBackward0>)\n",
      "572 tensor(61.3690, grad_fn=<MseLossBackward0>)\n",
      "573 tensor(24.6983, grad_fn=<MseLossBackward0>)\n",
      "574 tensor(75.5841, grad_fn=<MseLossBackward0>)\n",
      "575 tensor(47.5760, grad_fn=<MseLossBackward0>)\n",
      "576 tensor(89.0441, grad_fn=<MseLossBackward0>)\n",
      "577 tensor(45.1262, grad_fn=<MseLossBackward0>)\n",
      "578 tensor(39.1080, grad_fn=<MseLossBackward0>)\n",
      "579 tensor(22.5488, grad_fn=<MseLossBackward0>)\n",
      "580 tensor(43.8780, grad_fn=<MseLossBackward0>)\n",
      "581 tensor(49.5718, grad_fn=<MseLossBackward0>)\n",
      "582 tensor(28.3557, grad_fn=<MseLossBackward0>)\n",
      "583 tensor(28.4309, grad_fn=<MseLossBackward0>)\n",
      "584 tensor(40.4601, grad_fn=<MseLossBackward0>)\n",
      "585 tensor(54.7703, grad_fn=<MseLossBackward0>)\n",
      "586 tensor(36.0455, grad_fn=<MseLossBackward0>)\n",
      "587 tensor(50.0952, grad_fn=<MseLossBackward0>)\n",
      "588 tensor(69.8881, grad_fn=<MseLossBackward0>)\n",
      "589 tensor(37.9531, grad_fn=<MseLossBackward0>)\n",
      "590 tensor(36.3802, grad_fn=<MseLossBackward0>)\n",
      "591 tensor(48.5914, grad_fn=<MseLossBackward0>)\n",
      "592 tensor(31.2166, grad_fn=<MseLossBackward0>)\n",
      "593 tensor(68.9618, grad_fn=<MseLossBackward0>)\n",
      "594 tensor(27.9351, grad_fn=<MseLossBackward0>)\n",
      "595 tensor(22.1509, grad_fn=<MseLossBackward0>)\n",
      "596 tensor(45.1593, grad_fn=<MseLossBackward0>)\n",
      "597 tensor(33.5946, grad_fn=<MseLossBackward0>)\n",
      "598 tensor(64.1875, grad_fn=<MseLossBackward0>)\n",
      "599 tensor(64.3268, grad_fn=<MseLossBackward0>)\n",
      "600 tensor(24.0257, grad_fn=<MseLossBackward0>)\n",
      "601 tensor(25.9079, grad_fn=<MseLossBackward0>)\n",
      "602 tensor(39.1336, grad_fn=<MseLossBackward0>)\n",
      "603 tensor(68.5116, grad_fn=<MseLossBackward0>)\n",
      "604 tensor(25.4922, grad_fn=<MseLossBackward0>)\n",
      "605 tensor(32.5622, grad_fn=<MseLossBackward0>)\n",
      "606 tensor(33.6043, grad_fn=<MseLossBackward0>)\n",
      "607 tensor(20.1173, grad_fn=<MseLossBackward0>)\n",
      "608 tensor(32.3985, grad_fn=<MseLossBackward0>)\n",
      "609 tensor(15.7469, grad_fn=<MseLossBackward0>)\n",
      "610 tensor(23.0448, grad_fn=<MseLossBackward0>)\n",
      "611 tensor(34.2829, grad_fn=<MseLossBackward0>)\n",
      "612 tensor(21.1986, grad_fn=<MseLossBackward0>)\n",
      "613 tensor(29.9514, grad_fn=<MseLossBackward0>)\n",
      "614 tensor(43.9113, grad_fn=<MseLossBackward0>)\n",
      "615 tensor(20.8052, grad_fn=<MseLossBackward0>)\n",
      "616 tensor(72.6858, grad_fn=<MseLossBackward0>)\n",
      "617 tensor(45.6082, grad_fn=<MseLossBackward0>)\n",
      "618 tensor(20.0872, grad_fn=<MseLossBackward0>)\n",
      "619 tensor(51.8981, grad_fn=<MseLossBackward0>)\n",
      "620 tensor(30.4353, grad_fn=<MseLossBackward0>)\n",
      "621 tensor(26.1592, grad_fn=<MseLossBackward0>)\n",
      "622 tensor(23.5242, grad_fn=<MseLossBackward0>)\n",
      "623 tensor(66.9330, grad_fn=<MseLossBackward0>)\n",
      "624 tensor(14.6521, grad_fn=<MseLossBackward0>)\n",
      "625 tensor(14.8480, grad_fn=<MseLossBackward0>)\n",
      "626 tensor(44.0921, grad_fn=<MseLossBackward0>)\n",
      "627 tensor(51.2550, grad_fn=<MseLossBackward0>)\n",
      "628 tensor(35.8150, grad_fn=<MseLossBackward0>)\n",
      "629 tensor(24.7420, grad_fn=<MseLossBackward0>)\n",
      "630 tensor(21.1575, grad_fn=<MseLossBackward0>)\n",
      "631 tensor(19.9961, grad_fn=<MseLossBackward0>)\n",
      "632 tensor(20.5553, grad_fn=<MseLossBackward0>)\n",
      "633 tensor(122.4206, grad_fn=<MseLossBackward0>)\n",
      "634 tensor(28.6055, grad_fn=<MseLossBackward0>)\n",
      "635 tensor(36.8904, grad_fn=<MseLossBackward0>)\n",
      "636 tensor(21.8192, grad_fn=<MseLossBackward0>)\n",
      "637 tensor(21.6820, grad_fn=<MseLossBackward0>)\n",
      "638 tensor(17.7999, grad_fn=<MseLossBackward0>)\n",
      "639 tensor(18.4771, grad_fn=<MseLossBackward0>)\n",
      "640 tensor(71.9415, grad_fn=<MseLossBackward0>)\n",
      "641 tensor(31.4407, grad_fn=<MseLossBackward0>)\n",
      "642 tensor(21.4746, grad_fn=<MseLossBackward0>)\n",
      "643 tensor(33.0841, grad_fn=<MseLossBackward0>)\n",
      "644 tensor(44.9133, grad_fn=<MseLossBackward0>)\n",
      "645 tensor(21.5749, grad_fn=<MseLossBackward0>)\n",
      "646 tensor(30.9731, grad_fn=<MseLossBackward0>)\n",
      "647 tensor(26.3855, grad_fn=<MseLossBackward0>)\n",
      "648 tensor(22.5626, grad_fn=<MseLossBackward0>)\n",
      "649 tensor(21.3505, grad_fn=<MseLossBackward0>)\n",
      "650 tensor(25.1217, grad_fn=<MseLossBackward0>)\n",
      "651 tensor(18.8537, grad_fn=<MseLossBackward0>)\n",
      "652 tensor(19.4220, grad_fn=<MseLossBackward0>)\n",
      "653 tensor(21.4711, grad_fn=<MseLossBackward0>)\n",
      "654 tensor(41.1356, grad_fn=<MseLossBackward0>)\n",
      "655 tensor(19.3246, grad_fn=<MseLossBackward0>)\n",
      "656 tensor(17.7416, grad_fn=<MseLossBackward0>)\n",
      "657 tensor(20.8794, grad_fn=<MseLossBackward0>)\n",
      "658 tensor(59.1178, grad_fn=<MseLossBackward0>)\n",
      "659 tensor(22.0871, grad_fn=<MseLossBackward0>)\n",
      "660 tensor(55.0864, grad_fn=<MseLossBackward0>)\n",
      "661 tensor(16.9580, grad_fn=<MseLossBackward0>)\n",
      "662 tensor(23.0274, grad_fn=<MseLossBackward0>)\n",
      "663 tensor(20.1115, grad_fn=<MseLossBackward0>)\n",
      "664 tensor(21.8981, grad_fn=<MseLossBackward0>)\n",
      "665 tensor(75.5340, grad_fn=<MseLossBackward0>)\n",
      "666 tensor(21.0064, grad_fn=<MseLossBackward0>)\n",
      "667 tensor(11.8766, grad_fn=<MseLossBackward0>)\n",
      "668 tensor(39.2234, grad_fn=<MseLossBackward0>)\n",
      "669 tensor(47.6642, grad_fn=<MseLossBackward0>)\n",
      "670 tensor(42.1063, grad_fn=<MseLossBackward0>)\n",
      "671 tensor(28.4077, grad_fn=<MseLossBackward0>)\n",
      "672 tensor(85.3539, grad_fn=<MseLossBackward0>)\n",
      "673 tensor(35.0424, grad_fn=<MseLossBackward0>)\n",
      "674 tensor(28.1228, grad_fn=<MseLossBackward0>)\n",
      "675 tensor(11.9793, grad_fn=<MseLossBackward0>)\n",
      "676 tensor(23.4446, grad_fn=<MseLossBackward0>)\n",
      "677 tensor(12.0446, grad_fn=<MseLossBackward0>)\n",
      "678 tensor(18.9140, grad_fn=<MseLossBackward0>)\n",
      "679 tensor(19.4560, grad_fn=<MseLossBackward0>)\n",
      "680 tensor(64.4222, grad_fn=<MseLossBackward0>)\n",
      "681 tensor(34.1301, grad_fn=<MseLossBackward0>)\n",
      "682 tensor(13.4419, grad_fn=<MseLossBackward0>)\n",
      "683 tensor(45.6001, grad_fn=<MseLossBackward0>)\n",
      "684 tensor(15.3339, grad_fn=<MseLossBackward0>)\n",
      "685 tensor(38.6933, grad_fn=<MseLossBackward0>)\n",
      "686 tensor(35.6254, grad_fn=<MseLossBackward0>)\n",
      "687 tensor(55.6047, grad_fn=<MseLossBackward0>)\n",
      "688 tensor(21.3585, grad_fn=<MseLossBackward0>)\n",
      "689 tensor(19.8234, grad_fn=<MseLossBackward0>)\n",
      "690 tensor(25.2760, grad_fn=<MseLossBackward0>)\n",
      "691 tensor(9.6916, grad_fn=<MseLossBackward0>)\n",
      "692 tensor(38.2788, grad_fn=<MseLossBackward0>)\n",
      "693 tensor(37.9686, grad_fn=<MseLossBackward0>)\n",
      "694 tensor(37.0542, grad_fn=<MseLossBackward0>)\n",
      "695 tensor(16.1474, grad_fn=<MseLossBackward0>)\n",
      "696 tensor(20.0155, grad_fn=<MseLossBackward0>)\n",
      "697 tensor(22.7984, grad_fn=<MseLossBackward0>)\n",
      "698 tensor(23.1147, grad_fn=<MseLossBackward0>)\n",
      "699 tensor(43.1966, grad_fn=<MseLossBackward0>)\n",
      "700 tensor(13.7908, grad_fn=<MseLossBackward0>)\n",
      "701 tensor(52.0587, grad_fn=<MseLossBackward0>)\n",
      "702 tensor(14.6962, grad_fn=<MseLossBackward0>)\n",
      "703 tensor(38.2975, grad_fn=<MseLossBackward0>)\n",
      "704 tensor(73.3455, grad_fn=<MseLossBackward0>)\n",
      "705 tensor(34.8952, grad_fn=<MseLossBackward0>)\n",
      "706 tensor(36.8089, grad_fn=<MseLossBackward0>)\n",
      "707 tensor(14.8816, grad_fn=<MseLossBackward0>)\n",
      "708 tensor(12.0447, grad_fn=<MseLossBackward0>)\n",
      "709 tensor(16.0319, grad_fn=<MseLossBackward0>)\n",
      "710 tensor(39.7108, grad_fn=<MseLossBackward0>)\n",
      "711 tensor(26.5464, grad_fn=<MseLossBackward0>)\n",
      "712 tensor(23.4683, grad_fn=<MseLossBackward0>)\n",
      "713 tensor(21.2871, grad_fn=<MseLossBackward0>)\n",
      "714 tensor(22.4285, grad_fn=<MseLossBackward0>)\n",
      "715 tensor(34.3250, grad_fn=<MseLossBackward0>)\n",
      "716 tensor(14.9621, grad_fn=<MseLossBackward0>)\n",
      "717 tensor(17.7770, grad_fn=<MseLossBackward0>)\n",
      "718 tensor(35.0492, grad_fn=<MseLossBackward0>)\n",
      "719 tensor(17.7647, grad_fn=<MseLossBackward0>)\n",
      "720 tensor(39.2950, grad_fn=<MseLossBackward0>)\n",
      "721 tensor(32.7278, grad_fn=<MseLossBackward0>)\n",
      "722 tensor(21.9850, grad_fn=<MseLossBackward0>)\n",
      "723 tensor(23.5916, grad_fn=<MseLossBackward0>)\n",
      "724 tensor(30.9395, grad_fn=<MseLossBackward0>)\n",
      "725 tensor(19.6022, grad_fn=<MseLossBackward0>)\n",
      "726 tensor(16.9174, grad_fn=<MseLossBackward0>)\n",
      "727 tensor(9.0970, grad_fn=<MseLossBackward0>)\n",
      "728 tensor(19.4957, grad_fn=<MseLossBackward0>)\n",
      "729 tensor(35.6327, grad_fn=<MseLossBackward0>)\n",
      "730 tensor(27.4335, grad_fn=<MseLossBackward0>)\n",
      "731 tensor(43.5834, grad_fn=<MseLossBackward0>)\n",
      "732 tensor(13.3491, grad_fn=<MseLossBackward0>)\n",
      "733 tensor(19.1693, grad_fn=<MseLossBackward0>)\n",
      "734 tensor(18.5881, grad_fn=<MseLossBackward0>)\n",
      "735 tensor(9.7985, grad_fn=<MseLossBackward0>)\n",
      "736 tensor(14.5823, grad_fn=<MseLossBackward0>)\n",
      "737 tensor(46.8396, grad_fn=<MseLossBackward0>)\n",
      "738 tensor(62.6466, grad_fn=<MseLossBackward0>)\n",
      "739 tensor(25.2246, grad_fn=<MseLossBackward0>)\n",
      "740 tensor(29.6005, grad_fn=<MseLossBackward0>)\n",
      "741 tensor(63.9040, grad_fn=<MseLossBackward0>)\n",
      "742 tensor(31.6082, grad_fn=<MseLossBackward0>)\n",
      "743 tensor(13.7696, grad_fn=<MseLossBackward0>)\n",
      "744 tensor(19.7337, grad_fn=<MseLossBackward0>)\n",
      "745 tensor(35.8761, grad_fn=<MseLossBackward0>)\n",
      "746 tensor(16.5015, grad_fn=<MseLossBackward0>)\n",
      "747 tensor(32.4476, grad_fn=<MseLossBackward0>)\n",
      "748 tensor(13.7861, grad_fn=<MseLossBackward0>)\n",
      "749 tensor(27.5352, grad_fn=<MseLossBackward0>)\n",
      "750 tensor(13.8229, grad_fn=<MseLossBackward0>)\n",
      "751 tensor(36.0159, grad_fn=<MseLossBackward0>)\n",
      "752 tensor(9.6006, grad_fn=<MseLossBackward0>)\n",
      "753 tensor(19.3652, grad_fn=<MseLossBackward0>)\n",
      "754 tensor(65.8235, grad_fn=<MseLossBackward0>)\n",
      "755 tensor(34.7103, grad_fn=<MseLossBackward0>)\n",
      "756 tensor(11.2885, grad_fn=<MseLossBackward0>)\n",
      "757 tensor(15.0163, grad_fn=<MseLossBackward0>)\n",
      "758 tensor(10.3175, grad_fn=<MseLossBackward0>)\n",
      "759 tensor(12.1767, grad_fn=<MseLossBackward0>)\n",
      "760 tensor(42.9841, grad_fn=<MseLossBackward0>)\n",
      "761 tensor(24.6736, grad_fn=<MseLossBackward0>)\n",
      "762 tensor(17.0225, grad_fn=<MseLossBackward0>)\n",
      "763 tensor(17.0985, grad_fn=<MseLossBackward0>)\n",
      "764 tensor(8.6898, grad_fn=<MseLossBackward0>)\n",
      "765 tensor(22.4250, grad_fn=<MseLossBackward0>)\n",
      "766 tensor(22.8934, grad_fn=<MseLossBackward0>)\n",
      "767 tensor(12.9112, grad_fn=<MseLossBackward0>)\n",
      "768 tensor(19.4750, grad_fn=<MseLossBackward0>)\n",
      "769 tensor(15.9163, grad_fn=<MseLossBackward0>)\n",
      "770 tensor(24.5065, grad_fn=<MseLossBackward0>)\n",
      "771 tensor(15.9498, grad_fn=<MseLossBackward0>)\n",
      "772 tensor(15.6378, grad_fn=<MseLossBackward0>)\n",
      "773 tensor(41.7154, grad_fn=<MseLossBackward0>)\n",
      "774 tensor(37.6393, grad_fn=<MseLossBackward0>)\n",
      "775 tensor(27.9849, grad_fn=<MseLossBackward0>)\n",
      "776 tensor(27.4709, grad_fn=<MseLossBackward0>)\n",
      "777 tensor(49.2664, grad_fn=<MseLossBackward0>)\n",
      "778 tensor(13.1071, grad_fn=<MseLossBackward0>)\n",
      "779 tensor(14.3541, grad_fn=<MseLossBackward0>)\n",
      "780 tensor(19.3324, grad_fn=<MseLossBackward0>)\n",
      "781 tensor(32.7623, grad_fn=<MseLossBackward0>)\n",
      "782 tensor(16.4069, grad_fn=<MseLossBackward0>)\n",
      "783 tensor(11.5852, grad_fn=<MseLossBackward0>)\n",
      "784 tensor(7.5489, grad_fn=<MseLossBackward0>)\n",
      "785 tensor(62.5840, grad_fn=<MseLossBackward0>)\n",
      "786 tensor(12.6102, grad_fn=<MseLossBackward0>)\n",
      "787 tensor(9.2555, grad_fn=<MseLossBackward0>)\n",
      "788 tensor(13.9560, grad_fn=<MseLossBackward0>)\n",
      "789 tensor(14.4965, grad_fn=<MseLossBackward0>)\n",
      "790 tensor(14.7015, grad_fn=<MseLossBackward0>)\n",
      "791 tensor(15.5438, grad_fn=<MseLossBackward0>)\n",
      "792 tensor(17.6337, grad_fn=<MseLossBackward0>)\n",
      "793 tensor(12.1548, grad_fn=<MseLossBackward0>)\n",
      "794 tensor(48.3916, grad_fn=<MseLossBackward0>)\n",
      "795 tensor(16.8704, grad_fn=<MseLossBackward0>)\n",
      "796 tensor(9.0254, grad_fn=<MseLossBackward0>)\n",
      "797 tensor(25.8937, grad_fn=<MseLossBackward0>)\n",
      "798 tensor(14.5376, grad_fn=<MseLossBackward0>)\n",
      "799 tensor(14.5382, grad_fn=<MseLossBackward0>)\n",
      "800 tensor(45.0608, grad_fn=<MseLossBackward0>)\n",
      "801 tensor(6.8237, grad_fn=<MseLossBackward0>)\n",
      "802 tensor(24.8421, grad_fn=<MseLossBackward0>)\n",
      "803 tensor(12.3548, grad_fn=<MseLossBackward0>)\n",
      "804 tensor(9.3822, grad_fn=<MseLossBackward0>)\n",
      "805 tensor(7.9176, grad_fn=<MseLossBackward0>)\n",
      "806 tensor(13.2466, grad_fn=<MseLossBackward0>)\n",
      "807 tensor(17.6719, grad_fn=<MseLossBackward0>)\n",
      "808 tensor(9.9354, grad_fn=<MseLossBackward0>)\n",
      "809 tensor(24.4097, grad_fn=<MseLossBackward0>)\n",
      "810 tensor(18.5686, grad_fn=<MseLossBackward0>)\n",
      "811 tensor(13.5960, grad_fn=<MseLossBackward0>)\n",
      "812 tensor(36.9060, grad_fn=<MseLossBackward0>)\n",
      "813 tensor(13.4578, grad_fn=<MseLossBackward0>)\n",
      "814 tensor(16.1706, grad_fn=<MseLossBackward0>)\n",
      "815 tensor(18.4997, grad_fn=<MseLossBackward0>)\n",
      "816 tensor(27.2063, grad_fn=<MseLossBackward0>)\n",
      "817 tensor(11.8729, grad_fn=<MseLossBackward0>)\n",
      "818 tensor(42.6938, grad_fn=<MseLossBackward0>)\n",
      "819 tensor(16.8714, grad_fn=<MseLossBackward0>)\n",
      "820 tensor(12.5216, grad_fn=<MseLossBackward0>)\n",
      "821 tensor(22.4790, grad_fn=<MseLossBackward0>)\n",
      "822 tensor(51.3274, grad_fn=<MseLossBackward0>)\n",
      "823 tensor(12.8460, grad_fn=<MseLossBackward0>)\n",
      "824 tensor(22.9158, grad_fn=<MseLossBackward0>)\n",
      "825 tensor(11.1770, grad_fn=<MseLossBackward0>)\n",
      "826 tensor(14.0066, grad_fn=<MseLossBackward0>)\n",
      "827 tensor(26.4947, grad_fn=<MseLossBackward0>)\n",
      "828 tensor(23.1497, grad_fn=<MseLossBackward0>)\n",
      "829 tensor(30.5578, grad_fn=<MseLossBackward0>)\n",
      "830 tensor(13.5065, grad_fn=<MseLossBackward0>)\n",
      "831 tensor(10.9260, grad_fn=<MseLossBackward0>)\n",
      "832 tensor(17.2341, grad_fn=<MseLossBackward0>)\n",
      "833 tensor(10.4914, grad_fn=<MseLossBackward0>)\n",
      "834 tensor(13.1101, grad_fn=<MseLossBackward0>)\n",
      "835 tensor(44.0298, grad_fn=<MseLossBackward0>)\n",
      "836 tensor(20.5072, grad_fn=<MseLossBackward0>)\n",
      "837 tensor(9.0812, grad_fn=<MseLossBackward0>)\n",
      "838 tensor(23.3458, grad_fn=<MseLossBackward0>)\n",
      "839 tensor(8.7360, grad_fn=<MseLossBackward0>)\n",
      "840 tensor(14.2581, grad_fn=<MseLossBackward0>)\n",
      "841 tensor(12.4698, grad_fn=<MseLossBackward0>)\n",
      "842 tensor(9.1721, grad_fn=<MseLossBackward0>)\n",
      "843 tensor(12.6808, grad_fn=<MseLossBackward0>)\n",
      "844 tensor(15.0495, grad_fn=<MseLossBackward0>)\n",
      "845 tensor(51.7526, grad_fn=<MseLossBackward0>)\n",
      "846 tensor(40.4895, grad_fn=<MseLossBackward0>)\n",
      "847 tensor(44.1059, grad_fn=<MseLossBackward0>)\n",
      "848 tensor(36.8301, grad_fn=<MseLossBackward0>)\n",
      "849 tensor(40.8745, grad_fn=<MseLossBackward0>)\n",
      "850 tensor(19.3633, grad_fn=<MseLossBackward0>)\n",
      "851 tensor(8.6436, grad_fn=<MseLossBackward0>)\n",
      "852 tensor(26.1637, grad_fn=<MseLossBackward0>)\n",
      "853 tensor(44.5622, grad_fn=<MseLossBackward0>)\n",
      "854 tensor(8.9412, grad_fn=<MseLossBackward0>)\n",
      "855 tensor(16.7501, grad_fn=<MseLossBackward0>)\n",
      "856 tensor(9.6619, grad_fn=<MseLossBackward0>)\n",
      "857 tensor(44.4585, grad_fn=<MseLossBackward0>)\n",
      "858 tensor(15.7823, grad_fn=<MseLossBackward0>)\n",
      "859 tensor(13.0293, grad_fn=<MseLossBackward0>)\n",
      "860 tensor(28.7700, grad_fn=<MseLossBackward0>)\n",
      "861 tensor(46.6978, grad_fn=<MseLossBackward0>)\n",
      "862 tensor(7.3237, grad_fn=<MseLossBackward0>)\n",
      "863 tensor(8.0833, grad_fn=<MseLossBackward0>)\n",
      "864 tensor(23.2563, grad_fn=<MseLossBackward0>)\n",
      "865 tensor(14.4809, grad_fn=<MseLossBackward0>)\n",
      "866 tensor(16.4733, grad_fn=<MseLossBackward0>)\n",
      "867 tensor(24.5306, grad_fn=<MseLossBackward0>)\n",
      "868 tensor(9.8309, grad_fn=<MseLossBackward0>)\n",
      "869 tensor(15.1103, grad_fn=<MseLossBackward0>)\n",
      "870 tensor(26.0280, grad_fn=<MseLossBackward0>)\n",
      "871 tensor(36.9769, grad_fn=<MseLossBackward0>)\n",
      "872 tensor(8.1819, grad_fn=<MseLossBackward0>)\n",
      "873 tensor(11.3296, grad_fn=<MseLossBackward0>)\n",
      "874 tensor(11.2420, grad_fn=<MseLossBackward0>)\n",
      "875 tensor(11.0692, grad_fn=<MseLossBackward0>)\n",
      "876 tensor(10.7408, grad_fn=<MseLossBackward0>)\n",
      "877 tensor(37.7707, grad_fn=<MseLossBackward0>)\n",
      "878 tensor(36.1635, grad_fn=<MseLossBackward0>)\n",
      "879 tensor(12.1910, grad_fn=<MseLossBackward0>)\n",
      "880 tensor(11.9126, grad_fn=<MseLossBackward0>)\n",
      "881 tensor(10.6765, grad_fn=<MseLossBackward0>)\n",
      "882 tensor(11.2325, grad_fn=<MseLossBackward0>)\n",
      "883 tensor(9.0943, grad_fn=<MseLossBackward0>)\n",
      "884 tensor(20.8726, grad_fn=<MseLossBackward0>)\n",
      "885 tensor(10.9818, grad_fn=<MseLossBackward0>)\n",
      "886 tensor(13.2743, grad_fn=<MseLossBackward0>)\n",
      "887 tensor(8.8969, grad_fn=<MseLossBackward0>)\n",
      "888 tensor(12.1520, grad_fn=<MseLossBackward0>)\n",
      "889 tensor(9.0988, grad_fn=<MseLossBackward0>)\n",
      "890 tensor(24.0838, grad_fn=<MseLossBackward0>)\n",
      "891 tensor(19.2625, grad_fn=<MseLossBackward0>)\n",
      "892 tensor(17.7641, grad_fn=<MseLossBackward0>)\n",
      "893 tensor(10.6008, grad_fn=<MseLossBackward0>)\n",
      "894 tensor(11.1127, grad_fn=<MseLossBackward0>)\n",
      "895 tensor(14.2216, grad_fn=<MseLossBackward0>)\n",
      "896 tensor(11.4886, grad_fn=<MseLossBackward0>)\n",
      "897 tensor(18.9820, grad_fn=<MseLossBackward0>)\n",
      "898 tensor(20.6463, grad_fn=<MseLossBackward0>)\n",
      "899 tensor(15.1364, grad_fn=<MseLossBackward0>)\n",
      "900 tensor(13.1103, grad_fn=<MseLossBackward0>)\n",
      "901 tensor(7.2428, grad_fn=<MseLossBackward0>)\n",
      "902 tensor(19.9948, grad_fn=<MseLossBackward0>)\n",
      "903 tensor(7.2059, grad_fn=<MseLossBackward0>)\n",
      "904 tensor(10.6137, grad_fn=<MseLossBackward0>)\n",
      "905 tensor(10.8345, grad_fn=<MseLossBackward0>)\n",
      "906 tensor(20.8787, grad_fn=<MseLossBackward0>)\n",
      "907 tensor(9.4199, grad_fn=<MseLossBackward0>)\n",
      "908 tensor(22.2238, grad_fn=<MseLossBackward0>)\n",
      "909 tensor(12.3933, grad_fn=<MseLossBackward0>)\n",
      "910 tensor(11.3515, grad_fn=<MseLossBackward0>)\n",
      "911 tensor(10.7047, grad_fn=<MseLossBackward0>)\n",
      "912 tensor(9.5831, grad_fn=<MseLossBackward0>)\n",
      "913 tensor(9.2698, grad_fn=<MseLossBackward0>)\n",
      "914 tensor(22.8639, grad_fn=<MseLossBackward0>)\n",
      "915 tensor(9.3871, grad_fn=<MseLossBackward0>)\n",
      "916 tensor(8.1055, grad_fn=<MseLossBackward0>)\n",
      "917 tensor(18.6248, grad_fn=<MseLossBackward0>)\n",
      "918 tensor(38.3163, grad_fn=<MseLossBackward0>)\n",
      "919 tensor(18.0365, grad_fn=<MseLossBackward0>)\n",
      "920 tensor(10.5788, grad_fn=<MseLossBackward0>)\n",
      "921 tensor(7.8731, grad_fn=<MseLossBackward0>)\n",
      "922 tensor(10.0309, grad_fn=<MseLossBackward0>)\n",
      "923 tensor(23.3260, grad_fn=<MseLossBackward0>)\n",
      "924 tensor(7.7774, grad_fn=<MseLossBackward0>)\n",
      "925 tensor(17.9352, grad_fn=<MseLossBackward0>)\n",
      "926 tensor(37.1493, grad_fn=<MseLossBackward0>)\n",
      "927 tensor(11.2871, grad_fn=<MseLossBackward0>)\n",
      "928 tensor(35.7417, grad_fn=<MseLossBackward0>)\n",
      "929 tensor(38.7207, grad_fn=<MseLossBackward0>)\n",
      "930 tensor(8.0031, grad_fn=<MseLossBackward0>)\n",
      "931 tensor(8.2958, grad_fn=<MseLossBackward0>)\n",
      "932 tensor(9.9099, grad_fn=<MseLossBackward0>)\n",
      "933 tensor(8.9902, grad_fn=<MseLossBackward0>)\n",
      "934 tensor(5.6557, grad_fn=<MseLossBackward0>)\n",
      "935 tensor(20.9104, grad_fn=<MseLossBackward0>)\n",
      "936 tensor(10.9702, grad_fn=<MseLossBackward0>)\n",
      "937 tensor(19.7521, grad_fn=<MseLossBackward0>)\n",
      "938 tensor(11.0678, grad_fn=<MseLossBackward0>)\n",
      "939 tensor(7.1627, grad_fn=<MseLossBackward0>)\n",
      "940 tensor(12.0364, grad_fn=<MseLossBackward0>)\n",
      "941 tensor(18.0068, grad_fn=<MseLossBackward0>)\n",
      "942 tensor(17.5091, grad_fn=<MseLossBackward0>)\n",
      "943 tensor(9.8774, grad_fn=<MseLossBackward0>)\n",
      "944 tensor(8.1954, grad_fn=<MseLossBackward0>)\n",
      "945 tensor(8.6060, grad_fn=<MseLossBackward0>)\n",
      "946 tensor(9.8157, grad_fn=<MseLossBackward0>)\n",
      "947 tensor(10.6548, grad_fn=<MseLossBackward0>)\n",
      "948 tensor(6.9220, grad_fn=<MseLossBackward0>)\n",
      "949 tensor(8.8918, grad_fn=<MseLossBackward0>)\n",
      "950 tensor(6.7091, grad_fn=<MseLossBackward0>)\n",
      "951 tensor(20.6412, grad_fn=<MseLossBackward0>)\n",
      "952 tensor(9.8221, grad_fn=<MseLossBackward0>)\n",
      "953 tensor(7.7438, grad_fn=<MseLossBackward0>)\n",
      "954 tensor(7.3837, grad_fn=<MseLossBackward0>)\n",
      "955 tensor(7.4466, grad_fn=<MseLossBackward0>)\n",
      "956 tensor(10.0812, grad_fn=<MseLossBackward0>)\n",
      "957 tensor(9.9549, grad_fn=<MseLossBackward0>)\n",
      "958 tensor(18.1319, grad_fn=<MseLossBackward0>)\n",
      "959 tensor(6.3763, grad_fn=<MseLossBackward0>)\n",
      "960 tensor(7.8949, grad_fn=<MseLossBackward0>)\n",
      "961 tensor(10.8358, grad_fn=<MseLossBackward0>)\n",
      "962 tensor(11.0884, grad_fn=<MseLossBackward0>)\n",
      "963 tensor(20.4081, grad_fn=<MseLossBackward0>)\n",
      "964 tensor(6.9619, grad_fn=<MseLossBackward0>)\n",
      "965 tensor(6.5007, grad_fn=<MseLossBackward0>)\n",
      "966 tensor(5.8274, grad_fn=<MseLossBackward0>)\n",
      "967 tensor(8.2692, grad_fn=<MseLossBackward0>)\n",
      "968 tensor(6.4866, grad_fn=<MseLossBackward0>)\n",
      "969 tensor(9.5449, grad_fn=<MseLossBackward0>)\n",
      "970 tensor(22.0282, grad_fn=<MseLossBackward0>)\n",
      "971 tensor(44.4696, grad_fn=<MseLossBackward0>)\n",
      "972 tensor(7.6583, grad_fn=<MseLossBackward0>)\n",
      "973 tensor(20.1616, grad_fn=<MseLossBackward0>)\n",
      "974 tensor(11.0638, grad_fn=<MseLossBackward0>)\n",
      "975 tensor(7.4822, grad_fn=<MseLossBackward0>)\n",
      "976 tensor(6.2351, grad_fn=<MseLossBackward0>)\n",
      "977 tensor(8.0037, grad_fn=<MseLossBackward0>)\n",
      "978 tensor(8.1860, grad_fn=<MseLossBackward0>)\n",
      "979 tensor(11.4944, grad_fn=<MseLossBackward0>)\n",
      "980 tensor(14.4112, grad_fn=<MseLossBackward0>)\n",
      "981 tensor(7.4113, grad_fn=<MseLossBackward0>)\n",
      "982 tensor(9.9905, grad_fn=<MseLossBackward0>)\n",
      "983 tensor(8.0033, grad_fn=<MseLossBackward0>)\n",
      "984 tensor(6.0986, grad_fn=<MseLossBackward0>)\n",
      "985 tensor(6.3460, grad_fn=<MseLossBackward0>)\n",
      "986 tensor(7.4134, grad_fn=<MseLossBackward0>)\n",
      "987 tensor(8.7447, grad_fn=<MseLossBackward0>)\n",
      "988 tensor(16.7050, grad_fn=<MseLossBackward0>)\n",
      "989 tensor(7.4327, grad_fn=<MseLossBackward0>)\n",
      "990 tensor(7.3850, grad_fn=<MseLossBackward0>)\n",
      "991 tensor(18.4276, grad_fn=<MseLossBackward0>)\n",
      "992 tensor(35.2114, grad_fn=<MseLossBackward0>)\n",
      "993 tensor(7.2467, grad_fn=<MseLossBackward0>)\n",
      "994 tensor(7.0793, grad_fn=<MseLossBackward0>)\n",
      "995 tensor(8.8917, grad_fn=<MseLossBackward0>)\n",
      "996 tensor(8.6886, grad_fn=<MseLossBackward0>)\n",
      "997 tensor(26.0606, grad_fn=<MseLossBackward0>)\n",
      "998 tensor(10.0131, grad_fn=<MseLossBackward0>)\n",
      "999 tensor(46.2861, grad_fn=<MseLossBackward0>)\n",
      "1000 tensor(16.4373, grad_fn=<MseLossBackward0>)\n",
      "1001 tensor(5.6857, grad_fn=<MseLossBackward0>)\n",
      "1002 tensor(7.7909, grad_fn=<MseLossBackward0>)\n",
      "1003 tensor(5.0911, grad_fn=<MseLossBackward0>)\n",
      "1004 tensor(5.9792, grad_fn=<MseLossBackward0>)\n",
      "1005 tensor(35.3172, grad_fn=<MseLossBackward0>)\n",
      "1006 tensor(8.5547, grad_fn=<MseLossBackward0>)\n",
      "1007 tensor(6.3793, grad_fn=<MseLossBackward0>)\n",
      "1008 tensor(15.6073, grad_fn=<MseLossBackward0>)\n",
      "1009 tensor(7.8714, grad_fn=<MseLossBackward0>)\n",
      "1010 tensor(32.6098, grad_fn=<MseLossBackward0>)\n",
      "1011 tensor(7.6548, grad_fn=<MseLossBackward0>)\n",
      "1012 tensor(15.0540, grad_fn=<MseLossBackward0>)\n",
      "1013 tensor(5.8606, grad_fn=<MseLossBackward0>)\n",
      "1014 tensor(7.2904, grad_fn=<MseLossBackward0>)\n",
      "1015 tensor(7.4476, grad_fn=<MseLossBackward0>)\n",
      "1016 tensor(9.6907, grad_fn=<MseLossBackward0>)\n",
      "1017 tensor(48.7650, grad_fn=<MseLossBackward0>)\n",
      "1018 tensor(13.2732, grad_fn=<MseLossBackward0>)\n",
      "1019 tensor(8.6767, grad_fn=<MseLossBackward0>)\n",
      "1020 tensor(9.1107, grad_fn=<MseLossBackward0>)\n",
      "1021 tensor(5.6764, grad_fn=<MseLossBackward0>)\n",
      "1022 tensor(6.7455, grad_fn=<MseLossBackward0>)\n",
      "1023 tensor(5.1465, grad_fn=<MseLossBackward0>)\n",
      "1024 tensor(33.2532, grad_fn=<MseLossBackward0>)\n",
      "1025 tensor(11.7474, grad_fn=<MseLossBackward0>)\n",
      "1026 tensor(14.6001, grad_fn=<MseLossBackward0>)\n",
      "1027 tensor(17.3347, grad_fn=<MseLossBackward0>)\n",
      "1028 tensor(9.3406, grad_fn=<MseLossBackward0>)\n",
      "1029 tensor(25.7511, grad_fn=<MseLossBackward0>)\n",
      "1030 tensor(12.9176, grad_fn=<MseLossBackward0>)\n",
      "1031 tensor(7.8025, grad_fn=<MseLossBackward0>)\n",
      "1032 tensor(5.9640, grad_fn=<MseLossBackward0>)\n",
      "1033 tensor(6.0778, grad_fn=<MseLossBackward0>)\n",
      "1034 tensor(35.8925, grad_fn=<MseLossBackward0>)\n",
      "1035 tensor(15.7443, grad_fn=<MseLossBackward0>)\n",
      "1036 tensor(7.0377, grad_fn=<MseLossBackward0>)\n",
      "1037 tensor(32.5547, grad_fn=<MseLossBackward0>)\n",
      "1038 tensor(7.3479, grad_fn=<MseLossBackward0>)\n",
      "1039 tensor(8.4352, grad_fn=<MseLossBackward0>)\n",
      "1040 tensor(9.0117, grad_fn=<MseLossBackward0>)\n",
      "1041 tensor(5.9328, grad_fn=<MseLossBackward0>)\n",
      "1042 tensor(14.8143, grad_fn=<MseLossBackward0>)\n",
      "1043 tensor(6.3729, grad_fn=<MseLossBackward0>)\n",
      "1044 tensor(7.4141, grad_fn=<MseLossBackward0>)\n",
      "1045 tensor(5.7262, grad_fn=<MseLossBackward0>)\n",
      "1046 tensor(34.4644, grad_fn=<MseLossBackward0>)\n",
      "1047 tensor(7.9899, grad_fn=<MseLossBackward0>)\n",
      "1048 tensor(32.5336, grad_fn=<MseLossBackward0>)\n",
      "1049 tensor(5.9434, grad_fn=<MseLossBackward0>)\n",
      "1050 tensor(6.9467, grad_fn=<MseLossBackward0>)\n",
      "1051 tensor(9.2539, grad_fn=<MseLossBackward0>)\n",
      "1052 tensor(6.7557, grad_fn=<MseLossBackward0>)\n",
      "1053 tensor(9.4682, grad_fn=<MseLossBackward0>)\n",
      "1054 tensor(33.1637, grad_fn=<MseLossBackward0>)\n",
      "1055 tensor(6.3147, grad_fn=<MseLossBackward0>)\n",
      "1056 tensor(4.0540, grad_fn=<MseLossBackward0>)\n",
      "1057 tensor(7.6574, grad_fn=<MseLossBackward0>)\n",
      "1058 tensor(6.6364, grad_fn=<MseLossBackward0>)\n",
      "1059 tensor(19.5380, grad_fn=<MseLossBackward0>)\n",
      "1060 tensor(5.1539, grad_fn=<MseLossBackward0>)\n",
      "1061 tensor(7.4038, grad_fn=<MseLossBackward0>)\n",
      "1062 tensor(5.4836, grad_fn=<MseLossBackward0>)\n",
      "1063 tensor(6.9617, grad_fn=<MseLossBackward0>)\n",
      "1064 tensor(10.2780, grad_fn=<MseLossBackward0>)\n",
      "1065 tensor(5.2426, grad_fn=<MseLossBackward0>)\n",
      "1066 tensor(6.9507, grad_fn=<MseLossBackward0>)\n",
      "1067 tensor(8.2905, grad_fn=<MseLossBackward0>)\n",
      "1068 tensor(8.1999, grad_fn=<MseLossBackward0>)\n",
      "1069 tensor(6.9429, grad_fn=<MseLossBackward0>)\n",
      "1070 tensor(5.0915, grad_fn=<MseLossBackward0>)\n",
      "1071 tensor(4.6998, grad_fn=<MseLossBackward0>)\n",
      "1072 tensor(6.2059, grad_fn=<MseLossBackward0>)\n",
      "1073 tensor(32.4861, grad_fn=<MseLossBackward0>)\n",
      "1074 tensor(8.1074, grad_fn=<MseLossBackward0>)\n",
      "1075 tensor(30.9141, grad_fn=<MseLossBackward0>)\n",
      "1076 tensor(5.5655, grad_fn=<MseLossBackward0>)\n",
      "1077 tensor(22.6212, grad_fn=<MseLossBackward0>)\n",
      "1078 tensor(7.5235, grad_fn=<MseLossBackward0>)\n",
      "1079 tensor(6.7963, grad_fn=<MseLossBackward0>)\n",
      "1080 tensor(14.3773, grad_fn=<MseLossBackward0>)\n",
      "1081 tensor(6.5814, grad_fn=<MseLossBackward0>)\n",
      "1082 tensor(7.6322, grad_fn=<MseLossBackward0>)\n",
      "1083 tensor(6.1426, grad_fn=<MseLossBackward0>)\n",
      "1084 tensor(6.8238, grad_fn=<MseLossBackward0>)\n",
      "1085 tensor(32.9425, grad_fn=<MseLossBackward0>)\n",
      "1086 tensor(7.6474, grad_fn=<MseLossBackward0>)\n",
      "1087 tensor(5.8640, grad_fn=<MseLossBackward0>)\n",
      "1088 tensor(31.0186, grad_fn=<MseLossBackward0>)\n",
      "1089 tensor(5.9943, grad_fn=<MseLossBackward0>)\n",
      "1090 tensor(5.6428, grad_fn=<MseLossBackward0>)\n",
      "1091 tensor(29.8835, grad_fn=<MseLossBackward0>)\n",
      "1092 tensor(12.4140, grad_fn=<MseLossBackward0>)\n",
      "1093 tensor(32.9550, grad_fn=<MseLossBackward0>)\n",
      "1094 tensor(5.0626, grad_fn=<MseLossBackward0>)\n",
      "1095 tensor(29.8950, grad_fn=<MseLossBackward0>)\n",
      "1096 tensor(6.1496, grad_fn=<MseLossBackward0>)\n",
      "1097 tensor(7.3662, grad_fn=<MseLossBackward0>)\n",
      "1098 tensor(8.3119, grad_fn=<MseLossBackward0>)\n",
      "1099 tensor(4.9504, grad_fn=<MseLossBackward0>)\n",
      "1100 tensor(7.5236, grad_fn=<MseLossBackward0>)\n",
      "1101 tensor(15.8983, grad_fn=<MseLossBackward0>)\n",
      "1102 tensor(4.6997, grad_fn=<MseLossBackward0>)\n",
      "1103 tensor(15.6597, grad_fn=<MseLossBackward0>)\n",
      "1104 tensor(13.9939, grad_fn=<MseLossBackward0>)\n",
      "1105 tensor(8.3741, grad_fn=<MseLossBackward0>)\n",
      "1106 tensor(15.0904, grad_fn=<MseLossBackward0>)\n",
      "1107 tensor(6.7824, grad_fn=<MseLossBackward0>)\n",
      "1108 tensor(3.9928, grad_fn=<MseLossBackward0>)\n",
      "1109 tensor(5.2720, grad_fn=<MseLossBackward0>)\n",
      "1110 tensor(7.5259, grad_fn=<MseLossBackward0>)\n",
      "1111 tensor(8.3847, grad_fn=<MseLossBackward0>)\n",
      "1112 tensor(22.5184, grad_fn=<MseLossBackward0>)\n",
      "1113 tensor(7.6236, grad_fn=<MseLossBackward0>)\n",
      "1114 tensor(6.6395, grad_fn=<MseLossBackward0>)\n",
      "1115 tensor(16.8678, grad_fn=<MseLossBackward0>)\n",
      "1116 tensor(7.8373, grad_fn=<MseLossBackward0>)\n",
      "1117 tensor(6.1308, grad_fn=<MseLossBackward0>)\n",
      "1118 tensor(8.4579, grad_fn=<MseLossBackward0>)\n",
      "1119 tensor(15.6818, grad_fn=<MseLossBackward0>)\n",
      "1120 tensor(7.2765, grad_fn=<MseLossBackward0>)\n",
      "1121 tensor(20.6495, grad_fn=<MseLossBackward0>)\n",
      "1122 tensor(8.3189, grad_fn=<MseLossBackward0>)\n",
      "1123 tensor(5.8095, grad_fn=<MseLossBackward0>)\n",
      "1124 tensor(5.7531, grad_fn=<MseLossBackward0>)\n",
      "1125 tensor(7.0991, grad_fn=<MseLossBackward0>)\n",
      "1126 tensor(6.1560, grad_fn=<MseLossBackward0>)\n",
      "1127 tensor(6.6345, grad_fn=<MseLossBackward0>)\n",
      "1128 tensor(6.6500, grad_fn=<MseLossBackward0>)\n",
      "1129 tensor(14.8234, grad_fn=<MseLossBackward0>)\n",
      "1130 tensor(12.8843, grad_fn=<MseLossBackward0>)\n",
      "1131 tensor(14.9071, grad_fn=<MseLossBackward0>)\n",
      "1132 tensor(14.6803, grad_fn=<MseLossBackward0>)\n",
      "1133 tensor(5.9221, grad_fn=<MseLossBackward0>)\n",
      "1134 tensor(11.4577, grad_fn=<MseLossBackward0>)\n",
      "1135 tensor(8.6678, grad_fn=<MseLossBackward0>)\n",
      "1136 tensor(13.3073, grad_fn=<MseLossBackward0>)\n",
      "1137 tensor(16.6185, grad_fn=<MseLossBackward0>)\n",
      "1138 tensor(6.6480, grad_fn=<MseLossBackward0>)\n",
      "1139 tensor(30.0261, grad_fn=<MseLossBackward0>)\n",
      "1140 tensor(5.0732, grad_fn=<MseLossBackward0>)\n",
      "1141 tensor(6.9567, grad_fn=<MseLossBackward0>)\n",
      "1142 tensor(14.3119, grad_fn=<MseLossBackward0>)\n",
      "1143 tensor(3.9891, grad_fn=<MseLossBackward0>)\n",
      "1144 tensor(6.1070, grad_fn=<MseLossBackward0>)\n",
      "1145 tensor(3.9874, grad_fn=<MseLossBackward0>)\n",
      "1146 tensor(11.6658, grad_fn=<MseLossBackward0>)\n",
      "1147 tensor(13.1096, grad_fn=<MseLossBackward0>)\n",
      "1148 tensor(6.1329, grad_fn=<MseLossBackward0>)\n",
      "1149 tensor(5.7092, grad_fn=<MseLossBackward0>)\n",
      "1150 tensor(4.4160, grad_fn=<MseLossBackward0>)\n",
      "1151 tensor(6.0484, grad_fn=<MseLossBackward0>)\n",
      "1152 tensor(33.2540, grad_fn=<MseLossBackward0>)\n",
      "1153 tensor(6.4508, grad_fn=<MseLossBackward0>)\n",
      "1154 tensor(6.4177, grad_fn=<MseLossBackward0>)\n",
      "1155 tensor(6.4680, grad_fn=<MseLossBackward0>)\n",
      "1156 tensor(6.7680, grad_fn=<MseLossBackward0>)\n",
      "1157 tensor(6.6940, grad_fn=<MseLossBackward0>)\n",
      "1158 tensor(4.8112, grad_fn=<MseLossBackward0>)\n",
      "1159 tensor(15.1183, grad_fn=<MseLossBackward0>)\n",
      "1160 tensor(12.6914, grad_fn=<MseLossBackward0>)\n",
      "1161 tensor(36.1638, grad_fn=<MseLossBackward0>)\n",
      "1162 tensor(12.5790, grad_fn=<MseLossBackward0>)\n",
      "1163 tensor(14.1250, grad_fn=<MseLossBackward0>)\n",
      "1164 tensor(12.8375, grad_fn=<MseLossBackward0>)\n",
      "1165 tensor(7.7714, grad_fn=<MseLossBackward0>)\n",
      "1166 tensor(29.0877, grad_fn=<MseLossBackward0>)\n",
      "1167 tensor(5.8033, grad_fn=<MseLossBackward0>)\n",
      "1168 tensor(33.8168, grad_fn=<MseLossBackward0>)\n",
      "1169 tensor(9.9918, grad_fn=<MseLossBackward0>)\n",
      "1170 tensor(5.1128, grad_fn=<MseLossBackward0>)\n",
      "1171 tensor(4.1378, grad_fn=<MseLossBackward0>)\n",
      "1172 tensor(5.4187, grad_fn=<MseLossBackward0>)\n",
      "1173 tensor(28.1054, grad_fn=<MseLossBackward0>)\n",
      "1174 tensor(6.0408, grad_fn=<MseLossBackward0>)\n",
      "1175 tensor(14.4816, grad_fn=<MseLossBackward0>)\n",
      "1176 tensor(40.2691, grad_fn=<MseLossBackward0>)\n",
      "1177 tensor(4.6953, grad_fn=<MseLossBackward0>)\n",
      "1178 tensor(10.5348, grad_fn=<MseLossBackward0>)\n",
      "1179 tensor(12.9975, grad_fn=<MseLossBackward0>)\n",
      "1180 tensor(14.1128, grad_fn=<MseLossBackward0>)\n",
      "1181 tensor(5.0752, grad_fn=<MseLossBackward0>)\n",
      "1182 tensor(13.2889, grad_fn=<MseLossBackward0>)\n",
      "1183 tensor(5.4788, grad_fn=<MseLossBackward0>)\n",
      "1184 tensor(4.3208, grad_fn=<MseLossBackward0>)\n",
      "1185 tensor(8.1305, grad_fn=<MseLossBackward0>)\n",
      "1186 tensor(7.2157, grad_fn=<MseLossBackward0>)\n",
      "1187 tensor(37.9016, grad_fn=<MseLossBackward0>)\n",
      "1188 tensor(21.3764, grad_fn=<MseLossBackward0>)\n",
      "1189 tensor(13.5249, grad_fn=<MseLossBackward0>)\n",
      "1190 tensor(7.7031, grad_fn=<MseLossBackward0>)\n",
      "1191 tensor(13.3882, grad_fn=<MseLossBackward0>)\n",
      "1192 tensor(5.8487, grad_fn=<MseLossBackward0>)\n",
      "1193 tensor(4.7755, grad_fn=<MseLossBackward0>)\n",
      "1194 tensor(5.7760, grad_fn=<MseLossBackward0>)\n",
      "1195 tensor(5.4649, grad_fn=<MseLossBackward0>)\n",
      "1196 tensor(6.0866, grad_fn=<MseLossBackward0>)\n",
      "1197 tensor(5.9624, grad_fn=<MseLossBackward0>)\n",
      "1198 tensor(4.5077, grad_fn=<MseLossBackward0>)\n",
      "1199 tensor(6.1548, grad_fn=<MseLossBackward0>)\n",
      "1200 tensor(7.4770, grad_fn=<MseLossBackward0>)\n",
      "1201 tensor(13.4892, grad_fn=<MseLossBackward0>)\n",
      "1202 tensor(5.5215, grad_fn=<MseLossBackward0>)\n",
      "1203 tensor(6.1100, grad_fn=<MseLossBackward0>)\n",
      "1204 tensor(6.5709, grad_fn=<MseLossBackward0>)\n",
      "1205 tensor(3.8284, grad_fn=<MseLossBackward0>)\n",
      "1206 tensor(11.0574, grad_fn=<MseLossBackward0>)\n",
      "1207 tensor(7.3020, grad_fn=<MseLossBackward0>)\n",
      "1208 tensor(5.2482, grad_fn=<MseLossBackward0>)\n",
      "1209 tensor(5.2383, grad_fn=<MseLossBackward0>)\n",
      "1210 tensor(14.0233, grad_fn=<MseLossBackward0>)\n",
      "1211 tensor(5.7407, grad_fn=<MseLossBackward0>)\n",
      "1212 tensor(4.1768, grad_fn=<MseLossBackward0>)\n",
      "1213 tensor(5.1866, grad_fn=<MseLossBackward0>)\n",
      "1214 tensor(4.6087, grad_fn=<MseLossBackward0>)\n",
      "1215 tensor(5.7577, grad_fn=<MseLossBackward0>)\n",
      "1216 tensor(5.2965, grad_fn=<MseLossBackward0>)\n",
      "1217 tensor(6.9451, grad_fn=<MseLossBackward0>)\n",
      "1218 tensor(5.5012, grad_fn=<MseLossBackward0>)\n",
      "1219 tensor(4.7507, grad_fn=<MseLossBackward0>)\n",
      "1220 tensor(4.2700, grad_fn=<MseLossBackward0>)\n",
      "1221 tensor(5.4924, grad_fn=<MseLossBackward0>)\n",
      "1222 tensor(11.3114, grad_fn=<MseLossBackward0>)\n",
      "1223 tensor(15.4946, grad_fn=<MseLossBackward0>)\n",
      "1224 tensor(4.2106, grad_fn=<MseLossBackward0>)\n",
      "1225 tensor(11.1213, grad_fn=<MseLossBackward0>)\n",
      "1226 tensor(6.5088, grad_fn=<MseLossBackward0>)\n",
      "1227 tensor(5.4236, grad_fn=<MseLossBackward0>)\n",
      "1228 tensor(11.4642, grad_fn=<MseLossBackward0>)\n",
      "1229 tensor(13.1043, grad_fn=<MseLossBackward0>)\n",
      "1230 tensor(18.1070, grad_fn=<MseLossBackward0>)\n",
      "1231 tensor(4.8861, grad_fn=<MseLossBackward0>)\n",
      "1232 tensor(14.7637, grad_fn=<MseLossBackward0>)\n",
      "1233 tensor(6.2014, grad_fn=<MseLossBackward0>)\n",
      "1234 tensor(18.4360, grad_fn=<MseLossBackward0>)\n",
      "1235 tensor(19.1462, grad_fn=<MseLossBackward0>)\n",
      "1236 tensor(12.8008, grad_fn=<MseLossBackward0>)\n",
      "1237 tensor(11.3405, grad_fn=<MseLossBackward0>)\n",
      "1238 tensor(3.6131, grad_fn=<MseLossBackward0>)\n",
      "1239 tensor(3.8106, grad_fn=<MseLossBackward0>)\n",
      "1240 tensor(5.3449, grad_fn=<MseLossBackward0>)\n",
      "1241 tensor(11.9288, grad_fn=<MseLossBackward0>)\n",
      "1242 tensor(4.6887, grad_fn=<MseLossBackward0>)\n",
      "1243 tensor(7.7352, grad_fn=<MseLossBackward0>)\n",
      "1244 tensor(6.2218, grad_fn=<MseLossBackward0>)\n",
      "1245 tensor(6.6340, grad_fn=<MseLossBackward0>)\n",
      "1246 tensor(3.9916, grad_fn=<MseLossBackward0>)\n",
      "1247 tensor(5.1380, grad_fn=<MseLossBackward0>)\n",
      "1248 tensor(4.6049, grad_fn=<MseLossBackward0>)\n",
      "1249 tensor(5.6226, grad_fn=<MseLossBackward0>)\n",
      "1250 tensor(3.5633, grad_fn=<MseLossBackward0>)\n",
      "1251 tensor(12.6700, grad_fn=<MseLossBackward0>)\n",
      "1252 tensor(5.8916, grad_fn=<MseLossBackward0>)\n",
      "1253 tensor(12.0790, grad_fn=<MseLossBackward0>)\n",
      "1254 tensor(5.8955, grad_fn=<MseLossBackward0>)\n",
      "1255 tensor(4.6917, grad_fn=<MseLossBackward0>)\n",
      "1256 tensor(6.0943, grad_fn=<MseLossBackward0>)\n",
      "1257 tensor(6.4018, grad_fn=<MseLossBackward0>)\n",
      "1258 tensor(26.1658, grad_fn=<MseLossBackward0>)\n",
      "1259 tensor(4.3588, grad_fn=<MseLossBackward0>)\n",
      "1260 tensor(5.1988, grad_fn=<MseLossBackward0>)\n",
      "1261 tensor(4.3923, grad_fn=<MseLossBackward0>)\n",
      "1262 tensor(6.5880, grad_fn=<MseLossBackward0>)\n",
      "1263 tensor(6.3639, grad_fn=<MseLossBackward0>)\n",
      "1264 tensor(4.9867, grad_fn=<MseLossBackward0>)\n",
      "1265 tensor(4.6046, grad_fn=<MseLossBackward0>)\n",
      "1266 tensor(5.2976, grad_fn=<MseLossBackward0>)\n",
      "1267 tensor(5.6807, grad_fn=<MseLossBackward0>)\n",
      "1268 tensor(11.1773, grad_fn=<MseLossBackward0>)\n",
      "1269 tensor(7.5085, grad_fn=<MseLossBackward0>)\n",
      "1270 tensor(6.1350, grad_fn=<MseLossBackward0>)\n",
      "1271 tensor(7.1583, grad_fn=<MseLossBackward0>)\n",
      "1272 tensor(5.4702, grad_fn=<MseLossBackward0>)\n",
      "1273 tensor(5.5314, grad_fn=<MseLossBackward0>)\n",
      "1274 tensor(5.6069, grad_fn=<MseLossBackward0>)\n",
      "1275 tensor(3.6387, grad_fn=<MseLossBackward0>)\n",
      "1276 tensor(7.8462, grad_fn=<MseLossBackward0>)\n",
      "1277 tensor(5.3674, grad_fn=<MseLossBackward0>)\n",
      "1278 tensor(11.5869, grad_fn=<MseLossBackward0>)\n",
      "1279 tensor(27.0570, grad_fn=<MseLossBackward0>)\n",
      "1280 tensor(5.2327, grad_fn=<MseLossBackward0>)\n",
      "1281 tensor(6.1302, grad_fn=<MseLossBackward0>)\n",
      "1282 tensor(15.1980, grad_fn=<MseLossBackward0>)\n",
      "1283 tensor(12.7628, grad_fn=<MseLossBackward0>)\n",
      "1284 tensor(6.6009, grad_fn=<MseLossBackward0>)\n",
      "1285 tensor(11.6123, grad_fn=<MseLossBackward0>)\n",
      "1286 tensor(27.6092, grad_fn=<MseLossBackward0>)\n",
      "1287 tensor(5.1789, grad_fn=<MseLossBackward0>)\n",
      "1288 tensor(5.5407, grad_fn=<MseLossBackward0>)\n",
      "1289 tensor(4.9395, grad_fn=<MseLossBackward0>)\n",
      "1290 tensor(32.1717, grad_fn=<MseLossBackward0>)\n",
      "1291 tensor(3.8799, grad_fn=<MseLossBackward0>)\n",
      "1292 tensor(5.1299, grad_fn=<MseLossBackward0>)\n",
      "1293 tensor(16.5183, grad_fn=<MseLossBackward0>)\n",
      "1294 tensor(6.9858, grad_fn=<MseLossBackward0>)\n",
      "1295 tensor(32.5236, grad_fn=<MseLossBackward0>)\n",
      "1296 tensor(13.2022, grad_fn=<MseLossBackward0>)\n",
      "1297 tensor(4.9124, grad_fn=<MseLossBackward0>)\n",
      "1298 tensor(6.1717, grad_fn=<MseLossBackward0>)\n",
      "1299 tensor(5.7380, grad_fn=<MseLossBackward0>)\n",
      "1300 tensor(32.4888, grad_fn=<MseLossBackward0>)\n",
      "1301 tensor(3.9599, grad_fn=<MseLossBackward0>)\n",
      "1302 tensor(5.1413, grad_fn=<MseLossBackward0>)\n",
      "1303 tensor(5.4976, grad_fn=<MseLossBackward0>)\n",
      "1304 tensor(4.8170, grad_fn=<MseLossBackward0>)\n",
      "1305 tensor(6.6058, grad_fn=<MseLossBackward0>)\n",
      "1306 tensor(5.0399, grad_fn=<MseLossBackward0>)\n",
      "1307 tensor(11.1493, grad_fn=<MseLossBackward0>)\n",
      "1308 tensor(3.7966, grad_fn=<MseLossBackward0>)\n",
      "1309 tensor(6.1590, grad_fn=<MseLossBackward0>)\n",
      "1310 tensor(12.9609, grad_fn=<MseLossBackward0>)\n",
      "1311 tensor(13.2129, grad_fn=<MseLossBackward0>)\n",
      "1312 tensor(9.2542, grad_fn=<MseLossBackward0>)\n",
      "1313 tensor(4.2589, grad_fn=<MseLossBackward0>)\n",
      "1314 tensor(11.9305, grad_fn=<MseLossBackward0>)\n",
      "1315 tensor(9.5732, grad_fn=<MseLossBackward0>)\n",
      "1316 tensor(4.3193, grad_fn=<MseLossBackward0>)\n",
      "1317 tensor(16.9309, grad_fn=<MseLossBackward0>)\n",
      "1318 tensor(27.9157, grad_fn=<MseLossBackward0>)\n",
      "1319 tensor(5.2869, grad_fn=<MseLossBackward0>)\n",
      "1320 tensor(4.9208, grad_fn=<MseLossBackward0>)\n",
      "1321 tensor(10.4589, grad_fn=<MseLossBackward0>)\n",
      "1322 tensor(10.7687, grad_fn=<MseLossBackward0>)\n",
      "1323 tensor(10.8907, grad_fn=<MseLossBackward0>)\n",
      "1324 tensor(11.8075, grad_fn=<MseLossBackward0>)\n",
      "1325 tensor(3.8524, grad_fn=<MseLossBackward0>)\n",
      "1326 tensor(7.0316, grad_fn=<MseLossBackward0>)\n",
      "1327 tensor(4.3831, grad_fn=<MseLossBackward0>)\n",
      "1328 tensor(31.9339, grad_fn=<MseLossBackward0>)\n",
      "1329 tensor(7.1581, grad_fn=<MseLossBackward0>)\n",
      "1330 tensor(5.5875, grad_fn=<MseLossBackward0>)\n",
      "1331 tensor(3.3815, grad_fn=<MseLossBackward0>)\n",
      "1332 tensor(4.2326, grad_fn=<MseLossBackward0>)\n",
      "1333 tensor(6.5630, grad_fn=<MseLossBackward0>)\n",
      "1334 tensor(14.6057, grad_fn=<MseLossBackward0>)\n",
      "1335 tensor(4.4200, grad_fn=<MseLossBackward0>)\n",
      "1336 tensor(6.4170, grad_fn=<MseLossBackward0>)\n",
      "1337 tensor(5.0933, grad_fn=<MseLossBackward0>)\n",
      "1338 tensor(4.2178, grad_fn=<MseLossBackward0>)\n",
      "1339 tensor(6.8350, grad_fn=<MseLossBackward0>)\n",
      "1340 tensor(32.0007, grad_fn=<MseLossBackward0>)\n",
      "1341 tensor(3.0973, grad_fn=<MseLossBackward0>)\n",
      "1342 tensor(4.9904, grad_fn=<MseLossBackward0>)\n",
      "1343 tensor(4.1056, grad_fn=<MseLossBackward0>)\n",
      "1344 tensor(4.7011, grad_fn=<MseLossBackward0>)\n",
      "1345 tensor(7.2394, grad_fn=<MseLossBackward0>)\n",
      "1346 tensor(6.0037, grad_fn=<MseLossBackward0>)\n",
      "1347 tensor(6.2294, grad_fn=<MseLossBackward0>)\n",
      "1348 tensor(6.2870, grad_fn=<MseLossBackward0>)\n",
      "1349 tensor(11.6107, grad_fn=<MseLossBackward0>)\n",
      "1350 tensor(5.2935, grad_fn=<MseLossBackward0>)\n",
      "1351 tensor(25.1313, grad_fn=<MseLossBackward0>)\n",
      "1352 tensor(5.2397, grad_fn=<MseLossBackward0>)\n",
      "1353 tensor(12.4668, grad_fn=<MseLossBackward0>)\n",
      "1354 tensor(5.0471, grad_fn=<MseLossBackward0>)\n",
      "1355 tensor(6.8245, grad_fn=<MseLossBackward0>)\n",
      "1356 tensor(7.8517, grad_fn=<MseLossBackward0>)\n",
      "1357 tensor(9.0503, grad_fn=<MseLossBackward0>)\n",
      "1358 tensor(3.9818, grad_fn=<MseLossBackward0>)\n",
      "1359 tensor(4.6527, grad_fn=<MseLossBackward0>)\n",
      "1360 tensor(5.1144, grad_fn=<MseLossBackward0>)\n",
      "1361 tensor(4.9431, grad_fn=<MseLossBackward0>)\n",
      "1362 tensor(4.8151, grad_fn=<MseLossBackward0>)\n",
      "1363 tensor(11.4727, grad_fn=<MseLossBackward0>)\n",
      "1364 tensor(5.2995, grad_fn=<MseLossBackward0>)\n",
      "1365 tensor(11.9305, grad_fn=<MseLossBackward0>)\n",
      "1366 tensor(10.4282, grad_fn=<MseLossBackward0>)\n",
      "1367 tensor(6.0835, grad_fn=<MseLossBackward0>)\n",
      "1368 tensor(3.8519, grad_fn=<MseLossBackward0>)\n",
      "1369 tensor(10.7108, grad_fn=<MseLossBackward0>)\n",
      "1370 tensor(8.8571, grad_fn=<MseLossBackward0>)\n",
      "1371 tensor(4.6007, grad_fn=<MseLossBackward0>)\n",
      "1372 tensor(6.8944, grad_fn=<MseLossBackward0>)\n",
      "1373 tensor(27.1361, grad_fn=<MseLossBackward0>)\n",
      "1374 tensor(5.4302, grad_fn=<MseLossBackward0>)\n",
      "1375 tensor(6.4219, grad_fn=<MseLossBackward0>)\n",
      "1376 tensor(5.4464, grad_fn=<MseLossBackward0>)\n",
      "1377 tensor(4.6229, grad_fn=<MseLossBackward0>)\n",
      "1378 tensor(3.8072, grad_fn=<MseLossBackward0>)\n",
      "1379 tensor(4.8946, grad_fn=<MseLossBackward0>)\n",
      "1380 tensor(5.6296, grad_fn=<MseLossBackward0>)\n",
      "1381 tensor(5.3689, grad_fn=<MseLossBackward0>)\n",
      "1382 tensor(4.1477, grad_fn=<MseLossBackward0>)\n",
      "1383 tensor(15.3462, grad_fn=<MseLossBackward0>)\n",
      "1384 tensor(13.2457, grad_fn=<MseLossBackward0>)\n",
      "1385 tensor(5.0549, grad_fn=<MseLossBackward0>)\n",
      "1386 tensor(5.4933, grad_fn=<MseLossBackward0>)\n",
      "1387 tensor(12.0349, grad_fn=<MseLossBackward0>)\n",
      "1388 tensor(4.0135, grad_fn=<MseLossBackward0>)\n",
      "1389 tensor(9.4173, grad_fn=<MseLossBackward0>)\n",
      "1390 tensor(3.9265, grad_fn=<MseLossBackward0>)\n",
      "1391 tensor(33.1752, grad_fn=<MseLossBackward0>)\n",
      "1392 tensor(3.9574, grad_fn=<MseLossBackward0>)\n",
      "1393 tensor(5.5679, grad_fn=<MseLossBackward0>)\n",
      "1394 tensor(26.7632, grad_fn=<MseLossBackward0>)\n",
      "1395 tensor(24.1626, grad_fn=<MseLossBackward0>)\n",
      "1396 tensor(4.8939, grad_fn=<MseLossBackward0>)\n",
      "1397 tensor(12.2364, grad_fn=<MseLossBackward0>)\n",
      "1398 tensor(5.1224, grad_fn=<MseLossBackward0>)\n",
      "1399 tensor(7.2125, grad_fn=<MseLossBackward0>)\n",
      "1400 tensor(27.7025, grad_fn=<MseLossBackward0>)\n",
      "1401 tensor(11.6746, grad_fn=<MseLossBackward0>)\n",
      "1402 tensor(4.8015, grad_fn=<MseLossBackward0>)\n",
      "1403 tensor(10.9746, grad_fn=<MseLossBackward0>)\n",
      "1404 tensor(16.6104, grad_fn=<MseLossBackward0>)\n",
      "1405 tensor(4.1246, grad_fn=<MseLossBackward0>)\n",
      "1406 tensor(4.6721, grad_fn=<MseLossBackward0>)\n",
      "1407 tensor(6.3996, grad_fn=<MseLossBackward0>)\n",
      "1408 tensor(29.9081, grad_fn=<MseLossBackward0>)\n",
      "1409 tensor(5.9435, grad_fn=<MseLossBackward0>)\n",
      "1410 tensor(5.6907, grad_fn=<MseLossBackward0>)\n",
      "1411 tensor(5.7314, grad_fn=<MseLossBackward0>)\n",
      "1412 tensor(6.2732, grad_fn=<MseLossBackward0>)\n",
      "1413 tensor(5.3138, grad_fn=<MseLossBackward0>)\n",
      "1414 tensor(4.0916, grad_fn=<MseLossBackward0>)\n",
      "1415 tensor(3.7863, grad_fn=<MseLossBackward0>)\n",
      "1416 tensor(10.4512, grad_fn=<MseLossBackward0>)\n",
      "1417 tensor(4.7110, grad_fn=<MseLossBackward0>)\n",
      "1418 tensor(3.8478, grad_fn=<MseLossBackward0>)\n",
      "1419 tensor(4.4176, grad_fn=<MseLossBackward0>)\n",
      "1420 tensor(11.3995, grad_fn=<MseLossBackward0>)\n",
      "1421 tensor(26.0168, grad_fn=<MseLossBackward0>)\n",
      "1422 tensor(4.1061, grad_fn=<MseLossBackward0>)\n",
      "1423 tensor(4.8180, grad_fn=<MseLossBackward0>)\n",
      "1424 tensor(24.2131, grad_fn=<MseLossBackward0>)\n",
      "1425 tensor(29.3843, grad_fn=<MseLossBackward0>)\n",
      "1426 tensor(5.4754, grad_fn=<MseLossBackward0>)\n",
      "1427 tensor(6.3780, grad_fn=<MseLossBackward0>)\n",
      "1428 tensor(4.6550, grad_fn=<MseLossBackward0>)\n",
      "1429 tensor(3.7032, grad_fn=<MseLossBackward0>)\n",
      "1430 tensor(5.3138, grad_fn=<MseLossBackward0>)\n",
      "1431 tensor(5.1371, grad_fn=<MseLossBackward0>)\n",
      "1432 tensor(12.4918, grad_fn=<MseLossBackward0>)\n",
      "1433 tensor(10.7607, grad_fn=<MseLossBackward0>)\n",
      "1434 tensor(4.1640, grad_fn=<MseLossBackward0>)\n",
      "1435 tensor(6.1401, grad_fn=<MseLossBackward0>)\n",
      "1436 tensor(5.1333, grad_fn=<MseLossBackward0>)\n",
      "1437 tensor(8.2197, grad_fn=<MseLossBackward0>)\n",
      "1438 tensor(4.5886, grad_fn=<MseLossBackward0>)\n",
      "1439 tensor(8.1285, grad_fn=<MseLossBackward0>)\n",
      "1440 tensor(5.0728, grad_fn=<MseLossBackward0>)\n",
      "1441 tensor(10.8775, grad_fn=<MseLossBackward0>)\n",
      "1442 tensor(25.9106, grad_fn=<MseLossBackward0>)\n",
      "1443 tensor(3.5267, grad_fn=<MseLossBackward0>)\n",
      "1444 tensor(13.2032, grad_fn=<MseLossBackward0>)\n",
      "1445 tensor(10.6805, grad_fn=<MseLossBackward0>)\n",
      "1446 tensor(11.3937, grad_fn=<MseLossBackward0>)\n",
      "1447 tensor(4.2802, grad_fn=<MseLossBackward0>)\n",
      "1448 tensor(8.8151, grad_fn=<MseLossBackward0>)\n",
      "1449 tensor(10.9219, grad_fn=<MseLossBackward0>)\n",
      "1450 tensor(10.9868, grad_fn=<MseLossBackward0>)\n",
      "1451 tensor(12.8239, grad_fn=<MseLossBackward0>)\n",
      "1452 tensor(5.2860, grad_fn=<MseLossBackward0>)\n",
      "1453 tensor(11.8799, grad_fn=<MseLossBackward0>)\n",
      "1454 tensor(11.5983, grad_fn=<MseLossBackward0>)\n",
      "1455 tensor(4.9627, grad_fn=<MseLossBackward0>)\n",
      "1456 tensor(13.1179, grad_fn=<MseLossBackward0>)\n",
      "1457 tensor(22.8504, grad_fn=<MseLossBackward0>)\n",
      "1458 tensor(23.3705, grad_fn=<MseLossBackward0>)\n",
      "1459 tensor(24.4273, grad_fn=<MseLossBackward0>)\n",
      "1460 tensor(12.2136, grad_fn=<MseLossBackward0>)\n",
      "1461 tensor(5.5683, grad_fn=<MseLossBackward0>)\n",
      "1462 tensor(5.0246, grad_fn=<MseLossBackward0>)\n",
      "1463 tensor(14.3924, grad_fn=<MseLossBackward0>)\n",
      "1464 tensor(6.2289, grad_fn=<MseLossBackward0>)\n",
      "1465 tensor(4.1971, grad_fn=<MseLossBackward0>)\n",
      "1466 tensor(3.8755, grad_fn=<MseLossBackward0>)\n",
      "1467 tensor(4.4054, grad_fn=<MseLossBackward0>)\n",
      "1468 tensor(24.8384, grad_fn=<MseLossBackward0>)\n",
      "1469 tensor(4.2697, grad_fn=<MseLossBackward0>)\n",
      "1470 tensor(5.4539, grad_fn=<MseLossBackward0>)\n",
      "1471 tensor(3.5414, grad_fn=<MseLossBackward0>)\n",
      "1472 tensor(4.3024, grad_fn=<MseLossBackward0>)\n",
      "1473 tensor(3.7084, grad_fn=<MseLossBackward0>)\n",
      "1474 tensor(10.5947, grad_fn=<MseLossBackward0>)\n",
      "1475 tensor(5.9154, grad_fn=<MseLossBackward0>)\n",
      "1476 tensor(5.0037, grad_fn=<MseLossBackward0>)\n",
      "1477 tensor(12.1472, grad_fn=<MseLossBackward0>)\n",
      "1478 tensor(3.2189, grad_fn=<MseLossBackward0>)\n",
      "1479 tensor(4.6511, grad_fn=<MseLossBackward0>)\n",
      "1480 tensor(3.7200, grad_fn=<MseLossBackward0>)\n",
      "1481 tensor(5.7616, grad_fn=<MseLossBackward0>)\n",
      "1482 tensor(5.8318, grad_fn=<MseLossBackward0>)\n",
      "1483 tensor(4.4944, grad_fn=<MseLossBackward0>)\n",
      "1484 tensor(22.8559, grad_fn=<MseLossBackward0>)\n",
      "1485 tensor(5.1931, grad_fn=<MseLossBackward0>)\n",
      "1486 tensor(3.7360, grad_fn=<MseLossBackward0>)\n",
      "1487 tensor(4.0418, grad_fn=<MseLossBackward0>)\n",
      "1488 tensor(5.2963, grad_fn=<MseLossBackward0>)\n",
      "1489 tensor(10.1150, grad_fn=<MseLossBackward0>)\n",
      "1490 tensor(5.0025, grad_fn=<MseLossBackward0>)\n",
      "1491 tensor(30.3739, grad_fn=<MseLossBackward0>)\n",
      "1492 tensor(4.7572, grad_fn=<MseLossBackward0>)\n",
      "1493 tensor(10.5112, grad_fn=<MseLossBackward0>)\n",
      "1494 tensor(5.3376, grad_fn=<MseLossBackward0>)\n",
      "1495 tensor(5.6929, grad_fn=<MseLossBackward0>)\n",
      "1496 tensor(10.4784, grad_fn=<MseLossBackward0>)\n",
      "1497 tensor(6.5725, grad_fn=<MseLossBackward0>)\n",
      "1498 tensor(3.6240, grad_fn=<MseLossBackward0>)\n",
      "1499 tensor(4.8927, grad_fn=<MseLossBackward0>)\n",
      "1500 tensor(5.6798, grad_fn=<MseLossBackward0>)\n",
      "1501 tensor(10.9658, grad_fn=<MseLossBackward0>)\n",
      "1502 tensor(4.1837, grad_fn=<MseLossBackward0>)\n",
      "1503 tensor(4.6022, grad_fn=<MseLossBackward0>)\n",
      "1504 tensor(23.4363, grad_fn=<MseLossBackward0>)\n",
      "1505 tensor(8.8430, grad_fn=<MseLossBackward0>)\n",
      "1506 tensor(9.8125, grad_fn=<MseLossBackward0>)\n",
      "1507 tensor(3.6947, grad_fn=<MseLossBackward0>)\n",
      "1508 tensor(3.3266, grad_fn=<MseLossBackward0>)\n",
      "1509 tensor(24.1938, grad_fn=<MseLossBackward0>)\n",
      "1510 tensor(14.7204, grad_fn=<MseLossBackward0>)\n",
      "1511 tensor(10.4011, grad_fn=<MseLossBackward0>)\n",
      "1512 tensor(6.1276, grad_fn=<MseLossBackward0>)\n",
      "1513 tensor(5.4614, grad_fn=<MseLossBackward0>)\n",
      "1514 tensor(4.9940, grad_fn=<MseLossBackward0>)\n",
      "1515 tensor(5.0980, grad_fn=<MseLossBackward0>)\n",
      "1516 tensor(3.9022, grad_fn=<MseLossBackward0>)\n",
      "1517 tensor(22.5473, grad_fn=<MseLossBackward0>)\n",
      "1518 tensor(9.4603, grad_fn=<MseLossBackward0>)\n",
      "1519 tensor(12.4270, grad_fn=<MseLossBackward0>)\n",
      "1520 tensor(4.2448, grad_fn=<MseLossBackward0>)\n",
      "1521 tensor(5.7474, grad_fn=<MseLossBackward0>)\n",
      "1522 tensor(28.1662, grad_fn=<MseLossBackward0>)\n",
      "1523 tensor(9.8110, grad_fn=<MseLossBackward0>)\n",
      "1524 tensor(5.9532, grad_fn=<MseLossBackward0>)\n",
      "1525 tensor(3.9067, grad_fn=<MseLossBackward0>)\n",
      "1526 tensor(4.7762, grad_fn=<MseLossBackward0>)\n",
      "1527 tensor(8.8023, grad_fn=<MseLossBackward0>)\n",
      "1528 tensor(3.9654, grad_fn=<MseLossBackward0>)\n",
      "1529 tensor(6.0306, grad_fn=<MseLossBackward0>)\n",
      "1530 tensor(5.0091, grad_fn=<MseLossBackward0>)\n",
      "1531 tensor(4.3090, grad_fn=<MseLossBackward0>)\n",
      "1532 tensor(3.0408, grad_fn=<MseLossBackward0>)\n",
      "1533 tensor(3.7401, grad_fn=<MseLossBackward0>)\n",
      "1534 tensor(11.7867, grad_fn=<MseLossBackward0>)\n",
      "1535 tensor(5.2969, grad_fn=<MseLossBackward0>)\n",
      "1536 tensor(10.1669, grad_fn=<MseLossBackward0>)\n",
      "1537 tensor(4.9428, grad_fn=<MseLossBackward0>)\n",
      "1538 tensor(3.7644, grad_fn=<MseLossBackward0>)\n",
      "1539 tensor(4.0410, grad_fn=<MseLossBackward0>)\n",
      "1540 tensor(6.6013, grad_fn=<MseLossBackward0>)\n",
      "1541 tensor(4.8446, grad_fn=<MseLossBackward0>)\n",
      "1542 tensor(11.6258, grad_fn=<MseLossBackward0>)\n",
      "1543 tensor(3.5929, grad_fn=<MseLossBackward0>)\n",
      "1544 tensor(3.1115, grad_fn=<MseLossBackward0>)\n",
      "1545 tensor(4.6361, grad_fn=<MseLossBackward0>)\n",
      "1546 tensor(12.0630, grad_fn=<MseLossBackward0>)\n",
      "1547 tensor(23.8572, grad_fn=<MseLossBackward0>)\n",
      "1548 tensor(4.8950, grad_fn=<MseLossBackward0>)\n",
      "1549 tensor(3.9868, grad_fn=<MseLossBackward0>)\n",
      "1550 tensor(9.7111, grad_fn=<MseLossBackward0>)\n",
      "1551 tensor(8.2862, grad_fn=<MseLossBackward0>)\n",
      "1552 tensor(5.1208, grad_fn=<MseLossBackward0>)\n",
      "1553 tensor(8.6059, grad_fn=<MseLossBackward0>)\n",
      "1554 tensor(23.2985, grad_fn=<MseLossBackward0>)\n",
      "1555 tensor(4.2281, grad_fn=<MseLossBackward0>)\n",
      "1556 tensor(3.7830, grad_fn=<MseLossBackward0>)\n",
      "1557 tensor(3.4231, grad_fn=<MseLossBackward0>)\n",
      "1558 tensor(3.6587, grad_fn=<MseLossBackward0>)\n",
      "1559 tensor(4.1733, grad_fn=<MseLossBackward0>)\n",
      "1560 tensor(22.0029, grad_fn=<MseLossBackward0>)\n",
      "1561 tensor(4.5128, grad_fn=<MseLossBackward0>)\n",
      "1562 tensor(26.5428, grad_fn=<MseLossBackward0>)\n",
      "1563 tensor(5.0379, grad_fn=<MseLossBackward0>)\n",
      "1564 tensor(4.8874, grad_fn=<MseLossBackward0>)\n",
      "1565 tensor(28.5495, grad_fn=<MseLossBackward0>)\n",
      "1566 tensor(22.6621, grad_fn=<MseLossBackward0>)\n",
      "1567 tensor(4.4650, grad_fn=<MseLossBackward0>)\n",
      "1568 tensor(11.3882, grad_fn=<MseLossBackward0>)\n",
      "1569 tensor(4.8589, grad_fn=<MseLossBackward0>)\n",
      "1570 tensor(9.7477, grad_fn=<MseLossBackward0>)\n",
      "1571 tensor(5.1391, grad_fn=<MseLossBackward0>)\n",
      "1572 tensor(3.8054, grad_fn=<MseLossBackward0>)\n",
      "1573 tensor(3.6003, grad_fn=<MseLossBackward0>)\n",
      "1574 tensor(22.7219, grad_fn=<MseLossBackward0>)\n",
      "1575 tensor(15.2569, grad_fn=<MseLossBackward0>)\n",
      "1576 tensor(4.2929, grad_fn=<MseLossBackward0>)\n",
      "1577 tensor(15.6621, grad_fn=<MseLossBackward0>)\n",
      "1578 tensor(10.3838, grad_fn=<MseLossBackward0>)\n",
      "1579 tensor(10.9459, grad_fn=<MseLossBackward0>)\n",
      "1580 tensor(9.5115, grad_fn=<MseLossBackward0>)\n",
      "1581 tensor(29.2539, grad_fn=<MseLossBackward0>)\n",
      "1582 tensor(4.5531, grad_fn=<MseLossBackward0>)\n",
      "1583 tensor(5.6071, grad_fn=<MseLossBackward0>)\n",
      "1584 tensor(4.5361, grad_fn=<MseLossBackward0>)\n",
      "1585 tensor(10.7591, grad_fn=<MseLossBackward0>)\n",
      "1586 tensor(22.2761, grad_fn=<MseLossBackward0>)\n",
      "1587 tensor(4.9557, grad_fn=<MseLossBackward0>)\n",
      "1588 tensor(9.1830, grad_fn=<MseLossBackward0>)\n",
      "1589 tensor(27.5880, grad_fn=<MseLossBackward0>)\n",
      "1590 tensor(4.6621, grad_fn=<MseLossBackward0>)\n",
      "1591 tensor(6.0131, grad_fn=<MseLossBackward0>)\n",
      "1592 tensor(3.1345, grad_fn=<MseLossBackward0>)\n",
      "1593 tensor(10.0550, grad_fn=<MseLossBackward0>)\n",
      "1594 tensor(14.4152, grad_fn=<MseLossBackward0>)\n",
      "1595 tensor(27.1172, grad_fn=<MseLossBackward0>)\n",
      "1596 tensor(3.5219, grad_fn=<MseLossBackward0>)\n",
      "1597 tensor(5.5923, grad_fn=<MseLossBackward0>)\n",
      "1598 tensor(6.1340, grad_fn=<MseLossBackward0>)\n",
      "1599 tensor(3.6274, grad_fn=<MseLossBackward0>)\n",
      "1600 tensor(5.5024, grad_fn=<MseLossBackward0>)\n",
      "1601 tensor(4.1514, grad_fn=<MseLossBackward0>)\n",
      "1602 tensor(11.9220, grad_fn=<MseLossBackward0>)\n",
      "1603 tensor(3.8523, grad_fn=<MseLossBackward0>)\n",
      "1604 tensor(3.9096, grad_fn=<MseLossBackward0>)\n",
      "1605 tensor(4.9160, grad_fn=<MseLossBackward0>)\n",
      "1606 tensor(10.3098, grad_fn=<MseLossBackward0>)\n",
      "1607 tensor(3.8544, grad_fn=<MseLossBackward0>)\n",
      "1608 tensor(8.2716, grad_fn=<MseLossBackward0>)\n",
      "1609 tensor(5.9596, grad_fn=<MseLossBackward0>)\n",
      "1610 tensor(5.5091, grad_fn=<MseLossBackward0>)\n",
      "1611 tensor(3.9700, grad_fn=<MseLossBackward0>)\n",
      "1612 tensor(4.4599, grad_fn=<MseLossBackward0>)\n",
      "1613 tensor(4.5678, grad_fn=<MseLossBackward0>)\n",
      "1614 tensor(9.0776, grad_fn=<MseLossBackward0>)\n",
      "1615 tensor(14.0803, grad_fn=<MseLossBackward0>)\n",
      "1616 tensor(10.2111, grad_fn=<MseLossBackward0>)\n",
      "1617 tensor(21.5064, grad_fn=<MseLossBackward0>)\n",
      "1618 tensor(4.4037, grad_fn=<MseLossBackward0>)\n",
      "1619 tensor(11.7492, grad_fn=<MseLossBackward0>)\n",
      "1620 tensor(29.2033, grad_fn=<MseLossBackward0>)\n",
      "1621 tensor(5.8807, grad_fn=<MseLossBackward0>)\n",
      "1622 tensor(10.7071, grad_fn=<MseLossBackward0>)\n",
      "1623 tensor(10.4772, grad_fn=<MseLossBackward0>)\n",
      "1624 tensor(4.5336, grad_fn=<MseLossBackward0>)\n",
      "1625 tensor(10.5979, grad_fn=<MseLossBackward0>)\n",
      "1626 tensor(22.1160, grad_fn=<MseLossBackward0>)\n",
      "1627 tensor(22.6570, grad_fn=<MseLossBackward0>)\n",
      "1628 tensor(4.0512, grad_fn=<MseLossBackward0>)\n",
      "1629 tensor(4.1846, grad_fn=<MseLossBackward0>)\n",
      "1630 tensor(3.2154, grad_fn=<MseLossBackward0>)\n",
      "1631 tensor(3.8122, grad_fn=<MseLossBackward0>)\n",
      "1632 tensor(5.3252, grad_fn=<MseLossBackward0>)\n",
      "1633 tensor(12.2987, grad_fn=<MseLossBackward0>)\n",
      "1634 tensor(24.3193, grad_fn=<MseLossBackward0>)\n",
      "1635 tensor(4.5285, grad_fn=<MseLossBackward0>)\n",
      "1636 tensor(26.3478, grad_fn=<MseLossBackward0>)\n",
      "1637 tensor(3.6121, grad_fn=<MseLossBackward0>)\n",
      "1638 tensor(3.9742, grad_fn=<MseLossBackward0>)\n",
      "1639 tensor(3.3125, grad_fn=<MseLossBackward0>)\n",
      "1640 tensor(5.8353, grad_fn=<MseLossBackward0>)\n",
      "1641 tensor(4.4914, grad_fn=<MseLossBackward0>)\n",
      "1642 tensor(8.8471, grad_fn=<MseLossBackward0>)\n",
      "1643 tensor(4.3924, grad_fn=<MseLossBackward0>)\n",
      "1644 tensor(4.9572, grad_fn=<MseLossBackward0>)\n",
      "1645 tensor(8.9497, grad_fn=<MseLossBackward0>)\n",
      "1646 tensor(10.8193, grad_fn=<MseLossBackward0>)\n",
      "1647 tensor(4.2510, grad_fn=<MseLossBackward0>)\n",
      "1648 tensor(5.0080, grad_fn=<MseLossBackward0>)\n",
      "1649 tensor(8.9645, grad_fn=<MseLossBackward0>)\n",
      "1650 tensor(2.9853, grad_fn=<MseLossBackward0>)\n",
      "1651 tensor(4.0595, grad_fn=<MseLossBackward0>)\n",
      "1652 tensor(3.8076, grad_fn=<MseLossBackward0>)\n",
      "1653 tensor(4.8864, grad_fn=<MseLossBackward0>)\n",
      "1654 tensor(4.7420, grad_fn=<MseLossBackward0>)\n",
      "1655 tensor(4.9199, grad_fn=<MseLossBackward0>)\n",
      "1656 tensor(27.1486, grad_fn=<MseLossBackward0>)\n",
      "1657 tensor(4.4455, grad_fn=<MseLossBackward0>)\n",
      "1658 tensor(4.9851, grad_fn=<MseLossBackward0>)\n",
      "1659 tensor(21.6867, grad_fn=<MseLossBackward0>)\n",
      "1660 tensor(5.5882, grad_fn=<MseLossBackward0>)\n",
      "1661 tensor(4.5659, grad_fn=<MseLossBackward0>)\n",
      "1662 tensor(23.0817, grad_fn=<MseLossBackward0>)\n",
      "1663 tensor(4.4047, grad_fn=<MseLossBackward0>)\n",
      "1664 tensor(3.7694, grad_fn=<MseLossBackward0>)\n",
      "1665 tensor(11.2677, grad_fn=<MseLossBackward0>)\n",
      "1666 tensor(5.1649, grad_fn=<MseLossBackward0>)\n",
      "1667 tensor(3.8299, grad_fn=<MseLossBackward0>)\n",
      "1668 tensor(15.5940, grad_fn=<MseLossBackward0>)\n",
      "1669 tensor(9.5134, grad_fn=<MseLossBackward0>)\n",
      "1670 tensor(3.3183, grad_fn=<MseLossBackward0>)\n",
      "1671 tensor(5.9899, grad_fn=<MseLossBackward0>)\n",
      "1672 tensor(4.7928, grad_fn=<MseLossBackward0>)\n",
      "1673 tensor(4.9030, grad_fn=<MseLossBackward0>)\n",
      "1674 tensor(21.5890, grad_fn=<MseLossBackward0>)\n",
      "1675 tensor(10.4783, grad_fn=<MseLossBackward0>)\n",
      "1676 tensor(5.0441, grad_fn=<MseLossBackward0>)\n",
      "1677 tensor(4.3205, grad_fn=<MseLossBackward0>)\n",
      "1678 tensor(11.9675, grad_fn=<MseLossBackward0>)\n",
      "1679 tensor(4.5610, grad_fn=<MseLossBackward0>)\n",
      "1680 tensor(3.9676, grad_fn=<MseLossBackward0>)\n",
      "1681 tensor(11.1467, grad_fn=<MseLossBackward0>)\n",
      "1682 tensor(10.9293, grad_fn=<MseLossBackward0>)\n",
      "1683 tensor(3.8151, grad_fn=<MseLossBackward0>)\n",
      "1684 tensor(3.4949, grad_fn=<MseLossBackward0>)\n",
      "1685 tensor(10.1565, grad_fn=<MseLossBackward0>)\n",
      "1686 tensor(2.6529, grad_fn=<MseLossBackward0>)\n",
      "1687 tensor(3.9577, grad_fn=<MseLossBackward0>)\n",
      "1688 tensor(3.0890, grad_fn=<MseLossBackward0>)\n",
      "1689 tensor(2.8732, grad_fn=<MseLossBackward0>)\n",
      "1690 tensor(4.6769, grad_fn=<MseLossBackward0>)\n",
      "1691 tensor(3.1004, grad_fn=<MseLossBackward0>)\n",
      "1692 tensor(20.8052, grad_fn=<MseLossBackward0>)\n",
      "1693 tensor(21.6935, grad_fn=<MseLossBackward0>)\n",
      "1694 tensor(4.4531, grad_fn=<MseLossBackward0>)\n",
      "1695 tensor(10.3592, grad_fn=<MseLossBackward0>)\n",
      "1696 tensor(4.0998, grad_fn=<MseLossBackward0>)\n",
      "1697 tensor(5.1013, grad_fn=<MseLossBackward0>)\n",
      "1698 tensor(4.5682, grad_fn=<MseLossBackward0>)\n",
      "1699 tensor(4.5428, grad_fn=<MseLossBackward0>)\n",
      "1700 tensor(4.3723, grad_fn=<MseLossBackward0>)\n",
      "1701 tensor(3.4268, grad_fn=<MseLossBackward0>)\n",
      "1702 tensor(4.3758, grad_fn=<MseLossBackward0>)\n",
      "1703 tensor(5.0751, grad_fn=<MseLossBackward0>)\n",
      "1704 tensor(15.1449, grad_fn=<MseLossBackward0>)\n",
      "1705 tensor(9.5183, grad_fn=<MseLossBackward0>)\n",
      "1706 tensor(5.5894, grad_fn=<MseLossBackward0>)\n",
      "1707 tensor(5.4230, grad_fn=<MseLossBackward0>)\n",
      "1708 tensor(5.7169, grad_fn=<MseLossBackward0>)\n",
      "1709 tensor(4.6422, grad_fn=<MseLossBackward0>)\n",
      "1710 tensor(2.8054, grad_fn=<MseLossBackward0>)\n",
      "1711 tensor(3.5214, grad_fn=<MseLossBackward0>)\n",
      "1712 tensor(5.7893, grad_fn=<MseLossBackward0>)\n",
      "1713 tensor(10.5424, grad_fn=<MseLossBackward0>)\n",
      "1714 tensor(8.6131, grad_fn=<MseLossBackward0>)\n",
      "1715 tensor(10.6695, grad_fn=<MseLossBackward0>)\n",
      "1716 tensor(4.6750, grad_fn=<MseLossBackward0>)\n",
      "1717 tensor(4.1665, grad_fn=<MseLossBackward0>)\n",
      "1718 tensor(9.4755, grad_fn=<MseLossBackward0>)\n",
      "1719 tensor(3.8909, grad_fn=<MseLossBackward0>)\n",
      "1720 tensor(4.7274, grad_fn=<MseLossBackward0>)\n",
      "1721 tensor(20.2533, grad_fn=<MseLossBackward0>)\n",
      "1722 tensor(10.0009, grad_fn=<MseLossBackward0>)\n",
      "1723 tensor(25.4762, grad_fn=<MseLossBackward0>)\n",
      "1724 tensor(3.9254, grad_fn=<MseLossBackward0>)\n",
      "1725 tensor(2.9726, grad_fn=<MseLossBackward0>)\n",
      "1726 tensor(3.5283, grad_fn=<MseLossBackward0>)\n",
      "1727 tensor(8.2206, grad_fn=<MseLossBackward0>)\n",
      "1728 tensor(26.0352, grad_fn=<MseLossBackward0>)\n",
      "1729 tensor(4.7414, grad_fn=<MseLossBackward0>)\n",
      "1730 tensor(3.9614, grad_fn=<MseLossBackward0>)\n",
      "1731 tensor(4.1357, grad_fn=<MseLossBackward0>)\n",
      "1732 tensor(3.5773, grad_fn=<MseLossBackward0>)\n",
      "1733 tensor(4.2512, grad_fn=<MseLossBackward0>)\n",
      "1734 tensor(9.2779, grad_fn=<MseLossBackward0>)\n",
      "1735 tensor(4.5840, grad_fn=<MseLossBackward0>)\n",
      "1736 tensor(4.2596, grad_fn=<MseLossBackward0>)\n",
      "1737 tensor(4.2290, grad_fn=<MseLossBackward0>)\n",
      "1738 tensor(9.1931, grad_fn=<MseLossBackward0>)\n",
      "1739 tensor(20.9333, grad_fn=<MseLossBackward0>)\n",
      "1740 tensor(8.8419, grad_fn=<MseLossBackward0>)\n",
      "1741 tensor(4.3867, grad_fn=<MseLossBackward0>)\n",
      "1742 tensor(3.5528, grad_fn=<MseLossBackward0>)\n",
      "1743 tensor(10.0528, grad_fn=<MseLossBackward0>)\n",
      "1744 tensor(7.8711, grad_fn=<MseLossBackward0>)\n",
      "1745 tensor(4.1945, grad_fn=<MseLossBackward0>)\n",
      "1746 tensor(20.4520, grad_fn=<MseLossBackward0>)\n",
      "1747 tensor(3.8826, grad_fn=<MseLossBackward0>)\n",
      "1748 tensor(19.9582, grad_fn=<MseLossBackward0>)\n",
      "1749 tensor(5.4901, grad_fn=<MseLossBackward0>)\n",
      "1750 tensor(8.1721, grad_fn=<MseLossBackward0>)\n",
      "1751 tensor(4.8597, grad_fn=<MseLossBackward0>)\n",
      "1752 tensor(4.0707, grad_fn=<MseLossBackward0>)\n",
      "1753 tensor(14.1803, grad_fn=<MseLossBackward0>)\n",
      "1754 tensor(4.1485, grad_fn=<MseLossBackward0>)\n",
      "1755 tensor(3.9637, grad_fn=<MseLossBackward0>)\n",
      "1756 tensor(3.3408, grad_fn=<MseLossBackward0>)\n",
      "1757 tensor(3.5440, grad_fn=<MseLossBackward0>)\n",
      "1758 tensor(5.1876, grad_fn=<MseLossBackward0>)\n",
      "1759 tensor(4.7094, grad_fn=<MseLossBackward0>)\n",
      "1760 tensor(3.9979, grad_fn=<MseLossBackward0>)\n",
      "1761 tensor(6.1149, grad_fn=<MseLossBackward0>)\n",
      "1762 tensor(21.3608, grad_fn=<MseLossBackward0>)\n",
      "1763 tensor(3.3343, grad_fn=<MseLossBackward0>)\n",
      "1764 tensor(3.4798, grad_fn=<MseLossBackward0>)\n",
      "1765 tensor(3.3580, grad_fn=<MseLossBackward0>)\n",
      "1766 tensor(3.4791, grad_fn=<MseLossBackward0>)\n",
      "1767 tensor(9.8958, grad_fn=<MseLossBackward0>)\n",
      "1768 tensor(4.5484, grad_fn=<MseLossBackward0>)\n",
      "1769 tensor(9.8210, grad_fn=<MseLossBackward0>)\n",
      "1770 tensor(9.6988, grad_fn=<MseLossBackward0>)\n",
      "1771 tensor(3.8783, grad_fn=<MseLossBackward0>)\n",
      "1772 tensor(4.8938, grad_fn=<MseLossBackward0>)\n",
      "1773 tensor(2.9213, grad_fn=<MseLossBackward0>)\n",
      "1774 tensor(4.8757, grad_fn=<MseLossBackward0>)\n",
      "1775 tensor(15.1830, grad_fn=<MseLossBackward0>)\n",
      "1776 tensor(14.5614, grad_fn=<MseLossBackward0>)\n",
      "1777 tensor(9.2060, grad_fn=<MseLossBackward0>)\n",
      "1778 tensor(3.3725, grad_fn=<MseLossBackward0>)\n",
      "1779 tensor(13.5422, grad_fn=<MseLossBackward0>)\n",
      "1780 tensor(2.7951, grad_fn=<MseLossBackward0>)\n",
      "1781 tensor(10.2242, grad_fn=<MseLossBackward0>)\n",
      "1782 tensor(20.1046, grad_fn=<MseLossBackward0>)\n",
      "1783 tensor(8.8582, grad_fn=<MseLossBackward0>)\n",
      "1784 tensor(4.5036, grad_fn=<MseLossBackward0>)\n",
      "1785 tensor(2.8618, grad_fn=<MseLossBackward0>)\n",
      "1786 tensor(8.3379, grad_fn=<MseLossBackward0>)\n",
      "1787 tensor(4.0128, grad_fn=<MseLossBackward0>)\n",
      "1788 tensor(4.4538, grad_fn=<MseLossBackward0>)\n",
      "1789 tensor(20.2741, grad_fn=<MseLossBackward0>)\n",
      "1790 tensor(2.9665, grad_fn=<MseLossBackward0>)\n",
      "1791 tensor(8.6214, grad_fn=<MseLossBackward0>)\n",
      "1792 tensor(4.4673, grad_fn=<MseLossBackward0>)\n",
      "1793 tensor(6.3198, grad_fn=<MseLossBackward0>)\n",
      "1794 tensor(3.2185, grad_fn=<MseLossBackward0>)\n",
      "1795 tensor(13.3406, grad_fn=<MseLossBackward0>)\n",
      "1796 tensor(4.6214, grad_fn=<MseLossBackward0>)\n",
      "1797 tensor(10.3071, grad_fn=<MseLossBackward0>)\n",
      "1798 tensor(19.2203, grad_fn=<MseLossBackward0>)\n",
      "1799 tensor(7.9603, grad_fn=<MseLossBackward0>)\n",
      "1800 tensor(9.5763, grad_fn=<MseLossBackward0>)\n",
      "1801 tensor(8.5091, grad_fn=<MseLossBackward0>)\n",
      "1802 tensor(25.5824, grad_fn=<MseLossBackward0>)\n",
      "1803 tensor(8.6637, grad_fn=<MseLossBackward0>)\n",
      "1804 tensor(4.2076, grad_fn=<MseLossBackward0>)\n",
      "1805 tensor(4.5545, grad_fn=<MseLossBackward0>)\n",
      "1806 tensor(10.2091, grad_fn=<MseLossBackward0>)\n",
      "1807 tensor(9.8280, grad_fn=<MseLossBackward0>)\n",
      "1808 tensor(7.9931, grad_fn=<MseLossBackward0>)\n",
      "1809 tensor(3.3524, grad_fn=<MseLossBackward0>)\n",
      "1810 tensor(3.2860, grad_fn=<MseLossBackward0>)\n",
      "1811 tensor(2.9163, grad_fn=<MseLossBackward0>)\n",
      "1812 tensor(7.6004, grad_fn=<MseLossBackward0>)\n",
      "1813 tensor(8.4330, grad_fn=<MseLossBackward0>)\n",
      "1814 tensor(4.7878, grad_fn=<MseLossBackward0>)\n",
      "1815 tensor(4.6560, grad_fn=<MseLossBackward0>)\n",
      "1816 tensor(3.2439, grad_fn=<MseLossBackward0>)\n",
      "1817 tensor(4.6989, grad_fn=<MseLossBackward0>)\n",
      "1818 tensor(4.7144, grad_fn=<MseLossBackward0>)\n",
      "1819 tensor(25.8161, grad_fn=<MseLossBackward0>)\n",
      "1820 tensor(3.2525, grad_fn=<MseLossBackward0>)\n",
      "1821 tensor(24.7003, grad_fn=<MseLossBackward0>)\n",
      "1822 tensor(8.8134, grad_fn=<MseLossBackward0>)\n",
      "1823 tensor(4.9302, grad_fn=<MseLossBackward0>)\n",
      "1824 tensor(9.7587, grad_fn=<MseLossBackward0>)\n",
      "1825 tensor(10.0533, grad_fn=<MseLossBackward0>)\n",
      "1826 tensor(7.5822, grad_fn=<MseLossBackward0>)\n",
      "1827 tensor(3.4530, grad_fn=<MseLossBackward0>)\n",
      "1828 tensor(8.1113, grad_fn=<MseLossBackward0>)\n",
      "1829 tensor(2.5873, grad_fn=<MseLossBackward0>)\n",
      "1830 tensor(2.9467, grad_fn=<MseLossBackward0>)\n",
      "1831 tensor(19.0915, grad_fn=<MseLossBackward0>)\n",
      "1832 tensor(4.0845, grad_fn=<MseLossBackward0>)\n",
      "1833 tensor(4.2620, grad_fn=<MseLossBackward0>)\n",
      "1834 tensor(10.1427, grad_fn=<MseLossBackward0>)\n",
      "1835 tensor(4.0001, grad_fn=<MseLossBackward0>)\n",
      "1836 tensor(9.8420, grad_fn=<MseLossBackward0>)\n",
      "1837 tensor(4.2907, grad_fn=<MseLossBackward0>)\n",
      "1838 tensor(3.4439, grad_fn=<MseLossBackward0>)\n",
      "1839 tensor(3.3966, grad_fn=<MseLossBackward0>)\n",
      "1840 tensor(5.3018, grad_fn=<MseLossBackward0>)\n",
      "1841 tensor(3.2852, grad_fn=<MseLossBackward0>)\n",
      "1842 tensor(4.0561, grad_fn=<MseLossBackward0>)\n",
      "1843 tensor(7.9221, grad_fn=<MseLossBackward0>)\n",
      "1844 tensor(4.6308, grad_fn=<MseLossBackward0>)\n",
      "1845 tensor(2.5450, grad_fn=<MseLossBackward0>)\n",
      "1846 tensor(3.6117, grad_fn=<MseLossBackward0>)\n",
      "1847 tensor(7.9448, grad_fn=<MseLossBackward0>)\n",
      "1848 tensor(9.7279, grad_fn=<MseLossBackward0>)\n",
      "1849 tensor(2.9407, grad_fn=<MseLossBackward0>)\n",
      "1850 tensor(9.1548, grad_fn=<MseLossBackward0>)\n",
      "1851 tensor(9.0001, grad_fn=<MseLossBackward0>)\n",
      "1852 tensor(3.2398, grad_fn=<MseLossBackward0>)\n",
      "1853 tensor(3.8808, grad_fn=<MseLossBackward0>)\n",
      "1854 tensor(4.6030, grad_fn=<MseLossBackward0>)\n",
      "1855 tensor(7.8463, grad_fn=<MseLossBackward0>)\n",
      "1856 tensor(8.3343, grad_fn=<MseLossBackward0>)\n",
      "1857 tensor(3.2876, grad_fn=<MseLossBackward0>)\n",
      "1858 tensor(9.4641, grad_fn=<MseLossBackward0>)\n",
      "1859 tensor(3.6900, grad_fn=<MseLossBackward0>)\n",
      "1860 tensor(25.5946, grad_fn=<MseLossBackward0>)\n",
      "1861 tensor(4.8531, grad_fn=<MseLossBackward0>)\n",
      "1862 tensor(2.8533, grad_fn=<MseLossBackward0>)\n",
      "1863 tensor(2.7595, grad_fn=<MseLossBackward0>)\n",
      "1864 tensor(9.4780, grad_fn=<MseLossBackward0>)\n",
      "1865 tensor(3.6043, grad_fn=<MseLossBackward0>)\n",
      "1866 tensor(4.6089, grad_fn=<MseLossBackward0>)\n",
      "1867 tensor(8.5085, grad_fn=<MseLossBackward0>)\n",
      "1868 tensor(29.2966, grad_fn=<MseLossBackward0>)\n",
      "1869 tensor(13.6030, grad_fn=<MseLossBackward0>)\n",
      "1870 tensor(4.7177, grad_fn=<MseLossBackward0>)\n",
      "1871 tensor(9.0871, grad_fn=<MseLossBackward0>)\n",
      "1872 tensor(3.3701, grad_fn=<MseLossBackward0>)\n",
      "1873 tensor(4.2215, grad_fn=<MseLossBackward0>)\n",
      "1874 tensor(20.9718, grad_fn=<MseLossBackward0>)\n",
      "1875 tensor(7.9499, grad_fn=<MseLossBackward0>)\n",
      "1876 tensor(5.1287, grad_fn=<MseLossBackward0>)\n",
      "1877 tensor(4.3362, grad_fn=<MseLossBackward0>)\n",
      "1878 tensor(3.1185, grad_fn=<MseLossBackward0>)\n",
      "1879 tensor(4.9313, grad_fn=<MseLossBackward0>)\n",
      "1880 tensor(8.5559, grad_fn=<MseLossBackward0>)\n",
      "1881 tensor(19.4535, grad_fn=<MseLossBackward0>)\n",
      "1882 tensor(19.0653, grad_fn=<MseLossBackward0>)\n",
      "1883 tensor(4.2886, grad_fn=<MseLossBackward0>)\n",
      "1884 tensor(9.9239, grad_fn=<MseLossBackward0>)\n",
      "1885 tensor(3.6672, grad_fn=<MseLossBackward0>)\n",
      "1886 tensor(4.6123, grad_fn=<MseLossBackward0>)\n",
      "1887 tensor(4.4452, grad_fn=<MseLossBackward0>)\n",
      "1888 tensor(3.2017, grad_fn=<MseLossBackward0>)\n",
      "1889 tensor(4.9113, grad_fn=<MseLossBackward0>)\n",
      "1890 tensor(4.6550, grad_fn=<MseLossBackward0>)\n",
      "1891 tensor(3.3706, grad_fn=<MseLossBackward0>)\n",
      "1892 tensor(3.7504, grad_fn=<MseLossBackward0>)\n",
      "1893 tensor(4.1799, grad_fn=<MseLossBackward0>)\n",
      "1894 tensor(2.7233, grad_fn=<MseLossBackward0>)\n",
      "1895 tensor(2.5807, grad_fn=<MseLossBackward0>)\n",
      "1896 tensor(8.6609, grad_fn=<MseLossBackward0>)\n",
      "1897 tensor(4.2083, grad_fn=<MseLossBackward0>)\n",
      "1898 tensor(8.4551, grad_fn=<MseLossBackward0>)\n",
      "1899 tensor(12.6822, grad_fn=<MseLossBackward0>)\n",
      "1900 tensor(4.7758, grad_fn=<MseLossBackward0>)\n",
      "1901 tensor(4.1055, grad_fn=<MseLossBackward0>)\n",
      "1902 tensor(6.4011, grad_fn=<MseLossBackward0>)\n",
      "1903 tensor(3.8957, grad_fn=<MseLossBackward0>)\n",
      "1904 tensor(4.0905, grad_fn=<MseLossBackward0>)\n",
      "1905 tensor(8.1103, grad_fn=<MseLossBackward0>)\n",
      "1906 tensor(8.9967, grad_fn=<MseLossBackward0>)\n",
      "1907 tensor(3.5423, grad_fn=<MseLossBackward0>)\n",
      "1908 tensor(3.6073, grad_fn=<MseLossBackward0>)\n",
      "1909 tensor(18.8791, grad_fn=<MseLossBackward0>)\n",
      "1910 tensor(3.4121, grad_fn=<MseLossBackward0>)\n",
      "1911 tensor(3.4618, grad_fn=<MseLossBackward0>)\n",
      "1912 tensor(2.9168, grad_fn=<MseLossBackward0>)\n",
      "1913 tensor(3.2154, grad_fn=<MseLossBackward0>)\n",
      "1914 tensor(3.5287, grad_fn=<MseLossBackward0>)\n",
      "1915 tensor(5.4331, grad_fn=<MseLossBackward0>)\n",
      "1916 tensor(3.5329, grad_fn=<MseLossBackward0>)\n",
      "1917 tensor(5.1630, grad_fn=<MseLossBackward0>)\n",
      "1918 tensor(4.2471, grad_fn=<MseLossBackward0>)\n",
      "1919 tensor(4.7148, grad_fn=<MseLossBackward0>)\n",
      "1920 tensor(3.1488, grad_fn=<MseLossBackward0>)\n",
      "1921 tensor(5.9077, grad_fn=<MseLossBackward0>)\n",
      "1922 tensor(3.8133, grad_fn=<MseLossBackward0>)\n",
      "1923 tensor(5.1998, grad_fn=<MseLossBackward0>)\n",
      "1924 tensor(29.4807, grad_fn=<MseLossBackward0>)\n",
      "1925 tensor(3.0655, grad_fn=<MseLossBackward0>)\n",
      "1926 tensor(4.7769, grad_fn=<MseLossBackward0>)\n",
      "1927 tensor(9.5723, grad_fn=<MseLossBackward0>)\n",
      "1928 tensor(18.2707, grad_fn=<MseLossBackward0>)\n",
      "1929 tensor(2.7744, grad_fn=<MseLossBackward0>)\n",
      "1930 tensor(10.1528, grad_fn=<MseLossBackward0>)\n",
      "1931 tensor(8.2305, grad_fn=<MseLossBackward0>)\n",
      "1932 tensor(9.9925, grad_fn=<MseLossBackward0>)\n",
      "1933 tensor(3.4387, grad_fn=<MseLossBackward0>)\n",
      "1934 tensor(10.0613, grad_fn=<MseLossBackward0>)\n",
      "1935 tensor(3.4059, grad_fn=<MseLossBackward0>)\n",
      "1936 tensor(4.8733, grad_fn=<MseLossBackward0>)\n",
      "1937 tensor(10.2645, grad_fn=<MseLossBackward0>)\n",
      "1938 tensor(3.9979, grad_fn=<MseLossBackward0>)\n",
      "1939 tensor(9.7740, grad_fn=<MseLossBackward0>)\n",
      "1940 tensor(4.5847, grad_fn=<MseLossBackward0>)\n",
      "1941 tensor(4.4416, grad_fn=<MseLossBackward0>)\n",
      "1942 tensor(3.8355, grad_fn=<MseLossBackward0>)\n",
      "1943 tensor(12.3477, grad_fn=<MseLossBackward0>)\n",
      "1944 tensor(3.3443, grad_fn=<MseLossBackward0>)\n",
      "1945 tensor(9.7277, grad_fn=<MseLossBackward0>)\n",
      "1946 tensor(2.9924, grad_fn=<MseLossBackward0>)\n",
      "1947 tensor(8.4914, grad_fn=<MseLossBackward0>)\n",
      "1948 tensor(3.9128, grad_fn=<MseLossBackward0>)\n",
      "1949 tensor(3.8352, grad_fn=<MseLossBackward0>)\n",
      "1950 tensor(3.2164, grad_fn=<MseLossBackward0>)\n",
      "1951 tensor(4.7997, grad_fn=<MseLossBackward0>)\n",
      "1952 tensor(7.4139, grad_fn=<MseLossBackward0>)\n",
      "1953 tensor(9.4444, grad_fn=<MseLossBackward0>)\n",
      "1954 tensor(3.8187, grad_fn=<MseLossBackward0>)\n",
      "1955 tensor(7.6513, grad_fn=<MseLossBackward0>)\n",
      "1956 tensor(8.3889, grad_fn=<MseLossBackward0>)\n",
      "1957 tensor(7.7058, grad_fn=<MseLossBackward0>)\n",
      "1958 tensor(3.4453, grad_fn=<MseLossBackward0>)\n",
      "1959 tensor(19.0593, grad_fn=<MseLossBackward0>)\n",
      "1960 tensor(3.4424, grad_fn=<MseLossBackward0>)\n",
      "1961 tensor(8.5837, grad_fn=<MseLossBackward0>)\n",
      "1962 tensor(3.9537, grad_fn=<MseLossBackward0>)\n",
      "1963 tensor(4.0989, grad_fn=<MseLossBackward0>)\n",
      "1964 tensor(8.1031, grad_fn=<MseLossBackward0>)\n",
      "1965 tensor(6.0982, grad_fn=<MseLossBackward0>)\n",
      "1966 tensor(4.3932, grad_fn=<MseLossBackward0>)\n",
      "1967 tensor(10.1382, grad_fn=<MseLossBackward0>)\n",
      "1968 tensor(4.1273, grad_fn=<MseLossBackward0>)\n",
      "1969 tensor(4.4919, grad_fn=<MseLossBackward0>)\n",
      "1970 tensor(2.8661, grad_fn=<MseLossBackward0>)\n",
      "1971 tensor(8.7230, grad_fn=<MseLossBackward0>)\n",
      "1972 tensor(3.5358, grad_fn=<MseLossBackward0>)\n",
      "1973 tensor(3.9077, grad_fn=<MseLossBackward0>)\n",
      "1974 tensor(18.6916, grad_fn=<MseLossBackward0>)\n",
      "1975 tensor(9.7287, grad_fn=<MseLossBackward0>)\n",
      "1976 tensor(3.8539, grad_fn=<MseLossBackward0>)\n",
      "1977 tensor(3.2771, grad_fn=<MseLossBackward0>)\n",
      "1978 tensor(8.6392, grad_fn=<MseLossBackward0>)\n",
      "1979 tensor(17.9366, grad_fn=<MseLossBackward0>)\n",
      "1980 tensor(4.3368, grad_fn=<MseLossBackward0>)\n",
      "1981 tensor(4.9545, grad_fn=<MseLossBackward0>)\n",
      "1982 tensor(3.8724, grad_fn=<MseLossBackward0>)\n",
      "1983 tensor(2.8715, grad_fn=<MseLossBackward0>)\n",
      "1984 tensor(17.4743, grad_fn=<MseLossBackward0>)\n",
      "1985 tensor(9.3241, grad_fn=<MseLossBackward0>)\n",
      "1986 tensor(2.7170, grad_fn=<MseLossBackward0>)\n",
      "1987 tensor(4.0979, grad_fn=<MseLossBackward0>)\n",
      "1988 tensor(3.5040, grad_fn=<MseLossBackward0>)\n",
      "1989 tensor(8.1648, grad_fn=<MseLossBackward0>)\n",
      "1990 tensor(6.8671, grad_fn=<MseLossBackward0>)\n",
      "1991 tensor(8.9656, grad_fn=<MseLossBackward0>)\n",
      "1992 tensor(2.9411, grad_fn=<MseLossBackward0>)\n",
      "1993 tensor(7.5947, grad_fn=<MseLossBackward0>)\n",
      "1994 tensor(3.8836, grad_fn=<MseLossBackward0>)\n",
      "1995 tensor(2.1444, grad_fn=<MseLossBackward0>)\n",
      "1996 tensor(4.2192, grad_fn=<MseLossBackward0>)\n",
      "1997 tensor(8.8646, grad_fn=<MseLossBackward0>)\n",
      "1998 tensor(3.4896, grad_fn=<MseLossBackward0>)\n",
      "1999 tensor(3.6253, grad_fn=<MseLossBackward0>)\n",
      "2000 tensor(9.0851, grad_fn=<MseLossBackward0>)\n",
      "2001 tensor(4.4066, grad_fn=<MseLossBackward0>)\n",
      "2002 tensor(3.8322, grad_fn=<MseLossBackward0>)\n",
      "2003 tensor(8.2750, grad_fn=<MseLossBackward0>)\n",
      "2004 tensor(17.6507, grad_fn=<MseLossBackward0>)\n",
      "2005 tensor(3.4823, grad_fn=<MseLossBackward0>)\n",
      "2006 tensor(4.1526, grad_fn=<MseLossBackward0>)\n",
      "2007 tensor(3.2460, grad_fn=<MseLossBackward0>)\n",
      "2008 tensor(9.7345, grad_fn=<MseLossBackward0>)\n",
      "2009 tensor(5.3016, grad_fn=<MseLossBackward0>)\n",
      "2010 tensor(5.0604, grad_fn=<MseLossBackward0>)\n",
      "2011 tensor(12.2125, grad_fn=<MseLossBackward0>)\n",
      "2012 tensor(4.2053, grad_fn=<MseLossBackward0>)\n",
      "2013 tensor(8.8470, grad_fn=<MseLossBackward0>)\n",
      "2014 tensor(3.1380, grad_fn=<MseLossBackward0>)\n",
      "2015 tensor(7.6991, grad_fn=<MseLossBackward0>)\n",
      "2016 tensor(7.9322, grad_fn=<MseLossBackward0>)\n",
      "2017 tensor(7.8847, grad_fn=<MseLossBackward0>)\n",
      "2018 tensor(3.4982, grad_fn=<MseLossBackward0>)\n",
      "2019 tensor(4.0700, grad_fn=<MseLossBackward0>)\n",
      "2020 tensor(17.2067, grad_fn=<MseLossBackward0>)\n",
      "2021 tensor(18.2481, grad_fn=<MseLossBackward0>)\n",
      "2022 tensor(2.6202, grad_fn=<MseLossBackward0>)\n",
      "2023 tensor(7.3933, grad_fn=<MseLossBackward0>)\n",
      "2024 tensor(8.1503, grad_fn=<MseLossBackward0>)\n",
      "2025 tensor(4.1405, grad_fn=<MseLossBackward0>)\n",
      "2026 tensor(5.3330, grad_fn=<MseLossBackward0>)\n",
      "2027 tensor(5.1066, grad_fn=<MseLossBackward0>)\n",
      "2028 tensor(3.5742, grad_fn=<MseLossBackward0>)\n",
      "2029 tensor(8.8386, grad_fn=<MseLossBackward0>)\n",
      "2030 tensor(8.0856, grad_fn=<MseLossBackward0>)\n",
      "2031 tensor(2.9099, grad_fn=<MseLossBackward0>)\n",
      "2032 tensor(4.4284, grad_fn=<MseLossBackward0>)\n",
      "2033 tensor(3.6851, grad_fn=<MseLossBackward0>)\n",
      "2034 tensor(8.1093, grad_fn=<MseLossBackward0>)\n",
      "2035 tensor(3.1131, grad_fn=<MseLossBackward0>)\n",
      "2036 tensor(7.5130, grad_fn=<MseLossBackward0>)\n",
      "2037 tensor(3.6617, grad_fn=<MseLossBackward0>)\n",
      "2038 tensor(9.2765, grad_fn=<MseLossBackward0>)\n",
      "2039 tensor(3.3754, grad_fn=<MseLossBackward0>)\n",
      "2040 tensor(2.9430, grad_fn=<MseLossBackward0>)\n",
      "2041 tensor(2.8348, grad_fn=<MseLossBackward0>)\n",
      "2042 tensor(9.1062, grad_fn=<MseLossBackward0>)\n",
      "2043 tensor(3.7829, grad_fn=<MseLossBackward0>)\n",
      "2044 tensor(3.5247, grad_fn=<MseLossBackward0>)\n",
      "2045 tensor(9.4670, grad_fn=<MseLossBackward0>)\n",
      "2046 tensor(8.2423, grad_fn=<MseLossBackward0>)\n",
      "2047 tensor(3.6327, grad_fn=<MseLossBackward0>)\n",
      "2048 tensor(3.2595, grad_fn=<MseLossBackward0>)\n",
      "2049 tensor(9.6077, grad_fn=<MseLossBackward0>)\n",
      "2050 tensor(3.8697, grad_fn=<MseLossBackward0>)\n",
      "2051 tensor(21.2219, grad_fn=<MseLossBackward0>)\n",
      "2052 tensor(4.0770, grad_fn=<MseLossBackward0>)\n",
      "2053 tensor(7.0428, grad_fn=<MseLossBackward0>)\n",
      "2054 tensor(1.9803, grad_fn=<MseLossBackward0>)\n",
      "2055 tensor(3.6480, grad_fn=<MseLossBackward0>)\n",
      "2056 tensor(8.8730, grad_fn=<MseLossBackward0>)\n",
      "2057 tensor(4.0008, grad_fn=<MseLossBackward0>)\n",
      "2058 tensor(3.8638, grad_fn=<MseLossBackward0>)\n",
      "2059 tensor(7.3182, grad_fn=<MseLossBackward0>)\n",
      "2060 tensor(3.0787, grad_fn=<MseLossBackward0>)\n",
      "2061 tensor(2.4983, grad_fn=<MseLossBackward0>)\n",
      "2062 tensor(3.5326, grad_fn=<MseLossBackward0>)\n",
      "2063 tensor(7.9910, grad_fn=<MseLossBackward0>)\n",
      "2064 tensor(4.2085, grad_fn=<MseLossBackward0>)\n",
      "2065 tensor(4.2327, grad_fn=<MseLossBackward0>)\n",
      "2066 tensor(3.3756, grad_fn=<MseLossBackward0>)\n",
      "2067 tensor(16.8259, grad_fn=<MseLossBackward0>)\n",
      "2068 tensor(2.7184, grad_fn=<MseLossBackward0>)\n",
      "2069 tensor(3.0193, grad_fn=<MseLossBackward0>)\n",
      "2070 tensor(4.4992, grad_fn=<MseLossBackward0>)\n",
      "2071 tensor(3.1064, grad_fn=<MseLossBackward0>)\n",
      "2072 tensor(3.0305, grad_fn=<MseLossBackward0>)\n",
      "2073 tensor(3.2895, grad_fn=<MseLossBackward0>)\n",
      "2074 tensor(2.0428, grad_fn=<MseLossBackward0>)\n",
      "2075 tensor(7.7877, grad_fn=<MseLossBackward0>)\n",
      "2076 tensor(4.6265, grad_fn=<MseLossBackward0>)\n",
      "2077 tensor(20.7691, grad_fn=<MseLossBackward0>)\n",
      "2078 tensor(4.2383, grad_fn=<MseLossBackward0>)\n",
      "2079 tensor(3.9658, grad_fn=<MseLossBackward0>)\n",
      "2080 tensor(23.1519, grad_fn=<MseLossBackward0>)\n",
      "2081 tensor(20.8063, grad_fn=<MseLossBackward0>)\n",
      "2082 tensor(3.1583, grad_fn=<MseLossBackward0>)\n",
      "2083 tensor(2.2051, grad_fn=<MseLossBackward0>)\n",
      "2084 tensor(13.6546, grad_fn=<MseLossBackward0>)\n",
      "2085 tensor(8.8602, grad_fn=<MseLossBackward0>)\n",
      "2086 tensor(2.0024, grad_fn=<MseLossBackward0>)\n",
      "2087 tensor(3.2290, grad_fn=<MseLossBackward0>)\n",
      "2088 tensor(3.2680, grad_fn=<MseLossBackward0>)\n",
      "2089 tensor(3.2111, grad_fn=<MseLossBackward0>)\n",
      "2090 tensor(3.3587, grad_fn=<MseLossBackward0>)\n",
      "2091 tensor(1.8922, grad_fn=<MseLossBackward0>)\n",
      "2092 tensor(22.7619, grad_fn=<MseLossBackward0>)\n",
      "2093 tensor(4.9414, grad_fn=<MseLossBackward0>)\n",
      "2094 tensor(2.8365, grad_fn=<MseLossBackward0>)\n",
      "2095 tensor(22.7076, grad_fn=<MseLossBackward0>)\n",
      "2096 tensor(3.5920, grad_fn=<MseLossBackward0>)\n",
      "2097 tensor(3.4601, grad_fn=<MseLossBackward0>)\n",
      "2098 tensor(4.3533, grad_fn=<MseLossBackward0>)\n",
      "2099 tensor(8.7236, grad_fn=<MseLossBackward0>)\n",
      "2100 tensor(2.6925, grad_fn=<MseLossBackward0>)\n",
      "2101 tensor(2.9871, grad_fn=<MseLossBackward0>)\n",
      "2102 tensor(9.6442, grad_fn=<MseLossBackward0>)\n",
      "2103 tensor(3.3272, grad_fn=<MseLossBackward0>)\n",
      "2104 tensor(7.8987, grad_fn=<MseLossBackward0>)\n",
      "2105 tensor(12.3024, grad_fn=<MseLossBackward0>)\n",
      "2106 tensor(7.9924, grad_fn=<MseLossBackward0>)\n",
      "2107 tensor(4.4695, grad_fn=<MseLossBackward0>)\n",
      "2108 tensor(3.6387, grad_fn=<MseLossBackward0>)\n",
      "2109 tensor(3.2353, grad_fn=<MseLossBackward0>)\n",
      "2110 tensor(3.2801, grad_fn=<MseLossBackward0>)\n",
      "2111 tensor(3.5899, grad_fn=<MseLossBackward0>)\n",
      "2112 tensor(2.8063, grad_fn=<MseLossBackward0>)\n",
      "2113 tensor(2.9587, grad_fn=<MseLossBackward0>)\n",
      "2114 tensor(3.3254, grad_fn=<MseLossBackward0>)\n",
      "2115 tensor(2.7081, grad_fn=<MseLossBackward0>)\n",
      "2116 tensor(9.1409, grad_fn=<MseLossBackward0>)\n",
      "2117 tensor(5.0888, grad_fn=<MseLossBackward0>)\n",
      "2118 tensor(2.9767, grad_fn=<MseLossBackward0>)\n",
      "2119 tensor(3.9549, grad_fn=<MseLossBackward0>)\n",
      "2120 tensor(3.0229, grad_fn=<MseLossBackward0>)\n",
      "2121 tensor(4.9941, grad_fn=<MseLossBackward0>)\n",
      "2122 tensor(11.4856, grad_fn=<MseLossBackward0>)\n",
      "2123 tensor(2.7602, grad_fn=<MseLossBackward0>)\n",
      "2124 tensor(7.0494, grad_fn=<MseLossBackward0>)\n",
      "2125 tensor(7.3589, grad_fn=<MseLossBackward0>)\n",
      "2126 tensor(17.7775, grad_fn=<MseLossBackward0>)\n",
      "2127 tensor(2.7589, grad_fn=<MseLossBackward0>)\n",
      "2128 tensor(3.5362, grad_fn=<MseLossBackward0>)\n",
      "2129 tensor(4.0080, grad_fn=<MseLossBackward0>)\n",
      "2130 tensor(3.3117, grad_fn=<MseLossBackward0>)\n",
      "2131 tensor(3.8468, grad_fn=<MseLossBackward0>)\n",
      "2132 tensor(2.8797, grad_fn=<MseLossBackward0>)\n",
      "2133 tensor(4.2280, grad_fn=<MseLossBackward0>)\n",
      "2134 tensor(8.0668, grad_fn=<MseLossBackward0>)\n",
      "2135 tensor(7.7940, grad_fn=<MseLossBackward0>)\n",
      "2136 tensor(3.9408, grad_fn=<MseLossBackward0>)\n",
      "2137 tensor(7.8261, grad_fn=<MseLossBackward0>)\n",
      "2138 tensor(4.0767, grad_fn=<MseLossBackward0>)\n",
      "2139 tensor(15.9983, grad_fn=<MseLossBackward0>)\n",
      "2140 tensor(7.9296, grad_fn=<MseLossBackward0>)\n",
      "2141 tensor(2.5928, grad_fn=<MseLossBackward0>)\n",
      "2142 tensor(7.3377, grad_fn=<MseLossBackward0>)\n",
      "2143 tensor(4.2862, grad_fn=<MseLossBackward0>)\n",
      "2144 tensor(7.7311, grad_fn=<MseLossBackward0>)\n",
      "2145 tensor(7.5474, grad_fn=<MseLossBackward0>)\n",
      "2146 tensor(2.2444, grad_fn=<MseLossBackward0>)\n",
      "2147 tensor(4.4893, grad_fn=<MseLossBackward0>)\n",
      "2148 tensor(2.7639, grad_fn=<MseLossBackward0>)\n",
      "2149 tensor(3.9547, grad_fn=<MseLossBackward0>)\n",
      "2150 tensor(3.2292, grad_fn=<MseLossBackward0>)\n",
      "2151 tensor(3.7880, grad_fn=<MseLossBackward0>)\n",
      "2152 tensor(7.2948, grad_fn=<MseLossBackward0>)\n",
      "2153 tensor(3.5003, grad_fn=<MseLossBackward0>)\n",
      "2154 tensor(3.7602, grad_fn=<MseLossBackward0>)\n",
      "2155 tensor(2.8497, grad_fn=<MseLossBackward0>)\n",
      "2156 tensor(1.6217, grad_fn=<MseLossBackward0>)\n",
      "2157 tensor(4.2599, grad_fn=<MseLossBackward0>)\n",
      "2158 tensor(16.8898, grad_fn=<MseLossBackward0>)\n",
      "2159 tensor(17.2098, grad_fn=<MseLossBackward0>)\n",
      "2160 tensor(17.3822, grad_fn=<MseLossBackward0>)\n",
      "2161 tensor(7.9029, grad_fn=<MseLossBackward0>)\n",
      "2162 tensor(7.4055, grad_fn=<MseLossBackward0>)\n",
      "2163 tensor(1.9963, grad_fn=<MseLossBackward0>)\n",
      "2164 tensor(4.1221, grad_fn=<MseLossBackward0>)\n",
      "2165 tensor(3.6906, grad_fn=<MseLossBackward0>)\n",
      "2166 tensor(2.6266, grad_fn=<MseLossBackward0>)\n",
      "2167 tensor(2.8404, grad_fn=<MseLossBackward0>)\n",
      "2168 tensor(4.0684, grad_fn=<MseLossBackward0>)\n",
      "2169 tensor(3.7899, grad_fn=<MseLossBackward0>)\n",
      "2170 tensor(3.7962, grad_fn=<MseLossBackward0>)\n",
      "2171 tensor(12.5046, grad_fn=<MseLossBackward0>)\n",
      "2172 tensor(3.0640, grad_fn=<MseLossBackward0>)\n",
      "2173 tensor(2.7963, grad_fn=<MseLossBackward0>)\n",
      "2174 tensor(2.3472, grad_fn=<MseLossBackward0>)\n",
      "2175 tensor(3.5883, grad_fn=<MseLossBackward0>)\n",
      "2176 tensor(4.4214, grad_fn=<MseLossBackward0>)\n",
      "2177 tensor(3.8396, grad_fn=<MseLossBackward0>)\n",
      "2178 tensor(3.4791, grad_fn=<MseLossBackward0>)\n",
      "2179 tensor(3.2937, grad_fn=<MseLossBackward0>)\n",
      "2180 tensor(13.2944, grad_fn=<MseLossBackward0>)\n",
      "2181 tensor(8.5633, grad_fn=<MseLossBackward0>)\n",
      "2182 tensor(4.1402, grad_fn=<MseLossBackward0>)\n",
      "2183 tensor(4.8070, grad_fn=<MseLossBackward0>)\n",
      "2184 tensor(7.9919, grad_fn=<MseLossBackward0>)\n",
      "2185 tensor(3.8821, grad_fn=<MseLossBackward0>)\n",
      "2186 tensor(17.2883, grad_fn=<MseLossBackward0>)\n",
      "2187 tensor(3.1503, grad_fn=<MseLossBackward0>)\n",
      "2188 tensor(3.4280, grad_fn=<MseLossBackward0>)\n",
      "2189 tensor(3.8039, grad_fn=<MseLossBackward0>)\n",
      "2190 tensor(17.2269, grad_fn=<MseLossBackward0>)\n",
      "2191 tensor(3.2737, grad_fn=<MseLossBackward0>)\n",
      "2192 tensor(8.6001, grad_fn=<MseLossBackward0>)\n",
      "2193 tensor(8.6347, grad_fn=<MseLossBackward0>)\n",
      "2194 tensor(3.0712, grad_fn=<MseLossBackward0>)\n",
      "2195 tensor(3.5797, grad_fn=<MseLossBackward0>)\n",
      "2196 tensor(2.8912, grad_fn=<MseLossBackward0>)\n",
      "2197 tensor(7.3188, grad_fn=<MseLossBackward0>)\n",
      "2198 tensor(3.5172, grad_fn=<MseLossBackward0>)\n",
      "2199 tensor(7.7565, grad_fn=<MseLossBackward0>)\n",
      "2200 tensor(3.6376, grad_fn=<MseLossBackward0>)\n",
      "2201 tensor(15.6898, grad_fn=<MseLossBackward0>)\n",
      "2202 tensor(2.9984, grad_fn=<MseLossBackward0>)\n",
      "2203 tensor(2.3738, grad_fn=<MseLossBackward0>)\n",
      "2204 tensor(2.8201, grad_fn=<MseLossBackward0>)\n",
      "2205 tensor(2.7873, grad_fn=<MseLossBackward0>)\n",
      "2206 tensor(3.2449, grad_fn=<MseLossBackward0>)\n",
      "2207 tensor(2.0874, grad_fn=<MseLossBackward0>)\n",
      "2208 tensor(3.8742, grad_fn=<MseLossBackward0>)\n",
      "2209 tensor(3.5345, grad_fn=<MseLossBackward0>)\n",
      "2210 tensor(2.6389, grad_fn=<MseLossBackward0>)\n",
      "2211 tensor(7.6485, grad_fn=<MseLossBackward0>)\n",
      "2212 tensor(3.8561, grad_fn=<MseLossBackward0>)\n",
      "2213 tensor(4.3072, grad_fn=<MseLossBackward0>)\n",
      "2214 tensor(3.3243, grad_fn=<MseLossBackward0>)\n",
      "2215 tensor(3.4519, grad_fn=<MseLossBackward0>)\n",
      "2216 tensor(4.2475, grad_fn=<MseLossBackward0>)\n",
      "2217 tensor(3.3901, grad_fn=<MseLossBackward0>)\n",
      "2218 tensor(16.2219, grad_fn=<MseLossBackward0>)\n",
      "2219 tensor(3.2374, grad_fn=<MseLossBackward0>)\n",
      "2220 tensor(3.5064, grad_fn=<MseLossBackward0>)\n",
      "2221 tensor(8.8248, grad_fn=<MseLossBackward0>)\n",
      "2222 tensor(3.9283, grad_fn=<MseLossBackward0>)\n",
      "2223 tensor(2.8497, grad_fn=<MseLossBackward0>)\n",
      "2224 tensor(16.1377, grad_fn=<MseLossBackward0>)\n",
      "2225 tensor(8.4533, grad_fn=<MseLossBackward0>)\n",
      "2226 tensor(3.9034, grad_fn=<MseLossBackward0>)\n",
      "2227 tensor(5.5767, grad_fn=<MseLossBackward0>)\n",
      "2228 tensor(2.6802, grad_fn=<MseLossBackward0>)\n",
      "2229 tensor(3.8842, grad_fn=<MseLossBackward0>)\n",
      "2230 tensor(13.0351, grad_fn=<MseLossBackward0>)\n",
      "2231 tensor(21.7253, grad_fn=<MseLossBackward0>)\n",
      "2232 tensor(8.2378, grad_fn=<MseLossBackward0>)\n",
      "2233 tensor(2.8736, grad_fn=<MseLossBackward0>)\n",
      "2234 tensor(3.9264, grad_fn=<MseLossBackward0>)\n",
      "2235 tensor(3.7067, grad_fn=<MseLossBackward0>)\n",
      "2236 tensor(3.9898, grad_fn=<MseLossBackward0>)\n",
      "2237 tensor(2.9425, grad_fn=<MseLossBackward0>)\n",
      "2238 tensor(16.1402, grad_fn=<MseLossBackward0>)\n",
      "2239 tensor(7.2118, grad_fn=<MseLossBackward0>)\n",
      "2240 tensor(6.0422, grad_fn=<MseLossBackward0>)\n",
      "2241 tensor(6.9954, grad_fn=<MseLossBackward0>)\n",
      "2242 tensor(2.9439, grad_fn=<MseLossBackward0>)\n",
      "2243 tensor(4.9398, grad_fn=<MseLossBackward0>)\n",
      "2244 tensor(1.6994, grad_fn=<MseLossBackward0>)\n",
      "2245 tensor(4.3908, grad_fn=<MseLossBackward0>)\n",
      "2246 tensor(3.4336, grad_fn=<MseLossBackward0>)\n",
      "2247 tensor(3.7982, grad_fn=<MseLossBackward0>)\n",
      "2248 tensor(2.8566, grad_fn=<MseLossBackward0>)\n",
      "2249 tensor(3.4923, grad_fn=<MseLossBackward0>)\n",
      "2250 tensor(8.1197, grad_fn=<MseLossBackward0>)\n",
      "2251 tensor(3.8461, grad_fn=<MseLossBackward0>)\n",
      "2252 tensor(3.4336, grad_fn=<MseLossBackward0>)\n",
      "2253 tensor(14.9984, grad_fn=<MseLossBackward0>)\n",
      "2254 tensor(2.8105, grad_fn=<MseLossBackward0>)\n",
      "2255 tensor(3.6071, grad_fn=<MseLossBackward0>)\n",
      "2256 tensor(4.0601, grad_fn=<MseLossBackward0>)\n",
      "2257 tensor(6.9435, grad_fn=<MseLossBackward0>)\n",
      "2258 tensor(9.0865, grad_fn=<MseLossBackward0>)\n",
      "2259 tensor(2.8279, grad_fn=<MseLossBackward0>)\n",
      "2260 tensor(15.5231, grad_fn=<MseLossBackward0>)\n",
      "2261 tensor(8.1632, grad_fn=<MseLossBackward0>)\n",
      "2262 tensor(20.9842, grad_fn=<MseLossBackward0>)\n",
      "2263 tensor(3.7050, grad_fn=<MseLossBackward0>)\n",
      "2264 tensor(2.5275, grad_fn=<MseLossBackward0>)\n",
      "2265 tensor(7.3940, grad_fn=<MseLossBackward0>)\n",
      "2266 tensor(9.1136, grad_fn=<MseLossBackward0>)\n",
      "2267 tensor(9.6267, grad_fn=<MseLossBackward0>)\n",
      "2268 tensor(15.3245, grad_fn=<MseLossBackward0>)\n",
      "2269 tensor(2.9989, grad_fn=<MseLossBackward0>)\n",
      "2270 tensor(3.0681, grad_fn=<MseLossBackward0>)\n",
      "2271 tensor(3.0878, grad_fn=<MseLossBackward0>)\n",
      "2272 tensor(4.0579, grad_fn=<MseLossBackward0>)\n",
      "2273 tensor(15.3688, grad_fn=<MseLossBackward0>)\n",
      "2274 tensor(3.5268, grad_fn=<MseLossBackward0>)\n",
      "2275 tensor(3.2382, grad_fn=<MseLossBackward0>)\n",
      "2276 tensor(3.7932, grad_fn=<MseLossBackward0>)\n",
      "2277 tensor(3.9254, grad_fn=<MseLossBackward0>)\n",
      "2278 tensor(2.9197, grad_fn=<MseLossBackward0>)\n",
      "2279 tensor(7.5841, grad_fn=<MseLossBackward0>)\n",
      "2280 tensor(3.2571, grad_fn=<MseLossBackward0>)\n",
      "2281 tensor(2.4275, grad_fn=<MseLossBackward0>)\n",
      "2282 tensor(19.3377, grad_fn=<MseLossBackward0>)\n",
      "2283 tensor(2.2344, grad_fn=<MseLossBackward0>)\n",
      "2284 tensor(8.0952, grad_fn=<MseLossBackward0>)\n",
      "2285 tensor(2.8111, grad_fn=<MseLossBackward0>)\n",
      "2286 tensor(3.2912, grad_fn=<MseLossBackward0>)\n",
      "2287 tensor(3.0847, grad_fn=<MseLossBackward0>)\n",
      "2288 tensor(20.1435, grad_fn=<MseLossBackward0>)\n",
      "2289 tensor(4.0524, grad_fn=<MseLossBackward0>)\n",
      "2290 tensor(2.2566, grad_fn=<MseLossBackward0>)\n",
      "2291 tensor(8.8635, grad_fn=<MseLossBackward0>)\n",
      "2292 tensor(3.0641, grad_fn=<MseLossBackward0>)\n",
      "2293 tensor(3.2553, grad_fn=<MseLossBackward0>)\n",
      "2294 tensor(3.1055, grad_fn=<MseLossBackward0>)\n",
      "2295 tensor(6.8370, grad_fn=<MseLossBackward0>)\n",
      "2296 tensor(3.1424, grad_fn=<MseLossBackward0>)\n",
      "2297 tensor(2.9632, grad_fn=<MseLossBackward0>)\n",
      "2298 tensor(7.0821, grad_fn=<MseLossBackward0>)\n",
      "2299 tensor(3.8170, grad_fn=<MseLossBackward0>)\n",
      "2300 tensor(17.0149, grad_fn=<MseLossBackward0>)\n",
      "2301 tensor(15.1359, grad_fn=<MseLossBackward0>)\n",
      "2302 tensor(2.8939, grad_fn=<MseLossBackward0>)\n",
      "2303 tensor(6.2604, grad_fn=<MseLossBackward0>)\n",
      "2304 tensor(8.4075, grad_fn=<MseLossBackward0>)\n",
      "2305 tensor(2.1096, grad_fn=<MseLossBackward0>)\n",
      "2306 tensor(3.5791, grad_fn=<MseLossBackward0>)\n",
      "2307 tensor(7.7092, grad_fn=<MseLossBackward0>)\n",
      "2308 tensor(3.8173, grad_fn=<MseLossBackward0>)\n",
      "2309 tensor(3.5382, grad_fn=<MseLossBackward0>)\n",
      "2310 tensor(9.1345, grad_fn=<MseLossBackward0>)\n",
      "2311 tensor(6.7851, grad_fn=<MseLossBackward0>)\n",
      "2312 tensor(3.3985, grad_fn=<MseLossBackward0>)\n",
      "2313 tensor(2.6321, grad_fn=<MseLossBackward0>)\n",
      "2314 tensor(15.2782, grad_fn=<MseLossBackward0>)\n",
      "2315 tensor(6.3476, grad_fn=<MseLossBackward0>)\n",
      "2316 tensor(3.6012, grad_fn=<MseLossBackward0>)\n",
      "2317 tensor(3.8692, grad_fn=<MseLossBackward0>)\n",
      "2318 tensor(2.3808, grad_fn=<MseLossBackward0>)\n",
      "2319 tensor(1.7857, grad_fn=<MseLossBackward0>)\n",
      "2320 tensor(3.1047, grad_fn=<MseLossBackward0>)\n",
      "2321 tensor(1.8102, grad_fn=<MseLossBackward0>)\n",
      "2322 tensor(3.4753, grad_fn=<MseLossBackward0>)\n",
      "2323 tensor(7.1242, grad_fn=<MseLossBackward0>)\n",
      "2324 tensor(8.2667, grad_fn=<MseLossBackward0>)\n",
      "2325 tensor(2.3964, grad_fn=<MseLossBackward0>)\n",
      "2326 tensor(2.9999, grad_fn=<MseLossBackward0>)\n",
      "2327 tensor(6.9303, grad_fn=<MseLossBackward0>)\n",
      "2328 tensor(3.8944, grad_fn=<MseLossBackward0>)\n",
      "2329 tensor(3.1182, grad_fn=<MseLossBackward0>)\n",
      "2330 tensor(7.1855, grad_fn=<MseLossBackward0>)\n",
      "2331 tensor(2.9512, grad_fn=<MseLossBackward0>)\n",
      "2332 tensor(3.0780, grad_fn=<MseLossBackward0>)\n",
      "2333 tensor(2.8994, grad_fn=<MseLossBackward0>)\n",
      "2334 tensor(2.9763, grad_fn=<MseLossBackward0>)\n",
      "2335 tensor(3.5245, grad_fn=<MseLossBackward0>)\n",
      "2336 tensor(14.8787, grad_fn=<MseLossBackward0>)\n",
      "2337 tensor(6.2515, grad_fn=<MseLossBackward0>)\n",
      "2338 tensor(3.6495, grad_fn=<MseLossBackward0>)\n",
      "2339 tensor(3.0518, grad_fn=<MseLossBackward0>)\n",
      "2340 tensor(6.8688, grad_fn=<MseLossBackward0>)\n",
      "2341 tensor(8.6184, grad_fn=<MseLossBackward0>)\n",
      "2342 tensor(2.6778, grad_fn=<MseLossBackward0>)\n",
      "2343 tensor(3.7302, grad_fn=<MseLossBackward0>)\n",
      "2344 tensor(3.3272, grad_fn=<MseLossBackward0>)\n",
      "2345 tensor(3.5544, grad_fn=<MseLossBackward0>)\n",
      "2346 tensor(2.8995, grad_fn=<MseLossBackward0>)\n",
      "2347 tensor(2.6980, grad_fn=<MseLossBackward0>)\n",
      "2348 tensor(3.1843, grad_fn=<MseLossBackward0>)\n",
      "2349 tensor(2.4694, grad_fn=<MseLossBackward0>)\n",
      "2350 tensor(3.3210, grad_fn=<MseLossBackward0>)\n",
      "2351 tensor(3.9940, grad_fn=<MseLossBackward0>)\n",
      "2352 tensor(2.7036, grad_fn=<MseLossBackward0>)\n",
      "2353 tensor(3.2714, grad_fn=<MseLossBackward0>)\n",
      "2354 tensor(6.1907, grad_fn=<MseLossBackward0>)\n",
      "2355 tensor(7.3938, grad_fn=<MseLossBackward0>)\n",
      "2356 tensor(1.8236, grad_fn=<MseLossBackward0>)\n",
      "2357 tensor(2.9369, grad_fn=<MseLossBackward0>)\n",
      "2358 tensor(2.3493, grad_fn=<MseLossBackward0>)\n",
      "2359 tensor(2.3773, grad_fn=<MseLossBackward0>)\n",
      "2360 tensor(2.4086, grad_fn=<MseLossBackward0>)\n",
      "2361 tensor(3.2717, grad_fn=<MseLossBackward0>)\n",
      "2362 tensor(4.0802, grad_fn=<MseLossBackward0>)\n",
      "2363 tensor(3.1385, grad_fn=<MseLossBackward0>)\n",
      "2364 tensor(3.4403, grad_fn=<MseLossBackward0>)\n",
      "2365 tensor(3.4256, grad_fn=<MseLossBackward0>)\n",
      "2366 tensor(6.7864, grad_fn=<MseLossBackward0>)\n",
      "2367 tensor(3.5829, grad_fn=<MseLossBackward0>)\n",
      "2368 tensor(3.3160, grad_fn=<MseLossBackward0>)\n",
      "2369 tensor(10.5971, grad_fn=<MseLossBackward0>)\n",
      "2370 tensor(5.8487, grad_fn=<MseLossBackward0>)\n",
      "2371 tensor(3.5961, grad_fn=<MseLossBackward0>)\n",
      "2372 tensor(4.1216, grad_fn=<MseLossBackward0>)\n",
      "2373 tensor(8.5444, grad_fn=<MseLossBackward0>)\n",
      "2374 tensor(3.0274, grad_fn=<MseLossBackward0>)\n",
      "2375 tensor(7.7509, grad_fn=<MseLossBackward0>)\n",
      "2376 tensor(9.1040, grad_fn=<MseLossBackward0>)\n",
      "2377 tensor(3.9196, grad_fn=<MseLossBackward0>)\n",
      "2378 tensor(6.0875, grad_fn=<MseLossBackward0>)\n",
      "2379 tensor(8.2553, grad_fn=<MseLossBackward0>)\n",
      "2380 tensor(3.8718, grad_fn=<MseLossBackward0>)\n",
      "2381 tensor(6.1571, grad_fn=<MseLossBackward0>)\n",
      "2382 tensor(3.3119, grad_fn=<MseLossBackward0>)\n",
      "2383 tensor(2.8131, grad_fn=<MseLossBackward0>)\n",
      "2384 tensor(3.3897, grad_fn=<MseLossBackward0>)\n",
      "2385 tensor(3.0856, grad_fn=<MseLossBackward0>)\n",
      "2386 tensor(3.6063, grad_fn=<MseLossBackward0>)\n",
      "2387 tensor(2.5283, grad_fn=<MseLossBackward0>)\n",
      "2388 tensor(8.0901, grad_fn=<MseLossBackward0>)\n",
      "2389 tensor(14.6694, grad_fn=<MseLossBackward0>)\n",
      "2390 tensor(2.8321, grad_fn=<MseLossBackward0>)\n",
      "2391 tensor(3.2517, grad_fn=<MseLossBackward0>)\n",
      "2392 tensor(7.4591, grad_fn=<MseLossBackward0>)\n",
      "2393 tensor(6.7569, grad_fn=<MseLossBackward0>)\n",
      "2394 tensor(2.2704, grad_fn=<MseLossBackward0>)\n",
      "2395 tensor(2.3206, grad_fn=<MseLossBackward0>)\n",
      "2396 tensor(3.7786, grad_fn=<MseLossBackward0>)\n",
      "2397 tensor(15.5518, grad_fn=<MseLossBackward0>)\n",
      "2398 tensor(2.6572, grad_fn=<MseLossBackward0>)\n",
      "2399 tensor(6.6631, grad_fn=<MseLossBackward0>)\n",
      "2400 tensor(2.9182, grad_fn=<MseLossBackward0>)\n",
      "2401 tensor(3.0184, grad_fn=<MseLossBackward0>)\n",
      "2402 tensor(4.9412, grad_fn=<MseLossBackward0>)\n",
      "2403 tensor(2.3590, grad_fn=<MseLossBackward0>)\n",
      "2404 tensor(3.2529, grad_fn=<MseLossBackward0>)\n",
      "2405 tensor(13.6519, grad_fn=<MseLossBackward0>)\n",
      "2406 tensor(4.5934, grad_fn=<MseLossBackward0>)\n",
      "2407 tensor(3.0011, grad_fn=<MseLossBackward0>)\n",
      "2408 tensor(2.8873, grad_fn=<MseLossBackward0>)\n",
      "2409 tensor(6.9033, grad_fn=<MseLossBackward0>)\n",
      "2410 tensor(4.2817, grad_fn=<MseLossBackward0>)\n",
      "2411 tensor(3.2525, grad_fn=<MseLossBackward0>)\n",
      "2412 tensor(2.5309, grad_fn=<MseLossBackward0>)\n",
      "2413 tensor(2.6139, grad_fn=<MseLossBackward0>)\n",
      "2414 tensor(6.6149, grad_fn=<MseLossBackward0>)\n",
      "2415 tensor(6.7884, grad_fn=<MseLossBackward0>)\n",
      "2416 tensor(7.2121, grad_fn=<MseLossBackward0>)\n",
      "2417 tensor(3.0650, grad_fn=<MseLossBackward0>)\n",
      "2418 tensor(3.0930, grad_fn=<MseLossBackward0>)\n",
      "2419 tensor(3.6520, grad_fn=<MseLossBackward0>)\n",
      "2420 tensor(14.1443, grad_fn=<MseLossBackward0>)\n",
      "2421 tensor(4.2072, grad_fn=<MseLossBackward0>)\n",
      "2422 tensor(1.9603, grad_fn=<MseLossBackward0>)\n",
      "2423 tensor(15.5102, grad_fn=<MseLossBackward0>)\n",
      "2424 tensor(3.6585, grad_fn=<MseLossBackward0>)\n",
      "2425 tensor(2.5696, grad_fn=<MseLossBackward0>)\n",
      "2426 tensor(2.6620, grad_fn=<MseLossBackward0>)\n",
      "2427 tensor(2.7222, grad_fn=<MseLossBackward0>)\n",
      "2428 tensor(3.4278, grad_fn=<MseLossBackward0>)\n",
      "2429 tensor(3.4671, grad_fn=<MseLossBackward0>)\n",
      "2430 tensor(1.8619, grad_fn=<MseLossBackward0>)\n",
      "2431 tensor(3.0252, grad_fn=<MseLossBackward0>)\n",
      "2432 tensor(6.5103, grad_fn=<MseLossBackward0>)\n",
      "2433 tensor(2.5948, grad_fn=<MseLossBackward0>)\n",
      "2434 tensor(6.8025, grad_fn=<MseLossBackward0>)\n",
      "2435 tensor(2.1926, grad_fn=<MseLossBackward0>)\n",
      "2436 tensor(3.9740, grad_fn=<MseLossBackward0>)\n",
      "2437 tensor(2.5056, grad_fn=<MseLossBackward0>)\n",
      "2438 tensor(2.9543, grad_fn=<MseLossBackward0>)\n",
      "2439 tensor(15.1871, grad_fn=<MseLossBackward0>)\n",
      "2440 tensor(2.3572, grad_fn=<MseLossBackward0>)\n",
      "2441 tensor(6.6588, grad_fn=<MseLossBackward0>)\n",
      "2442 tensor(3.3257, grad_fn=<MseLossBackward0>)\n",
      "2443 tensor(3.8794, grad_fn=<MseLossBackward0>)\n",
      "2444 tensor(2.7054, grad_fn=<MseLossBackward0>)\n",
      "2445 tensor(18.6233, grad_fn=<MseLossBackward0>)\n",
      "2446 tensor(3.4438, grad_fn=<MseLossBackward0>)\n",
      "2447 tensor(23.2017, grad_fn=<MseLossBackward0>)\n",
      "2448 tensor(2.5680, grad_fn=<MseLossBackward0>)\n",
      "2449 tensor(3.1689, grad_fn=<MseLossBackward0>)\n",
      "2450 tensor(14.4272, grad_fn=<MseLossBackward0>)\n",
      "2451 tensor(2.3863, grad_fn=<MseLossBackward0>)\n",
      "2452 tensor(17.9461, grad_fn=<MseLossBackward0>)\n",
      "2453 tensor(2.0663, grad_fn=<MseLossBackward0>)\n",
      "2454 tensor(7.9649, grad_fn=<MseLossBackward0>)\n",
      "2455 tensor(15.7468, grad_fn=<MseLossBackward0>)\n",
      "2456 tensor(3.6624, grad_fn=<MseLossBackward0>)\n",
      "2457 tensor(14.9284, grad_fn=<MseLossBackward0>)\n",
      "2458 tensor(3.2837, grad_fn=<MseLossBackward0>)\n",
      "2459 tensor(7.9762, grad_fn=<MseLossBackward0>)\n",
      "2460 tensor(14.0358, grad_fn=<MseLossBackward0>)\n",
      "2461 tensor(2.9604, grad_fn=<MseLossBackward0>)\n",
      "2462 tensor(6.4944, grad_fn=<MseLossBackward0>)\n",
      "2463 tensor(3.0165, grad_fn=<MseLossBackward0>)\n",
      "2464 tensor(2.3047, grad_fn=<MseLossBackward0>)\n",
      "2465 tensor(2.8198, grad_fn=<MseLossBackward0>)\n",
      "2466 tensor(2.6345, grad_fn=<MseLossBackward0>)\n",
      "2467 tensor(14.4697, grad_fn=<MseLossBackward0>)\n",
      "2468 tensor(2.4809, grad_fn=<MseLossBackward0>)\n",
      "2469 tensor(2.4618, grad_fn=<MseLossBackward0>)\n",
      "2470 tensor(14.6575, grad_fn=<MseLossBackward0>)\n",
      "2471 tensor(7.1115, grad_fn=<MseLossBackward0>)\n",
      "2472 tensor(14.0017, grad_fn=<MseLossBackward0>)\n",
      "2473 tensor(3.4849, grad_fn=<MseLossBackward0>)\n",
      "2474 tensor(3.1752, grad_fn=<MseLossBackward0>)\n",
      "2475 tensor(13.5350, grad_fn=<MseLossBackward0>)\n",
      "2476 tensor(6.6938, grad_fn=<MseLossBackward0>)\n",
      "2477 tensor(3.8251, grad_fn=<MseLossBackward0>)\n",
      "2478 tensor(6.5936, grad_fn=<MseLossBackward0>)\n",
      "2479 tensor(2.8943, grad_fn=<MseLossBackward0>)\n",
      "2480 tensor(2.6669, grad_fn=<MseLossBackward0>)\n",
      "2481 tensor(6.1293, grad_fn=<MseLossBackward0>)\n",
      "2482 tensor(3.6425, grad_fn=<MseLossBackward0>)\n",
      "2483 tensor(2.7932, grad_fn=<MseLossBackward0>)\n",
      "2484 tensor(2.5883, grad_fn=<MseLossBackward0>)\n",
      "2485 tensor(2.1360, grad_fn=<MseLossBackward0>)\n",
      "2486 tensor(2.4238, grad_fn=<MseLossBackward0>)\n",
      "2487 tensor(7.8438, grad_fn=<MseLossBackward0>)\n",
      "2488 tensor(2.7101, grad_fn=<MseLossBackward0>)\n",
      "2489 tensor(2.4715, grad_fn=<MseLossBackward0>)\n",
      "2490 tensor(13.4503, grad_fn=<MseLossBackward0>)\n",
      "2491 tensor(2.7593, grad_fn=<MseLossBackward0>)\n",
      "2492 tensor(3.7661, grad_fn=<MseLossBackward0>)\n",
      "2493 tensor(3.9996, grad_fn=<MseLossBackward0>)\n",
      "2494 tensor(2.1489, grad_fn=<MseLossBackward0>)\n",
      "2495 tensor(4.0734, grad_fn=<MseLossBackward0>)\n",
      "2496 tensor(2.3022, grad_fn=<MseLossBackward0>)\n",
      "2497 tensor(3.7032, grad_fn=<MseLossBackward0>)\n",
      "2498 tensor(2.3956, grad_fn=<MseLossBackward0>)\n",
      "2499 tensor(2.1334, grad_fn=<MseLossBackward0>)\n",
      "2500 tensor(2.4198, grad_fn=<MseLossBackward0>)\n",
      "2501 tensor(11.0225, grad_fn=<MseLossBackward0>)\n",
      "2502 tensor(2.8894, grad_fn=<MseLossBackward0>)\n",
      "2503 tensor(3.8774, grad_fn=<MseLossBackward0>)\n",
      "2504 tensor(2.7431, grad_fn=<MseLossBackward0>)\n",
      "2505 tensor(7.7179, grad_fn=<MseLossBackward0>)\n",
      "2506 tensor(15.1835, grad_fn=<MseLossBackward0>)\n",
      "2507 tensor(2.1279, grad_fn=<MseLossBackward0>)\n",
      "2508 tensor(2.2327, grad_fn=<MseLossBackward0>)\n",
      "2509 tensor(2.8185, grad_fn=<MseLossBackward0>)\n",
      "2510 tensor(6.0120, grad_fn=<MseLossBackward0>)\n",
      "2511 tensor(2.1539, grad_fn=<MseLossBackward0>)\n",
      "2512 tensor(3.6023, grad_fn=<MseLossBackward0>)\n",
      "2513 tensor(7.3764, grad_fn=<MseLossBackward0>)\n",
      "2514 tensor(11.1365, grad_fn=<MseLossBackward0>)\n",
      "2515 tensor(3.2098, grad_fn=<MseLossBackward0>)\n",
      "2516 tensor(3.1718, grad_fn=<MseLossBackward0>)\n",
      "2517 tensor(4.2406, grad_fn=<MseLossBackward0>)\n",
      "2518 tensor(2.8874, grad_fn=<MseLossBackward0>)\n",
      "2519 tensor(2.4473, grad_fn=<MseLossBackward0>)\n",
      "2520 tensor(13.3958, grad_fn=<MseLossBackward0>)\n",
      "2521 tensor(7.5934, grad_fn=<MseLossBackward0>)\n",
      "2522 tensor(2.1710, grad_fn=<MseLossBackward0>)\n",
      "2523 tensor(5.3267, grad_fn=<MseLossBackward0>)\n",
      "2524 tensor(5.9333, grad_fn=<MseLossBackward0>)\n",
      "2525 tensor(3.6670, grad_fn=<MseLossBackward0>)\n",
      "2526 tensor(3.1419, grad_fn=<MseLossBackward0>)\n",
      "2527 tensor(5.3110, grad_fn=<MseLossBackward0>)\n",
      "2528 tensor(3.9840, grad_fn=<MseLossBackward0>)\n",
      "2529 tensor(2.6619, grad_fn=<MseLossBackward0>)\n",
      "2530 tensor(3.9298, grad_fn=<MseLossBackward0>)\n",
      "2531 tensor(2.4176, grad_fn=<MseLossBackward0>)\n",
      "2532 tensor(1.9257, grad_fn=<MseLossBackward0>)\n",
      "2533 tensor(12.5680, grad_fn=<MseLossBackward0>)\n",
      "2534 tensor(7.7343, grad_fn=<MseLossBackward0>)\n",
      "2535 tensor(7.0830, grad_fn=<MseLossBackward0>)\n",
      "2536 tensor(2.5903, grad_fn=<MseLossBackward0>)\n",
      "2537 tensor(13.6603, grad_fn=<MseLossBackward0>)\n",
      "2538 tensor(1.8389, grad_fn=<MseLossBackward0>)\n",
      "2539 tensor(2.3157, grad_fn=<MseLossBackward0>)\n",
      "2540 tensor(7.3494, grad_fn=<MseLossBackward0>)\n",
      "2541 tensor(6.8628, grad_fn=<MseLossBackward0>)\n",
      "2542 tensor(8.3156, grad_fn=<MseLossBackward0>)\n",
      "2543 tensor(2.6845, grad_fn=<MseLossBackward0>)\n",
      "2544 tensor(3.0808, grad_fn=<MseLossBackward0>)\n",
      "2545 tensor(5.1271, grad_fn=<MseLossBackward0>)\n",
      "2546 tensor(22.6362, grad_fn=<MseLossBackward0>)\n",
      "2547 tensor(2.4911, grad_fn=<MseLossBackward0>)\n",
      "2548 tensor(11.0216, grad_fn=<MseLossBackward0>)\n",
      "2549 tensor(2.3382, grad_fn=<MseLossBackward0>)\n",
      "2550 tensor(2.2734, grad_fn=<MseLossBackward0>)\n",
      "2551 tensor(14.7742, grad_fn=<MseLossBackward0>)\n",
      "2552 tensor(2.3567, grad_fn=<MseLossBackward0>)\n",
      "2553 tensor(2.4786, grad_fn=<MseLossBackward0>)\n",
      "2554 tensor(2.3651, grad_fn=<MseLossBackward0>)\n",
      "2555 tensor(2.4738, grad_fn=<MseLossBackward0>)\n",
      "2556 tensor(2.2585, grad_fn=<MseLossBackward0>)\n",
      "2557 tensor(5.7375, grad_fn=<MseLossBackward0>)\n",
      "2558 tensor(6.0453, grad_fn=<MseLossBackward0>)\n",
      "2559 tensor(2.9315, grad_fn=<MseLossBackward0>)\n",
      "2560 tensor(2.6514, grad_fn=<MseLossBackward0>)\n",
      "2561 tensor(3.7374, grad_fn=<MseLossBackward0>)\n",
      "2562 tensor(2.8462, grad_fn=<MseLossBackward0>)\n",
      "2563 tensor(2.5288, grad_fn=<MseLossBackward0>)\n",
      "2564 tensor(6.0755, grad_fn=<MseLossBackward0>)\n",
      "2565 tensor(3.1061, grad_fn=<MseLossBackward0>)\n",
      "2566 tensor(8.1773, grad_fn=<MseLossBackward0>)\n",
      "2567 tensor(3.5872, grad_fn=<MseLossBackward0>)\n",
      "2568 tensor(2.0632, grad_fn=<MseLossBackward0>)\n",
      "2569 tensor(6.7916, grad_fn=<MseLossBackward0>)\n",
      "2570 tensor(14.8553, grad_fn=<MseLossBackward0>)\n",
      "2571 tensor(1.8002, grad_fn=<MseLossBackward0>)\n",
      "2572 tensor(8.4451, grad_fn=<MseLossBackward0>)\n",
      "2573 tensor(6.8355, grad_fn=<MseLossBackward0>)\n",
      "2574 tensor(2.5137, grad_fn=<MseLossBackward0>)\n",
      "2575 tensor(3.1161, grad_fn=<MseLossBackward0>)\n",
      "2576 tensor(2.5309, grad_fn=<MseLossBackward0>)\n",
      "2577 tensor(8.0203, grad_fn=<MseLossBackward0>)\n",
      "2578 tensor(2.8152, grad_fn=<MseLossBackward0>)\n",
      "2579 tensor(2.7732, grad_fn=<MseLossBackward0>)\n",
      "2580 tensor(2.3679, grad_fn=<MseLossBackward0>)\n",
      "2581 tensor(7.2392, grad_fn=<MseLossBackward0>)\n",
      "2582 tensor(7.1642, grad_fn=<MseLossBackward0>)\n",
      "2583 tensor(2.2259, grad_fn=<MseLossBackward0>)\n",
      "2584 tensor(2.8875, grad_fn=<MseLossBackward0>)\n",
      "2585 tensor(13.9852, grad_fn=<MseLossBackward0>)\n",
      "2586 tensor(2.4412, grad_fn=<MseLossBackward0>)\n",
      "2587 tensor(2.2461, grad_fn=<MseLossBackward0>)\n",
      "2588 tensor(2.0984, grad_fn=<MseLossBackward0>)\n",
      "2589 tensor(2.4380, grad_fn=<MseLossBackward0>)\n",
      "2590 tensor(2.8989, grad_fn=<MseLossBackward0>)\n",
      "2591 tensor(3.0033, grad_fn=<MseLossBackward0>)\n",
      "2592 tensor(2.6388, grad_fn=<MseLossBackward0>)\n",
      "2593 tensor(2.3937, grad_fn=<MseLossBackward0>)\n",
      "2594 tensor(2.2860, grad_fn=<MseLossBackward0>)\n",
      "2595 tensor(13.9759, grad_fn=<MseLossBackward0>)\n",
      "2596 tensor(3.9941, grad_fn=<MseLossBackward0>)\n",
      "2597 tensor(2.6333, grad_fn=<MseLossBackward0>)\n",
      "2598 tensor(2.4120, grad_fn=<MseLossBackward0>)\n",
      "2599 tensor(17.6399, grad_fn=<MseLossBackward0>)\n",
      "2600 tensor(2.9575, grad_fn=<MseLossBackward0>)\n",
      "2601 tensor(6.8978, grad_fn=<MseLossBackward0>)\n",
      "2602 tensor(6.0469, grad_fn=<MseLossBackward0>)\n",
      "2603 tensor(9.0404, grad_fn=<MseLossBackward0>)\n",
      "2604 tensor(2.1083, grad_fn=<MseLossBackward0>)\n",
      "2605 tensor(2.4183, grad_fn=<MseLossBackward0>)\n",
      "2606 tensor(3.1699, grad_fn=<MseLossBackward0>)\n",
      "2607 tensor(2.4898, grad_fn=<MseLossBackward0>)\n",
      "2608 tensor(2.5164, grad_fn=<MseLossBackward0>)\n",
      "2609 tensor(2.7727, grad_fn=<MseLossBackward0>)\n",
      "2610 tensor(3.5002, grad_fn=<MseLossBackward0>)\n",
      "2611 tensor(2.1082, grad_fn=<MseLossBackward0>)\n",
      "2612 tensor(5.0234, grad_fn=<MseLossBackward0>)\n",
      "2613 tensor(2.9267, grad_fn=<MseLossBackward0>)\n",
      "2614 tensor(14.5360, grad_fn=<MseLossBackward0>)\n",
      "2615 tensor(3.3510, grad_fn=<MseLossBackward0>)\n",
      "2616 tensor(2.9401, grad_fn=<MseLossBackward0>)\n",
      "2617 tensor(2.6953, grad_fn=<MseLossBackward0>)\n",
      "2618 tensor(2.8529, grad_fn=<MseLossBackward0>)\n",
      "2619 tensor(2.1291, grad_fn=<MseLossBackward0>)\n",
      "2620 tensor(2.0179, grad_fn=<MseLossBackward0>)\n",
      "2621 tensor(8.2739, grad_fn=<MseLossBackward0>)\n",
      "2622 tensor(3.0782, grad_fn=<MseLossBackward0>)\n",
      "2623 tensor(6.8179, grad_fn=<MseLossBackward0>)\n",
      "2624 tensor(1.8628, grad_fn=<MseLossBackward0>)\n",
      "2625 tensor(3.0264, grad_fn=<MseLossBackward0>)\n",
      "2626 tensor(1.9229, grad_fn=<MseLossBackward0>)\n",
      "2627 tensor(6.1516, grad_fn=<MseLossBackward0>)\n",
      "2628 tensor(7.6361, grad_fn=<MseLossBackward0>)\n",
      "2629 tensor(2.8645, grad_fn=<MseLossBackward0>)\n",
      "2630 tensor(2.5376, grad_fn=<MseLossBackward0>)\n",
      "2631 tensor(7.6440, grad_fn=<MseLossBackward0>)\n",
      "2632 tensor(2.3647, grad_fn=<MseLossBackward0>)\n",
      "2633 tensor(8.1655, grad_fn=<MseLossBackward0>)\n",
      "2634 tensor(2.5206, grad_fn=<MseLossBackward0>)\n",
      "2635 tensor(7.7823, grad_fn=<MseLossBackward0>)\n",
      "2636 tensor(13.4809, grad_fn=<MseLossBackward0>)\n",
      "2637 tensor(7.7256, grad_fn=<MseLossBackward0>)\n",
      "2638 tensor(7.8326, grad_fn=<MseLossBackward0>)\n",
      "2639 tensor(3.0506, grad_fn=<MseLossBackward0>)\n",
      "2640 tensor(2.2833, grad_fn=<MseLossBackward0>)\n",
      "2641 tensor(12.8987, grad_fn=<MseLossBackward0>)\n",
      "2642 tensor(2.2100, grad_fn=<MseLossBackward0>)\n",
      "2643 tensor(2.1659, grad_fn=<MseLossBackward0>)\n",
      "2644 tensor(16.0368, grad_fn=<MseLossBackward0>)\n",
      "2645 tensor(6.2463, grad_fn=<MseLossBackward0>)\n",
      "2646 tensor(2.8019, grad_fn=<MseLossBackward0>)\n",
      "2647 tensor(4.8657, grad_fn=<MseLossBackward0>)\n",
      "2648 tensor(2.5323, grad_fn=<MseLossBackward0>)\n",
      "2649 tensor(2.6623, grad_fn=<MseLossBackward0>)\n",
      "2650 tensor(13.4210, grad_fn=<MseLossBackward0>)\n",
      "2651 tensor(1.8148, grad_fn=<MseLossBackward0>)\n",
      "2652 tensor(9.0633, grad_fn=<MseLossBackward0>)\n",
      "2653 tensor(3.1537, grad_fn=<MseLossBackward0>)\n",
      "2654 tensor(6.8244, grad_fn=<MseLossBackward0>)\n",
      "2655 tensor(3.0020, grad_fn=<MseLossBackward0>)\n",
      "2656 tensor(8.1924, grad_fn=<MseLossBackward0>)\n",
      "2657 tensor(3.4762, grad_fn=<MseLossBackward0>)\n",
      "2658 tensor(2.4694, grad_fn=<MseLossBackward0>)\n",
      "2659 tensor(10.2176, grad_fn=<MseLossBackward0>)\n",
      "2660 tensor(6.6663, grad_fn=<MseLossBackward0>)\n",
      "2661 tensor(3.3555, grad_fn=<MseLossBackward0>)\n",
      "2662 tensor(19.7972, grad_fn=<MseLossBackward0>)\n",
      "2663 tensor(2.5043, grad_fn=<MseLossBackward0>)\n",
      "2664 tensor(15.8511, grad_fn=<MseLossBackward0>)\n",
      "2665 tensor(1.7763, grad_fn=<MseLossBackward0>)\n",
      "2666 tensor(2.8471, grad_fn=<MseLossBackward0>)\n",
      "2667 tensor(12.8909, grad_fn=<MseLossBackward0>)\n",
      "2668 tensor(2.6340, grad_fn=<MseLossBackward0>)\n",
      "2669 tensor(3.3231, grad_fn=<MseLossBackward0>)\n",
      "2670 tensor(2.4786, grad_fn=<MseLossBackward0>)\n",
      "2671 tensor(2.7477, grad_fn=<MseLossBackward0>)\n",
      "2672 tensor(2.4153, grad_fn=<MseLossBackward0>)\n",
      "2673 tensor(12.2909, grad_fn=<MseLossBackward0>)\n",
      "2674 tensor(2.6454, grad_fn=<MseLossBackward0>)\n",
      "2675 tensor(5.7298, grad_fn=<MseLossBackward0>)\n",
      "2676 tensor(12.7768, grad_fn=<MseLossBackward0>)\n",
      "2677 tensor(2.5150, grad_fn=<MseLossBackward0>)\n",
      "2678 tensor(2.4475, grad_fn=<MseLossBackward0>)\n",
      "2679 tensor(1.8318, grad_fn=<MseLossBackward0>)\n",
      "2680 tensor(2.8693, grad_fn=<MseLossBackward0>)\n",
      "2681 tensor(3.4045, grad_fn=<MseLossBackward0>)\n",
      "2682 tensor(8.0078, grad_fn=<MseLossBackward0>)\n",
      "2683 tensor(7.0793, grad_fn=<MseLossBackward0>)\n",
      "2684 tensor(2.4143, grad_fn=<MseLossBackward0>)\n",
      "2685 tensor(3.0609, grad_fn=<MseLossBackward0>)\n",
      "2686 tensor(2.9095, grad_fn=<MseLossBackward0>)\n",
      "2687 tensor(2.1520, grad_fn=<MseLossBackward0>)\n",
      "2688 tensor(2.7979, grad_fn=<MseLossBackward0>)\n",
      "2689 tensor(2.7186, grad_fn=<MseLossBackward0>)\n",
      "2690 tensor(2.1940, grad_fn=<MseLossBackward0>)\n",
      "2691 tensor(3.4740, grad_fn=<MseLossBackward0>)\n",
      "2692 tensor(3.0483, grad_fn=<MseLossBackward0>)\n",
      "2693 tensor(2.2735, grad_fn=<MseLossBackward0>)\n",
      "2694 tensor(5.0407, grad_fn=<MseLossBackward0>)\n",
      "2695 tensor(2.7997, grad_fn=<MseLossBackward0>)\n",
      "2696 tensor(3.7968, grad_fn=<MseLossBackward0>)\n",
      "2697 tensor(2.5163, grad_fn=<MseLossBackward0>)\n",
      "2698 tensor(5.5691, grad_fn=<MseLossBackward0>)\n",
      "2699 tensor(2.5661, grad_fn=<MseLossBackward0>)\n",
      "2700 tensor(2.2255, grad_fn=<MseLossBackward0>)\n",
      "2701 tensor(13.8035, grad_fn=<MseLossBackward0>)\n",
      "2702 tensor(8.1836, grad_fn=<MseLossBackward0>)\n",
      "2703 tensor(2.0762, grad_fn=<MseLossBackward0>)\n",
      "2704 tensor(7.8870, grad_fn=<MseLossBackward0>)\n",
      "2705 tensor(2.2622, grad_fn=<MseLossBackward0>)\n",
      "2706 tensor(2.3710, grad_fn=<MseLossBackward0>)\n",
      "2707 tensor(2.7716, grad_fn=<MseLossBackward0>)\n",
      "2708 tensor(1.7165, grad_fn=<MseLossBackward0>)\n",
      "2709 tensor(11.8097, grad_fn=<MseLossBackward0>)\n",
      "2710 tensor(2.2812, grad_fn=<MseLossBackward0>)\n",
      "2711 tensor(2.7993, grad_fn=<MseLossBackward0>)\n",
      "2712 tensor(5.9763, grad_fn=<MseLossBackward0>)\n",
      "2713 tensor(1.9690, grad_fn=<MseLossBackward0>)\n",
      "2714 tensor(7.4984, grad_fn=<MseLossBackward0>)\n",
      "2715 tensor(3.0805, grad_fn=<MseLossBackward0>)\n",
      "2716 tensor(3.2862, grad_fn=<MseLossBackward0>)\n",
      "2717 tensor(2.5875, grad_fn=<MseLossBackward0>)\n",
      "2718 tensor(3.9328, grad_fn=<MseLossBackward0>)\n",
      "2719 tensor(2.8908, grad_fn=<MseLossBackward0>)\n",
      "2720 tensor(2.1871, grad_fn=<MseLossBackward0>)\n",
      "2721 tensor(1.5705, grad_fn=<MseLossBackward0>)\n",
      "2722 tensor(12.6557, grad_fn=<MseLossBackward0>)\n",
      "2723 tensor(2.3136, grad_fn=<MseLossBackward0>)\n",
      "2724 tensor(2.6587, grad_fn=<MseLossBackward0>)\n",
      "2725 tensor(2.8590, grad_fn=<MseLossBackward0>)\n",
      "2726 tensor(3.4630, grad_fn=<MseLossBackward0>)\n",
      "2727 tensor(2.1915, grad_fn=<MseLossBackward0>)\n",
      "2728 tensor(2.5828, grad_fn=<MseLossBackward0>)\n",
      "2729 tensor(13.4030, grad_fn=<MseLossBackward0>)\n",
      "2730 tensor(2.9869, grad_fn=<MseLossBackward0>)\n",
      "2731 tensor(1.8784, grad_fn=<MseLossBackward0>)\n",
      "2732 tensor(5.9620, grad_fn=<MseLossBackward0>)\n",
      "2733 tensor(2.4794, grad_fn=<MseLossBackward0>)\n",
      "2734 tensor(12.0312, grad_fn=<MseLossBackward0>)\n",
      "2735 tensor(10.7868, grad_fn=<MseLossBackward0>)\n",
      "2736 tensor(6.9932, grad_fn=<MseLossBackward0>)\n",
      "2737 tensor(1.7165, grad_fn=<MseLossBackward0>)\n",
      "2738 tensor(1.8144, grad_fn=<MseLossBackward0>)\n",
      "2739 tensor(3.2155, grad_fn=<MseLossBackward0>)\n",
      "2740 tensor(2.3649, grad_fn=<MseLossBackward0>)\n",
      "2741 tensor(1.9763, grad_fn=<MseLossBackward0>)\n",
      "2742 tensor(2.3160, grad_fn=<MseLossBackward0>)\n",
      "2743 tensor(2.9594, grad_fn=<MseLossBackward0>)\n",
      "2744 tensor(6.5412, grad_fn=<MseLossBackward0>)\n",
      "2745 tensor(2.4334, grad_fn=<MseLossBackward0>)\n",
      "2746 tensor(1.9131, grad_fn=<MseLossBackward0>)\n",
      "2747 tensor(2.8430, grad_fn=<MseLossBackward0>)\n",
      "2748 tensor(6.7737, grad_fn=<MseLossBackward0>)\n",
      "2749 tensor(1.8184, grad_fn=<MseLossBackward0>)\n",
      "2750 tensor(2.1208, grad_fn=<MseLossBackward0>)\n",
      "2751 tensor(5.7695, grad_fn=<MseLossBackward0>)\n",
      "2752 tensor(12.5793, grad_fn=<MseLossBackward0>)\n",
      "2753 tensor(6.8578, grad_fn=<MseLossBackward0>)\n",
      "2754 tensor(4.2990, grad_fn=<MseLossBackward0>)\n",
      "2755 tensor(2.0244, grad_fn=<MseLossBackward0>)\n",
      "2756 tensor(7.4803, grad_fn=<MseLossBackward0>)\n",
      "2757 tensor(1.7351, grad_fn=<MseLossBackward0>)\n",
      "2758 tensor(2.6345, grad_fn=<MseLossBackward0>)\n",
      "2759 tensor(2.4949, grad_fn=<MseLossBackward0>)\n",
      "2760 tensor(3.7015, grad_fn=<MseLossBackward0>)\n",
      "2761 tensor(2.4120, grad_fn=<MseLossBackward0>)\n",
      "2762 tensor(3.5156, grad_fn=<MseLossBackward0>)\n",
      "2763 tensor(7.3547, grad_fn=<MseLossBackward0>)\n",
      "2764 tensor(13.1243, grad_fn=<MseLossBackward0>)\n",
      "2765 tensor(2.6658, grad_fn=<MseLossBackward0>)\n",
      "2766 tensor(1.8571, grad_fn=<MseLossBackward0>)\n",
      "2767 tensor(13.0449, grad_fn=<MseLossBackward0>)\n",
      "2768 tensor(7.3037, grad_fn=<MseLossBackward0>)\n",
      "2769 tensor(2.9470, grad_fn=<MseLossBackward0>)\n",
      "2770 tensor(4.4184, grad_fn=<MseLossBackward0>)\n",
      "2771 tensor(6.0958, grad_fn=<MseLossBackward0>)\n",
      "2772 tensor(8.1871, grad_fn=<MseLossBackward0>)\n",
      "2773 tensor(4.5769, grad_fn=<MseLossBackward0>)\n",
      "2774 tensor(14.7851, grad_fn=<MseLossBackward0>)\n",
      "2775 tensor(2.0595, grad_fn=<MseLossBackward0>)\n",
      "2776 tensor(5.8420, grad_fn=<MseLossBackward0>)\n",
      "2777 tensor(2.1213, grad_fn=<MseLossBackward0>)\n",
      "2778 tensor(3.6446, grad_fn=<MseLossBackward0>)\n",
      "2779 tensor(6.9970, grad_fn=<MseLossBackward0>)\n",
      "2780 tensor(2.4476, grad_fn=<MseLossBackward0>)\n",
      "2781 tensor(2.9208, grad_fn=<MseLossBackward0>)\n",
      "2782 tensor(12.6088, grad_fn=<MseLossBackward0>)\n",
      "2783 tensor(5.4046, grad_fn=<MseLossBackward0>)\n",
      "2784 tensor(2.8883, grad_fn=<MseLossBackward0>)\n",
      "2785 tensor(2.8333, grad_fn=<MseLossBackward0>)\n",
      "2786 tensor(12.3160, grad_fn=<MseLossBackward0>)\n",
      "2787 tensor(2.6340, grad_fn=<MseLossBackward0>)\n",
      "2788 tensor(3.0633, grad_fn=<MseLossBackward0>)\n",
      "2789 tensor(2.5097, grad_fn=<MseLossBackward0>)\n",
      "2790 tensor(6.9582, grad_fn=<MseLossBackward0>)\n",
      "2791 tensor(5.5953, grad_fn=<MseLossBackward0>)\n",
      "2792 tensor(3.5520, grad_fn=<MseLossBackward0>)\n",
      "2793 tensor(1.1526, grad_fn=<MseLossBackward0>)\n",
      "2794 tensor(2.7779, grad_fn=<MseLossBackward0>)\n",
      "2795 tensor(2.4224, grad_fn=<MseLossBackward0>)\n",
      "2796 tensor(1.9506, grad_fn=<MseLossBackward0>)\n",
      "2797 tensor(1.5842, grad_fn=<MseLossBackward0>)\n",
      "2798 tensor(3.1216, grad_fn=<MseLossBackward0>)\n",
      "2799 tensor(12.0961, grad_fn=<MseLossBackward0>)\n",
      "2800 tensor(2.3731, grad_fn=<MseLossBackward0>)\n",
      "2801 tensor(3.2714, grad_fn=<MseLossBackward0>)\n",
      "2802 tensor(1.4663, grad_fn=<MseLossBackward0>)\n",
      "2803 tensor(2.0033, grad_fn=<MseLossBackward0>)\n",
      "2804 tensor(2.4883, grad_fn=<MseLossBackward0>)\n",
      "2805 tensor(12.3495, grad_fn=<MseLossBackward0>)\n",
      "2806 tensor(2.0421, grad_fn=<MseLossBackward0>)\n",
      "2807 tensor(6.3960, grad_fn=<MseLossBackward0>)\n",
      "2808 tensor(13.1958, grad_fn=<MseLossBackward0>)\n",
      "2809 tensor(5.2560, grad_fn=<MseLossBackward0>)\n",
      "2810 tensor(13.3286, grad_fn=<MseLossBackward0>)\n",
      "2811 tensor(6.2119, grad_fn=<MseLossBackward0>)\n",
      "2812 tensor(1.9754, grad_fn=<MseLossBackward0>)\n",
      "2813 tensor(11.6949, grad_fn=<MseLossBackward0>)\n",
      "2814 tensor(2.5023, grad_fn=<MseLossBackward0>)\n",
      "2815 tensor(2.2237, grad_fn=<MseLossBackward0>)\n",
      "2816 tensor(3.4672, grad_fn=<MseLossBackward0>)\n",
      "2817 tensor(1.6104, grad_fn=<MseLossBackward0>)\n",
      "2818 tensor(11.1006, grad_fn=<MseLossBackward0>)\n",
      "2819 tensor(3.5549, grad_fn=<MseLossBackward0>)\n",
      "2820 tensor(5.8767, grad_fn=<MseLossBackward0>)\n",
      "2821 tensor(11.8808, grad_fn=<MseLossBackward0>)\n",
      "2822 tensor(2.3330, grad_fn=<MseLossBackward0>)\n",
      "2823 tensor(2.9673, grad_fn=<MseLossBackward0>)\n",
      "2824 tensor(6.9333, grad_fn=<MseLossBackward0>)\n",
      "2825 tensor(6.2658, grad_fn=<MseLossBackward0>)\n",
      "2826 tensor(8.2428, grad_fn=<MseLossBackward0>)\n",
      "2827 tensor(5.5300, grad_fn=<MseLossBackward0>)\n",
      "2828 tensor(3.9202, grad_fn=<MseLossBackward0>)\n",
      "2829 tensor(6.7846, grad_fn=<MseLossBackward0>)\n",
      "2830 tensor(6.4197, grad_fn=<MseLossBackward0>)\n",
      "2831 tensor(6.5776, grad_fn=<MseLossBackward0>)\n",
      "2832 tensor(6.8794, grad_fn=<MseLossBackward0>)\n",
      "2833 tensor(5.9418, grad_fn=<MseLossBackward0>)\n",
      "2834 tensor(3.6218, grad_fn=<MseLossBackward0>)\n",
      "2835 tensor(3.2962, grad_fn=<MseLossBackward0>)\n",
      "2836 tensor(1.5284, grad_fn=<MseLossBackward0>)\n",
      "2837 tensor(12.0028, grad_fn=<MseLossBackward0>)\n",
      "2838 tensor(11.4290, grad_fn=<MseLossBackward0>)\n",
      "2839 tensor(7.0443, grad_fn=<MseLossBackward0>)\n",
      "2840 tensor(2.6602, grad_fn=<MseLossBackward0>)\n",
      "2841 tensor(3.1028, grad_fn=<MseLossBackward0>)\n",
      "2842 tensor(2.0451, grad_fn=<MseLossBackward0>)\n",
      "2843 tensor(12.6206, grad_fn=<MseLossBackward0>)\n",
      "2844 tensor(2.2964, grad_fn=<MseLossBackward0>)\n",
      "2845 tensor(2.4391, grad_fn=<MseLossBackward0>)\n",
      "2846 tensor(11.4303, grad_fn=<MseLossBackward0>)\n",
      "2847 tensor(2.9475, grad_fn=<MseLossBackward0>)\n",
      "2848 tensor(6.6373, grad_fn=<MseLossBackward0>)\n",
      "2849 tensor(2.4114, grad_fn=<MseLossBackward0>)\n",
      "2850 tensor(3.9235, grad_fn=<MseLossBackward0>)\n",
      "2851 tensor(2.5556, grad_fn=<MseLossBackward0>)\n",
      "2852 tensor(2.5591, grad_fn=<MseLossBackward0>)\n",
      "2853 tensor(6.3453, grad_fn=<MseLossBackward0>)\n",
      "2854 tensor(2.0354, grad_fn=<MseLossBackward0>)\n",
      "2855 tensor(2.2921, grad_fn=<MseLossBackward0>)\n",
      "2856 tensor(2.7007, grad_fn=<MseLossBackward0>)\n",
      "2857 tensor(12.1926, grad_fn=<MseLossBackward0>)\n",
      "2858 tensor(14.5262, grad_fn=<MseLossBackward0>)\n",
      "2859 tensor(2.3736, grad_fn=<MseLossBackward0>)\n",
      "2860 tensor(2.2163, grad_fn=<MseLossBackward0>)\n",
      "2861 tensor(2.3785, grad_fn=<MseLossBackward0>)\n",
      "2862 tensor(2.9804, grad_fn=<MseLossBackward0>)\n",
      "2863 tensor(2.3468, grad_fn=<MseLossBackward0>)\n",
      "2864 tensor(1.5787, grad_fn=<MseLossBackward0>)\n",
      "2865 tensor(11.6644, grad_fn=<MseLossBackward0>)\n",
      "2866 tensor(7.1656, grad_fn=<MseLossBackward0>)\n",
      "2867 tensor(2.5965, grad_fn=<MseLossBackward0>)\n",
      "2868 tensor(3.6679, grad_fn=<MseLossBackward0>)\n",
      "2869 tensor(2.8449, grad_fn=<MseLossBackward0>)\n",
      "2870 tensor(5.8388, grad_fn=<MseLossBackward0>)\n",
      "2871 tensor(2.5339, grad_fn=<MseLossBackward0>)\n",
      "2872 tensor(2.2949, grad_fn=<MseLossBackward0>)\n",
      "2873 tensor(4.2679, grad_fn=<MseLossBackward0>)\n",
      "2874 tensor(1.8953, grad_fn=<MseLossBackward0>)\n",
      "2875 tensor(3.0165, grad_fn=<MseLossBackward0>)\n",
      "2876 tensor(1.8930, grad_fn=<MseLossBackward0>)\n",
      "2877 tensor(1.5660, grad_fn=<MseLossBackward0>)\n",
      "2878 tensor(2.4630, grad_fn=<MseLossBackward0>)\n",
      "2879 tensor(2.3296, grad_fn=<MseLossBackward0>)\n",
      "2880 tensor(7.3832, grad_fn=<MseLossBackward0>)\n",
      "2881 tensor(6.5319, grad_fn=<MseLossBackward0>)\n",
      "2882 tensor(1.9753, grad_fn=<MseLossBackward0>)\n",
      "2883 tensor(1.7134, grad_fn=<MseLossBackward0>)\n",
      "2884 tensor(3.6638, grad_fn=<MseLossBackward0>)\n",
      "2885 tensor(1.9411, grad_fn=<MseLossBackward0>)\n",
      "2886 tensor(4.3533, grad_fn=<MseLossBackward0>)\n",
      "2887 tensor(3.5044, grad_fn=<MseLossBackward0>)\n",
      "2888 tensor(2.8577, grad_fn=<MseLossBackward0>)\n",
      "2889 tensor(3.2206, grad_fn=<MseLossBackward0>)\n",
      "2890 tensor(7.7487, grad_fn=<MseLossBackward0>)\n",
      "2891 tensor(3.2013, grad_fn=<MseLossBackward0>)\n",
      "2892 tensor(1.9762, grad_fn=<MseLossBackward0>)\n",
      "2893 tensor(2.1238, grad_fn=<MseLossBackward0>)\n",
      "2894 tensor(3.2736, grad_fn=<MseLossBackward0>)\n",
      "2895 tensor(2.4450, grad_fn=<MseLossBackward0>)\n",
      "2896 tensor(1.5396, grad_fn=<MseLossBackward0>)\n",
      "2897 tensor(2.3159, grad_fn=<MseLossBackward0>)\n",
      "2898 tensor(3.0698, grad_fn=<MseLossBackward0>)\n",
      "2899 tensor(7.1144, grad_fn=<MseLossBackward0>)\n",
      "2900 tensor(1.5024, grad_fn=<MseLossBackward0>)\n",
      "2901 tensor(2.5055, grad_fn=<MseLossBackward0>)\n",
      "2902 tensor(5.9388, grad_fn=<MseLossBackward0>)\n",
      "2903 tensor(14.4805, grad_fn=<MseLossBackward0>)\n",
      "2904 tensor(2.9298, grad_fn=<MseLossBackward0>)\n",
      "2905 tensor(5.6048, grad_fn=<MseLossBackward0>)\n",
      "2906 tensor(2.1346, grad_fn=<MseLossBackward0>)\n",
      "2907 tensor(18.9012, grad_fn=<MseLossBackward0>)\n",
      "2908 tensor(2.2303, grad_fn=<MseLossBackward0>)\n",
      "2909 tensor(6.3194, grad_fn=<MseLossBackward0>)\n",
      "2910 tensor(5.7536, grad_fn=<MseLossBackward0>)\n",
      "2911 tensor(8.8954, grad_fn=<MseLossBackward0>)\n",
      "2912 tensor(6.6816, grad_fn=<MseLossBackward0>)\n",
      "2913 tensor(6.7416, grad_fn=<MseLossBackward0>)\n",
      "2914 tensor(6.8293, grad_fn=<MseLossBackward0>)\n",
      "2915 tensor(4.2283, grad_fn=<MseLossBackward0>)\n",
      "2916 tensor(2.8393, grad_fn=<MseLossBackward0>)\n",
      "2917 tensor(2.8090, grad_fn=<MseLossBackward0>)\n",
      "2918 tensor(11.4441, grad_fn=<MseLossBackward0>)\n",
      "2919 tensor(2.2337, grad_fn=<MseLossBackward0>)\n",
      "2920 tensor(2.5491, grad_fn=<MseLossBackward0>)\n",
      "2921 tensor(2.8656, grad_fn=<MseLossBackward0>)\n",
      "2922 tensor(1.2476, grad_fn=<MseLossBackward0>)\n",
      "2923 tensor(1.8431, grad_fn=<MseLossBackward0>)\n",
      "2924 tensor(2.3719, grad_fn=<MseLossBackward0>)\n",
      "2925 tensor(3.0929, grad_fn=<MseLossBackward0>)\n",
      "2926 tensor(6.4591, grad_fn=<MseLossBackward0>)\n",
      "2927 tensor(1.6534, grad_fn=<MseLossBackward0>)\n",
      "2928 tensor(2.2583, grad_fn=<MseLossBackward0>)\n",
      "2929 tensor(11.1619, grad_fn=<MseLossBackward0>)\n",
      "2930 tensor(2.8042, grad_fn=<MseLossBackward0>)\n",
      "2931 tensor(7.1590, grad_fn=<MseLossBackward0>)\n",
      "2932 tensor(14.8911, grad_fn=<MseLossBackward0>)\n",
      "2933 tensor(1.6205, grad_fn=<MseLossBackward0>)\n",
      "2934 tensor(3.5707, grad_fn=<MseLossBackward0>)\n",
      "2935 tensor(4.5533, grad_fn=<MseLossBackward0>)\n",
      "2936 tensor(1.8997, grad_fn=<MseLossBackward0>)\n",
      "2937 tensor(4.0038, grad_fn=<MseLossBackward0>)\n",
      "2938 tensor(1.7984, grad_fn=<MseLossBackward0>)\n",
      "2939 tensor(5.5399, grad_fn=<MseLossBackward0>)\n",
      "2940 tensor(7.1617, grad_fn=<MseLossBackward0>)\n",
      "2941 tensor(7.1359, grad_fn=<MseLossBackward0>)\n",
      "2942 tensor(1.5427, grad_fn=<MseLossBackward0>)\n",
      "2943 tensor(6.8022, grad_fn=<MseLossBackward0>)\n",
      "2944 tensor(2.1582, grad_fn=<MseLossBackward0>)\n",
      "2945 tensor(5.0132, grad_fn=<MseLossBackward0>)\n",
      "2946 tensor(5.6791, grad_fn=<MseLossBackward0>)\n",
      "2947 tensor(4.1108, grad_fn=<MseLossBackward0>)\n",
      "2948 tensor(2.5607, grad_fn=<MseLossBackward0>)\n",
      "2949 tensor(1.9589, grad_fn=<MseLossBackward0>)\n",
      "2950 tensor(2.5949, grad_fn=<MseLossBackward0>)\n",
      "2951 tensor(2.2504, grad_fn=<MseLossBackward0>)\n",
      "2952 tensor(11.5697, grad_fn=<MseLossBackward0>)\n",
      "2953 tensor(5.4195, grad_fn=<MseLossBackward0>)\n",
      "2954 tensor(3.4039, grad_fn=<MseLossBackward0>)\n",
      "2955 tensor(2.4020, grad_fn=<MseLossBackward0>)\n",
      "2956 tensor(2.2330, grad_fn=<MseLossBackward0>)\n",
      "2957 tensor(5.7078, grad_fn=<MseLossBackward0>)\n",
      "2958 tensor(2.1921, grad_fn=<MseLossBackward0>)\n",
      "2959 tensor(18.7699, grad_fn=<MseLossBackward0>)\n",
      "2960 tensor(1.6860, grad_fn=<MseLossBackward0>)\n",
      "2961 tensor(2.5650, grad_fn=<MseLossBackward0>)\n",
      "2962 tensor(2.9763, grad_fn=<MseLossBackward0>)\n",
      "2963 tensor(2.6317, grad_fn=<MseLossBackward0>)\n",
      "2964 tensor(6.4662, grad_fn=<MseLossBackward0>)\n",
      "2965 tensor(2.4026, grad_fn=<MseLossBackward0>)\n",
      "2966 tensor(1.5490, grad_fn=<MseLossBackward0>)\n",
      "2967 tensor(3.0546, grad_fn=<MseLossBackward0>)\n",
      "2968 tensor(2.7832, grad_fn=<MseLossBackward0>)\n",
      "2969 tensor(10.8585, grad_fn=<MseLossBackward0>)\n",
      "2970 tensor(5.2793, grad_fn=<MseLossBackward0>)\n",
      "2971 tensor(5.7970, grad_fn=<MseLossBackward0>)\n",
      "2972 tensor(2.4549, grad_fn=<MseLossBackward0>)\n",
      "2973 tensor(2.1408, grad_fn=<MseLossBackward0>)\n",
      "2974 tensor(2.5758, grad_fn=<MseLossBackward0>)\n",
      "2975 tensor(2.1510, grad_fn=<MseLossBackward0>)\n",
      "2976 tensor(2.8965, grad_fn=<MseLossBackward0>)\n",
      "2977 tensor(1.4739, grad_fn=<MseLossBackward0>)\n",
      "2978 tensor(7.2622, grad_fn=<MseLossBackward0>)\n",
      "2979 tensor(10.4009, grad_fn=<MseLossBackward0>)\n",
      "2980 tensor(3.9928, grad_fn=<MseLossBackward0>)\n",
      "2981 tensor(2.2002, grad_fn=<MseLossBackward0>)\n",
      "2982 tensor(2.6234, grad_fn=<MseLossBackward0>)\n",
      "2983 tensor(2.0666, grad_fn=<MseLossBackward0>)\n",
      "2984 tensor(2.4420, grad_fn=<MseLossBackward0>)\n",
      "2985 tensor(2.8948, grad_fn=<MseLossBackward0>)\n",
      "2986 tensor(2.8881, grad_fn=<MseLossBackward0>)\n",
      "2987 tensor(4.6021, grad_fn=<MseLossBackward0>)\n",
      "2988 tensor(3.0372, grad_fn=<MseLossBackward0>)\n",
      "2989 tensor(1.9247, grad_fn=<MseLossBackward0>)\n",
      "2990 tensor(6.5447, grad_fn=<MseLossBackward0>)\n",
      "2991 tensor(13.9441, grad_fn=<MseLossBackward0>)\n",
      "2992 tensor(10.8599, grad_fn=<MseLossBackward0>)\n",
      "2993 tensor(3.0218, grad_fn=<MseLossBackward0>)\n",
      "2994 tensor(2.8710, grad_fn=<MseLossBackward0>)\n",
      "2995 tensor(2.6563, grad_fn=<MseLossBackward0>)\n",
      "2996 tensor(17.0183, grad_fn=<MseLossBackward0>)\n",
      "2997 tensor(11.2472, grad_fn=<MseLossBackward0>)\n",
      "2998 tensor(9.2244, grad_fn=<MseLossBackward0>)\n",
      "2999 tensor(2.2682, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad\n",
    "for epoch in range(3000):\n",
    "    for bx, by in loader:\n",
    "        # 현재 파라미터를 이용해서 예측갑을 구하고 f(x)=ax^2+bx+c\n",
    "        pred = model(bx)\n",
    "        loss = loss_fn(pred, by)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        opt.step()\n",
    "    print(epoch, loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6cf276b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f8c1a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.192817211151123\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, pred, '.')\n",
    "plt.plot(x, y, '.')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\", ((pred - y) ** 2).mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88e67b",
   "metadata": {},
   "source": [
    "선형 vs 2차 비교\n",
    "\n",
    "            선형 y=3x+4      |      2차 y=2x^2+3x+5\n",
    "kernel        [x, 1]                  [x^2, x, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
