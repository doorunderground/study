{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 과제\n",
    "# 2차방정식 풀이 해보기\n",
    "# 현재까지 만들어진 소스를 가지고\n",
    "# 2차방정식을 풀어본다\n",
    "# 2차 방정식 artificial 샘플을 만들고\n",
    "# 2차 방정식을 위한 model (kernel 생성,  weight 크기 수정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45925509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "a[[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093c83be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2987ca89870>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 100\n",
    "x = torch.rand(m, 1) * 2\n",
    "y = x * 3 + 4 + torch.randn(m, 1)\n",
    "\n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b7a532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1]), torch.Size([20, 1]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(x, y))\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size = 20, shuffle = True)\n",
    "\n",
    "for xi , yi in loader: # DataLoader에서 첫 번째 배치 하나만 꺼내서 확인\n",
    "    break\n",
    "\n",
    "xi.shape, yi.shape, xi.dtype, yi.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d58bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1627],\n",
       "        [3.8577]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.pinv(torch.cat([x, x**0], axis = 1)) @ y\n",
    "\n",
    "# [[1],\n",
    "# [2],\n",
    "# [3]]\n",
    "# -> \n",
    "# [[1, 1],\n",
    "# [2, 1],\n",
    "# [3, 1]]\n",
    "\n",
    "# shape = (100, 2)\n",
    "# a(기울기) / b(절편)을 구한거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135e7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((1, 5), requires_grad= True) # 이 파라미터는 학습 대상이다.\n",
    "B1 = torch.randn((5,), requires_grad= True)\n",
    "W2 = torch.randn((5, 1), requires_grad= True)\n",
    "B2 = torch.randn((1,), requires_grad= True)\n",
    "\n",
    "# x.shape = (batch_size, 1)\n",
    "# (batch, 1) @ (1, 5)\n",
    "#  = (batch, 5)\n",
    "\n",
    "# (batch, 5) @ (5, 1)\n",
    "#  = (batch, 1)\n",
    "\n",
    "def model(x):\n",
    "    h = torch.relu(x @ W1 + B1)\n",
    "    y = (h @ W2 + B2)\n",
    "    return y\n",
    "\n",
    "# optimizer(최적화)\n",
    "opt = torch.optim.Adam([W1, B1, W2, B2], lr=0.0001)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db6ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(12.8785, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(18.0724, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(16.2836, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(14.0841, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(15.5683, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(17.5049, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(12.2009, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(17.5845, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(17.6413, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(16.1682, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad\n",
    "\n",
    "# 100개 데이터\n",
    "# 20개씩 나눔\n",
    "# 한 epoch당 5번 update 발생\n",
    "\n",
    "# pred = model(bx)\n",
    "# W1, B1, W2, B2로 예측값 계산\n",
    "\n",
    "for epoch in range(10):\n",
    "    for bx, by in loader:\n",
    "        # 현재 파라미터를 이용해서 예측값을 구하고 f(x) = ax+b\n",
    "        pred = model(bx)\n",
    "\n",
    "        # 에러를 구하고\n",
    "        loss = loss_fn(pred, by)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        opt.step()\n",
    "    print(epoch, loss)         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b8063",
   "metadata": {},
   "source": [
    "23 tensor(58.8977, grad_fn=<MseLossBackward0>)\n",
    "\n",
    "24 tensor(62.9800, grad_fn=<MseLossBackward0>)\n",
    "\n",
    "-> 예측과 실제값 차이의 제곱 평균\n",
    "\n",
    "숫자가 클수록 예측이 많이 틀린 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6609cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2576, -0.0177,  0.1657,  2.8885, -0.0877]], requires_grad=True) tensor([-0.8647,  0.8362,  0.7735,  0.2636,  0.6314], requires_grad=True) tensor([[-2.0342],\n",
      "        [ 0.5230],\n",
      "        [ 0.7603],\n",
      "        [ 0.5918],\n",
      "        [-0.8804]], requires_grad=True) tensor([0.7458], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W1, B1, W2, B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24c153d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29802247010>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "\n",
    "plt.plot(x, pred, '.')\n",
    "\n",
    "((pred-y) ** 2).mean()\n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe62cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74974d7c",
   "metadata": {},
   "source": [
    "# 과제\n",
    "1. 2차방정식 풀이 해보기\n",
    "2. 현재까지 만들어진 소스를 가지고\n",
    "3. 2차방정식을 풀어본다\n",
    "4. 2차 방정식 artificial 샘플을 만들고\n",
    "5. 2차 방정식을 위한 model (kernel 생성,  weight 크기 수정)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c405a",
   "metadata": {},
   "source": [
    "y= 2X^2 + 3X + 5 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe51651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x298022e48b0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANElJREFUeJzt3Q94k/W5+P+7RVqg0PLXlo4WKoJF+SMiQoWhsp5TGdtEmIKy31CZfxEF5lHKb+rcPBbZmTCdgDIFPRNBj4BzCrtcnShSASso6EDFYqvQYh1t+SOF0Xyv+6NPTNKkTdL0yb/367pypXnyNHmSNsmd+3N/7k+Cw+FwCAAAgE0S7bojAAAAgg8AAGA7Mh8AAMBWBB8AAMBWBB8AAMBWBB8AAMBWBB8AAMBWBB8AAMBWp0mEaWhokP3790unTp0kISEh3IcDAAD8oD1LDx8+LJmZmZKYmBhdwYcGHllZWeE+DAAAEISKigrp1atXdAUfmvGwDj41NTXchwMAAPxQV1dnkgfW53hUBR/WUIsGHgQfAABEF39KJig4BQAAtiL4AAAAtiL4AAAAtiL4AAAAtiL4AAAAtiL4AAAAtiL4AAAAtiL4AAAAtiL4AAAAtiL4AAAAtiL4AAAgjhyo/Vo2760251ERfJw6dUruvvtuycnJkfbt20vfvn3lt7/9rVlG16I/33PPPdKzZ0+zT35+vnz88cetcewAACAAq7eVy6j5r8nVy7aYc70c8cHHgw8+KEuWLJE//vGP8s9//tNcXrBggTzyyCPOffTyww8/LEuXLpUtW7ZISkqKFBQUyPHjx1vj+AEAgB8001G4Zqc0fJsv0PN5a3aFJQMS0Kq2mzdvlssuu0zGjx9vLvfp00eeffZZ2bp1qzPrsWjRIvnVr35l9lNPP/20pKeny7p162TKlCmt8RgAAEAzyqqPOgMPyymHQ/ZVH5Oeae0lYjMfF154oRQXF8tHH31kLr/33nuyadMmGTdunLlcVlYmlZWVZqjFkpaWJiNGjJCSkhKvt1lfXy91dXVuJwAAEFo53VMk0WO1+zYJCdKnewexW0DBx9y5c032Ijc3V9q2bStDhw6VWbNmydSpU831GngozXS40svWdZ6KiopMgGKdsrKygn80AADAK81uFE0cZAIOpecPTBxoe9Yj4GGX5557Tp555hlZuXKlnHPOObJjxw4TfGRmZsq0adOCOoDCwkKZM2eO87JmPghAAAAIvcnDs2VM/x5mqEUzHuEIPAIOPv7rv/7Lmf1QgwYNks8++8xkLzT4yMjIMNurqqrMbBeLXj733HO93mZycrI5AQCA1qcBR7iCjqCGXY4dOyaJie6/0qZNG2loaDA/6xRcDUC0LsQ1k6GzXvLy8kJ1zAAAIIoFlPn48Y9/LP/93/8t2dnZZthl+/bt8tBDD8l1111nrk9ISDDDMPfff7/069fPBCPaF0SHZSZMmNBajwEAADRBp9PqbBctOg131iPg4EP7eWgwccstt8jBgwdNUHHjjTeapmKWO++8U44ePSo33HCD1NTUyOjRo2XDhg3Srl271jh+AADQBG0kZvX30NkuWnSqtR/hlOBwbU8aAXSYRme91NbWSmpqargPBwCAqM54jJr/mlt/D53lsmnuJSHPgATy+c3aLgAAxGFjsXAi+AAAIEblRFBjMVcEHwAAxKieEdRYLOiCUwAAEF0mR0hjMVcEHwAAxLieEdBYzBXDLgAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAAwFYEHwAARIkDtV/L5r3V5jyanRbuAwAAAM1bva1cCtfslAaHSGKCSNHEQTJ5eLZEIzIfAABEuAO1XzsDD6Xn89bsitoMCMEHAAARrqz6qDPwsJxyOGRf9TGJRgQfAABEuJzuKWaoxVWbhATp072DRCOCDwAAIlzPtPamxkMDDqXnD0wcaLZHIwpOAQCIApOHZ8uY/j3MUItmPKI18FAEHwAARImeae2jOuiwMOwCAEAEOxAjvT1ckfkAACBCrY6h3h5BZz769OkjCQkJjU4zZsww1x8/ftz83K1bN+nYsaNMmjRJqqqqWuvYAQCIWQdirLdH0MHHtm3b5MCBA87Tq6++arZfccUV5nz27Nny0ksvyfPPPy8bN26U/fv3y8SJE1vnyAEAiGFlMdbbI+hhlx49erhdnj9/vvTt21cuuugiqa2tlSeeeEJWrlwpY8eONdcvX75cBgwYIG+//baMHDkytEcOAEAc9PZocMRGb4+QFJyeOHFC/vznP8t1111nhl5KS0vl5MmTkp+f79wnNzdXsrOzpaSkxOft1NfXS11dndsJAIB41zPGenuEpOB03bp1UlNTI9dcc425XFlZKUlJSdK5c2e3/dLT0811vhQVFcl9990X7GEAABCzJsdQb4+QZD50iGXcuHGSmZnZogMoLCw0QzbWqaKiokW3BwBALOmZ1l7y+naLmcAj6MzHZ599Jn//+99lzZo1zm0ZGRlmKEazIa7ZD53totf5kpycbE4AACA+BJX50ELS008/XcaPH+/cNmzYMGnbtq0UFxc7t+3Zs0fKy8slLy8vNEcLAADiL/PR0NBggo9p06bJaad99+tpaWkyffp0mTNnjnTt2lVSU1Nl5syZJvBgpgsAAAg6+NDhFs1m6CwXTwsXLpTExETTXExnsRQUFMjixYsDvQsAABDDEhwOh0cLk/DSqbaaRdHiU82eAACAyBfI5zcLywEAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfAAAgMgOPr744gv52c9+Jt26dZP27dvLoEGD5J133nFe73A45J577pGePXua6/Pz8+Xjjz8O9XEDAIB4CD4OHToko0aNkrZt28r69evlww8/lN///vfSpUsX5z4LFiyQhx9+WJYuXSpbtmyRlJQUKSgokOPHj7fG8QMAgCiT4NBUhZ/mzp0rb731lrz55pter9ebyszMlF/+8pdyxx13mG21tbWSnp4uK1askClTpjR7H3V1dZKWlmZ+LzU1NZDHAgAAwiSQz++AMh9/+ctf5Pzzz5crrrhCTj/9dBk6dKgsW7bMeX1ZWZlUVlaaoRaLHsiIESOkpKQkmMcCAABiTEDBx6effipLliyRfv36yd/+9je5+eab5bbbbpOnnnrKXK+Bh9JMhyu9bF3nqb6+3kRLricAABC7Tgtk54aGBpP5eOCBB8xlzXzs2rXL1HdMmzYtqAMoKiqS++67L6jfBQAAMZ750BksZ599ttu2AQMGSHl5ufk5IyPDnFdVVbnto5et6zwVFhaa8SHrVFFREehjAAAAsRp86EyXPXv2uG376KOPpHfv3ubnnJwcE2QUFxc7r9dhFJ31kpeX5/U2k5OTTWGK6wkAAMSugIZdZs+eLRdeeKEZdrnyyitl69at8vjjj5uTSkhIkFmzZsn9999v6kI0GLn77rvNDJgJEya01mMAAACxGnwMHz5c1q5da4ZKfvOb35jgYtGiRTJ16lTnPnfeeaccPXpUbrjhBqmpqZHRo0fLhg0bpF27dq1x/AAAIJb7fNiBPh8AAESfVuvzAQAA0FIEHwAAwFYEHwAAwFYEHwAANONA7deyeW+1OYfNs10AAIg3q7eVS+GandLgEElMECmaOEgmD88O92FFNTIfAAD4oJkOK/BQej5vzS4yIC1E8AEAgA9l1UedgYfllMMh+6qP8Zy1AMEHAAA+5HRPMUMtrtokJEif7h14zlqA4AMAAB96prU3NR4acCg9v3PcWSYjQvFp8Cg4BQCgCVpcOqZ/DzPU8v7nNfLg+t0Un7YQmQ8AAPzIgOhQy4Mbvgk8FMWnwSP4AADADxSfhg7BBwAAfqD4NHQIPgAACLL49IGJA812BIaCUwAAgig+1RoQAo/gEHwAABAADTgIOlqGYRcAAGArgg8AAGArgg8AQFzTTqWb91bTsdRG1HwAAOLW6m3lzlVrdQ0Xnc2iRaVoXWQ+AABxm/GwAg9Fx1L7EHwAAOISHUvDh+ADABCX6FgaPgQfAIC4RMfS8KHgFAAQt+hYGh4EHwCAuEbHUvsx7AIAAGxF8AEAAGxF8AEAAGxF8AEAAGxF8AEAAGxF8AEAAGxF8AEAACI3+Pj1r38tCQkJbqfc3Fzn9cePH5cZM2ZIt27dpGPHjjJp0iSpqqpqjeMGAADxkvk455xz5MCBA87Tpk2bnNfNnj1bXnrpJXn++edl48aNsn//fpk4cWKojxkAAMRTh9PTTjtNMjIyGm2vra2VJ554QlauXCljx44125YvXy4DBgyQt99+W0aOHBmaIwYAAPGV+fj4448lMzNTzjjjDJk6daqUl5eb7aWlpXLy5EnJz8937qtDMtnZ2VJSUuLz9urr66Wurs7tBAAAYldAwceIESNkxYoVsmHDBlmyZImUlZXJ97//fTl8+LBUVlZKUlKSdO7c2e130tPTzXW+FBUVSVpamvOUlZUV/KMBAACxNewybtw458+DBw82wUjv3r3lueeek/bt2wd1AIWFhTJnzhznZc18EIAAABC7WjTVVrMc/fv3l08++cTUgZw4cUJqamrc9tHZLt5qRCzJycmSmprqdgIAALGrRcHHkSNHZO/evdKzZ08ZNmyYtG3bVoqLi53X79mzx9SE5OXlheJYAQBAvA273HHHHfLjH//YDLXoNNp7771X2rRpI1dddZWp15g+fboZQunatavJYMycOdMEHsx0AQAAQQUfn3/+uQk0vvrqK+nRo4eMHj3aTKPVn9XChQslMTHRNBfTWSwFBQWyePHiQO4CAADEuASHw+GQCKIFp5pF0b4h1H8AABAdAvn8Zm0XAABgK4IPAABgK4IPAABgK4IPAABgK4IPAABgK4IPAABgK4IPAABgK4IPAABgK4IPAEDUO1D7tWzeW23OEWPt1QEAiDSrt5VL4Zqd0uAQSUwQKZo4SCYPzw73YaEJZD4AAFFLMx1W4KH0fN6aXWRAIhzBBwAgapVVH3UGHpZTDofsqz4WrkOCHwg+AABRK6d7ihlqcdUmIUH6dO8QrkOCHwg+AABRq2dae1PjoQGH0vMHJg402xG5KDgFAEQ1LS4d07+HGWrRjAeBR+Qj+AAARD0NOAg6ogfDLgAAwFYEHwAAwFYEHwAAwFYEHwCAqEIr9ehHwSkAIGrQSj02kPkAAERFtuOv7++XuS/QSj0WkPkAAERNtsOT1UqdabbRhcwHACBqFo7zRCv16ETwAQCIqoXjLLRSj14MuwAAIn7hONcARC8/PGWoDOvTheGWKEXmAwAQVQvH6eUfDckk8IhiZD4AABGNheNiD8EHACDisXBcbGHYBQAA2IrgAwAAEHwAAIDY1aLMx/z58yUhIUFmzZrl3Hb8+HGZMWOGdOvWTTp27CiTJk2SqqqqUBwrAACI5+Bj27Zt8thjj8ngwYPdts+ePVteeuklef7552Xjxo2yf/9+mThxYiiOFQAAxGvwceTIEZk6daosW7ZMunTp4txeW1srTzzxhDz00EMyduxYGTZsmCxfvlw2b94sb7/9diiPGwAAxFPwocMq48ePl/z8fLftpaWlcvLkSbftubm5kp2dLSUlJV5vq76+Xurq6txOAAAgdgXc52PVqlXy7rvvmmEXT5WVlZKUlCSdO3d2256enm6u86aoqEjuu+++QA8DAADEQ+ajoqJCbr/9dnnmmWekXbt2ITmAwsJCM1xjnfQ+AABA7Aoo+NBhlYMHD8p5550np512mjlpUenDDz9sftYMx4kTJ6Smpsbt93S2S0ZGhtfbTE5OltTUVLcTACC+HKj9WjbvrTbniH0BDbv84Ac/kJ07d7ptu/baa01dx1133SVZWVnStm1bKS4uNlNs1Z49e6S8vFzy8vJCe+QAgJiwelu5FK7ZaVau1RVrdeE4Xc8FsSug4KNTp04ycOBAt20pKSmmp4e1ffr06TJnzhzp2rWryWLMnDnTBB4jR44M7ZEDAKKeZjqswEPp+bw1u2RM/x6sWhvDQr6w3MKFCyUxMdFkPnQmS0FBgSxevDjUdwMAiAFl1UedgYfllMMh+6qPEXzEsASHw+HxZw8vnWqblpZmik+p/wCA2M98jJr/mlsA0iYhQTbNvYTgI8oE8vnNwnIAgLDpmdbe1HhowKH0/IGJAwk8YlzIh10AAAiEFpdqjYcOtfTp3oHAIw4QfAAAIiIDoifEB4ZdAACArQg+AACArQg+AACArQg+AACArQg+AACArQg+AACArQg+AACArQg+AACt0jZ9895qcw54oskYACCkVm8rd65Um5ggpn26djEFLGQ+AAAho5kOK/BQej5vzS4yIHBD8AEACJmy6qNuK9SqUw6HWbcFsBB8AABCJqd7ihlqcaUr1eqCcYCF4AMAEDK6OJzWeGjAofT8gYkDWTQObig4BQCElBaXjunfwwy1aMaD1WrhieADABB0canWeOhQi2eAoZcJOuALwQcAIGBMp0VLUPMBAAgI02nRUgQfAICAMJ0WLUXwAQAICNNp0VIEHwCAgDCdFi1FwSkAIGBMp0VLEHwAAILCdFoEi2EXAABgK4IPAABgq8R4n6u+eW81Sz0DAGCjuK35oDsfAADhEZeZD7rzAUDg75tkihEqcZn5aKo7HwshAYA7MsUItbjMfNCdDwD8Q6YYYQ8+lixZIoMHD5bU1FRzysvLk/Xr1zuvP378uMyYMUO6desmHTt2lEmTJklVVZVEGrrzAYB/WMcFYR926dWrl8yfP1/69esnDodDnnrqKbnssstk+/btcs4558js2bPl5Zdflueff17S0tLk1ltvlYkTJ8pbb70lkRLB6wtJMx905wMA/zPFrkPVbRISpE/3Djx9CFqCQ6OIFujatav87ne/k5/+9KfSo0cPWblypflZ7d69WwYMGCAlJSUycuRIv26vrq7OBC61tbUmuxIqjFkCQPDvn/PW7DK1cRp4PDBxoPkCBwT7+R10wempU6dMhuPo0aNm+KW0tFROnjwp+fn5zn1yc3MlOzu7yeCjvr7enFwP3q4xyzH9e1BgCgDNIFOMsBec7ty509RzJCcny0033SRr166Vs88+WyorKyUpKUk6d+7stn96erq5zpeioiITKVmnrKwsCTXGLAGg5bVyeX278YUN4Qk+zjrrLNmxY4ds2bJFbr75Zpk2bZp8+OGHQR9AYWGhSdFYp4qKCgk1ZrcAABA5Ah520ezGmWeeaX4eNmyYbNu2Tf7whz/I5MmT5cSJE1JTU+OW/dDZLhkZGT5vTzMoerJjdovnmCU9PQAAiMImYw0NDaZmQwORtm3bSnFxsZliq/bs2SPl5eWmJiTcGLMEACAKgw8dIhk3bpwpIj18+LCZ2fL666/L3/72N1OvMX36dJkzZ46ZAaOVrjNnzjSBh78zXVqbZjrIdgCAe+sB3hcR0cHHwYMH5ec//7kcOHDABBvacEwDj//4j/8w1y9cuFASExNN5kOzIQUFBbJ48eLWOnYAQBBoPYCo7/MRaq3V5yOU+MYAIFrp+9eo+a81ahq2ae4lZEAQ+X0+Yp2vAOOxjXtl/vrdoq9b7fqnhaw02wEQLVhYE5GA4COAlORjb+yVovW7nfvRrAxAtKFdOiJBXK5qG0w31PcqDpmMhyeduruv+pj9BwoAQWBhTUQCMh9+piS37Tsk3qpjNHpjgSUA0YTWAwg3gg8/U5LD+3RptF3dNS6XIi0AUYfWAwgnhl38TEkOyeritl0DkcIf5sqNF/W1/68GAEAUY6ptE7UfWsuhQyqus118bQcAIJ7VMdW29VKSpCoBRCP6EyGSUPMBADGOjqaINNR8AEActg/Q7UC4EHwAQJx2NAXCheADAOKgfYArnbVHfyKEE8FHK9GU5ua91aQ2AYQVHU0RiSg4bQUUdwGIJHQ0RaQh8xFiFHcBiNQMSF7fbvQnQkQg+AgxirsA2IXhXUQrhl1CjOWqAdiB4V1EMzIfIUZxF4BwD++SEUGkI/PRCijuAhCu4d03PvrSGZjoFFtdEFPfk4BIQuajlVDcBcDu3h0dkhLpZoqoQPABAFE+vKuByJ3jzpKjJ07RzRRRgeADAKKQDqXceelZ5mcdYpn/ym7Z/Em1eCRE6GaKiETNBwBEIS0qnb9+t/OyloD88R973fbRzMgDEwfS2wMRh+ADAKLQO/v+ZQIOX3QoZs0teTIkq4uNRwX4h2EXAIhCCd/We/iiQzHHTjTYdjxAIAg+ACAKePbuGNa7S6P6DlesXItIxrCLjfRNQ+fn6zQ5rVYHgJZ0M50/aZBzuxWI6FAMtR6IdAQfNgUdT24qkyc2ldH4B0BIupmO6d+jUUNDZf3MFxxEMoIPG76xzH1hp1thmOubB28QAILtZqrvH9bJwnsKogE1HzZ8Y/FWkW69eQBAMN1MrUwHEI0IPmz+xmLhzQOAPwWmyrWbKfUciAUMu9jwjcUzANFtNP4B4E+BqYYcc8flyqa5l1DPgfjMfBQVFcnw4cOlU6dOcvrpp8uECRNkz549bvscP35cZsyYId26dZOOHTvKpEmTpKqqSuJRo/UXROSGMTny1tyxrDIJwK8CUz0rWr9b/vLefsnr242aDsSEBIfD0VSTPDeXXnqpTJkyxQQg//73v2XevHmya9cu+fDDDyUlJcXsc/PNN8vLL78sK1askLS0NLn11lslMTFR3nrrLb/uo66uzvxebW2tpKamSqy8mVCBDsAfOtRy9bItjbZrxlS/uFBQikgVyOd3QMGHpy+//NJkQDZu3Chjxowxd9ijRw9ZuXKl/PSnPzX77N69WwYMGCAlJSUycuTIkB48AMQa/bJyYdFrXgvVn71+pMl+AJEokM/vFhWc6h2orl27mvPS0lI5efKk5OfnO/fJzc2V7OxsE3x4U19fbw7Y9QQA8UozG1rj4YkidcSSoIOPhoYGmTVrlowaNUoGDhxotlVWVkpSUpJ07tzZbd/09HRzna86Eo2UrFNWVlawhxRXrZUBxCZ9jQ/qlSa3XtLXOcWWGS6INUHPdtGiUq332LRpU4sOoLCwUObMmeO8rJkPAhD/WisDiO3X+l2X5srgXp3pWIqYE1TmQ4tI//rXv8o//vEP6dWrl3N7RkaGnDhxQmpqatz219kuep03ycnJZmzI9YTmWyuTAQFi/7W+YMMeAg/EpICCD61N1cBj7dq18tprr0lOTo7b9cOGDZO2bdtKcXGxc5tOxS0vL5e8vLzQHXUcaaq1MoDYwWsd8eS0QIdadCbLiy++aHp9WHUcWqvRvn17cz59+nQzjKJFqJrFmDlzpgk8/JnpAv8alellWisD0b+ytXU5JamNfHWkvtFrnSJTxKqAgo8lS5aY84svvtht+/Lly+Waa64xPy9cuND09dDmYjqTpaCgQBYvXhzKY47LRmVzdY0Yq+mQQ+SNj76k7gOI4nqOy4d+T9Zu/6JRZlNrTHUTRaaIZS3q89Ea6PPRmH47yit6zW2bvjFpu2UaDgHR8RoeNf81n2s9ebJaqt94Ud/WPjQg+vp8wB5PbiprtI26DyA2Fpn0RnfVYlMKyxGrCD4inL75POEl+NA/HHUfQHTVbgWCLxiIZQQfUfqN6RdjchhyAaJ0kUk9n3Te95yXvaHYFLEs6CZjCO9sl2tH5TRZRQ8g/Fxfl9oYMDejk2zbd0iG9+kiQ7K6yB0FZ5lp8x2SEuXl9yvlT5s+Na91ik0R6wg+ouQbkzYW0zSstzclOqAC0TW7xbVTsfVa1mDk2tF9WAEbcYHZLlH0DUq/IWmdh2vg4a2KnpkwgL2vTc+soz+zW3idIp5nu5D5iBL6puZtOKWprogMvwD2r8Wii8Jpw7DmZrfwOkU8I/iIwZoQCtWA1qfZjbkv7DTTYpW+BovW7zY/62vSahbmC69TxDNmu8RgFb1nTQiA0Cv97JDP4MJ8GUj47g3Wc3YLr1PEOzIfMUCL1sb079FsoRozYoDQ+dfR+iav197Rf7x6qHRNSXa+Lq3ZLU29ToF4QPAR4zUhFmbEAKGjr6df/+XDZvc7r3cXt9dlc69TIF4w7BIHNONhFcUpPdepu7RuBry/Xjbvrfb5+vB8PQEIHJmPOMCMGMA//mQIA1mnhVlngHdkPuJ0XQkq7YHgMoT+rtOi+7D+EuAdwUccaG5GTHNpZiCeM4Sl+w65vT70daPdSpuiLzV9zVHfAXjHsEucz4ihEBXw3TNHw/XbVm032/Tnqy7IksnDs0ybdE/m+hFZMqpv90aFpgDc0V49Tum3uHf2/UtuX7WD1uzAtzQYt9ZR0rSwxiH+1pU+evVQGT84k+cScauO9upoKuh4clOZPLGpzGvRHC2fEc9cM4RfHa2XW1du9+v3dChTsx0A/MOwS5x9q3NtB+0NhaiId1YvDg3UtXZDm4V5Yw3R0K0UCBzBR5xV8jcXeNCaHbEq0A6/us/ccblS9Mo367V4uuisHnLD9/vSrRQIAsFHnGiqN4GObT9y9VCK5BCzAi2stgKVnwzJNEUf1oJxrv6x+0uZ9YN+FJYCQWCqbZzw1ZvAvBFPGmQK5ajORywKtMOvBiqj5r8mVy/bYs47d2grM8f29brvO/sOteahAzGL4CNOe33oH/6GMTny1tyxTX4DVPQBQTRrqsOvv4HKuVmdvd72oWMnWuWYgVjHsEsc8Xf1W1f0AUEs9u/wVVjtK1DpkNRWxg3MkPW7Kt2uW/L6pzJ1ZG+yhkCAyHzEGQ048vp28+vNkgXpEA8dfv1diuD/y+vdaH9fGRQATSPzAZ9YkA7xlvWzAhWr0ZhnoOJvBgVA0wg+EJJ0NRDpU2it63Rf18u+AhVd00V7pg/7tnlYc4EJAP8RfMAn3mwR6QKpSQpk3zc++tLrvsHUTQFojLVd4Nc3S95sEYn/lzoV1jMzt2nuJY2CgtbaF0Bwa7tQcAq/OHz0RmUaLiKtJkmHSzbvrXbr4xHIdNtA9gUQHIZd0CRvqWpNO+sb9M7Pa+XBDbv9SmMH2toaCKYmSSeq3LZqe6P/yUDql6h1Alofwy7wyVv6Wd/cdcait1btvlLTrgGM/r6ul3HjRd47RgKB0P8tqwBU07j6b+n6r6nb3ioca/4nXfe1ikWbqg/xd18AgQ+7BJz5eOONN+R3v/udlJaWyoEDB2Tt2rUyYcIE5/UOh0PuvfdeWbZsmdTU1MioUaNkyZIl0q9fv0DvCmHmLf1s3tx9rBFjpaZdgw/PXiF6ZtbJSBC5cQwBCFpGA4LcjE6ybd8hSW6bKHev+8Dt+gYRWb5pn8wbPyCgYlEKS4HWFXDNx9GjR2XIkCHy6KOPer1+wYIF8vDDD8vSpUtly5YtkpKSIgUFBXL8+PFQHC8iYD0YX7ylsX0taPfg+t0+19YA/PXYG3tlwuLNcv/L/5R7X3QPPCx/2vSp838tkCZ7gewLIDABZz7GjRtnTt5o1mPRokXyq1/9Si677DKz7emnn5b09HRZt26dTJkyJdC7QwRNtfWW1nZ156VnNXqj1gAmwcvvaEDimSUBAvHYxr1uq836WrXZ+l9T1B0BMVhwWlZWJpWVlZKfn+/cpuM/I0aMkJKSEq/BR319vTm5jhkhcnimn03/gxd2mnS2p8G9OnstLtUaD88lyb1lSShKhb/0f2W+l2XuvdHs3fuf18jUP73tV3E0gCgLPjTwUJrpcKWXres8FRUVyX333RfKw0CIaQBhZSisMXZNdbvWfugbevWR4+bbqOcMGFNcmvDNUIu1/brRfdzugwXs4Iu3oFQv+8rAebrl4r7O/0nXlWo1qCbzBoRH2Pt8FBYWmspY61RRURHuQ0IzhmR1kfkuC3XpmQYiM5/dYTIcnm/y+uGhxaVvzR0rN4zJMfsue7PMzKTRoIMF7OCN/l888PKHcmHRa3L1si1u/y9fHalvth5Jry4clysXntmdvh1ALGc+MjIyzHlVVZX07NnTuV0vn3vuuV5/Jzk52ZwQncMx7352SG5dud3nt1DPGTB/erPMua8VnPzhqnN9fjjwzTQ+uWbCLPrz3Bd2Oqd6m2nfTdQgPXLVUPnRkEwTrLBGERDDmY+cnBwTgBQXF7vVcOisl7y8vFDeFSKABgZdUpKaTH/rP5hV2+Grc6TegK9lzBF/NFjQIMNbAalucp227et/T/9/hvVxXxDOytSxIBwQhZmPI0eOyCeffOJWZLpjxw7p2rWrZGdny6xZs+T+++83fT00GLn77rslMzPTrRcIYoe3bpCufjEmx5m98NU5Uj8kWC0Ulic3fZcdC4a34IK+HUCUdzh9/fXX5ZJLLmm0fdq0abJixQpnk7HHH3/cNBkbPXq0LF68WPr37x/yDmmIoBS5lxkwGmhonYfrh0BTnSNZwC4++JrVpNvf2fcvuW3VDq+N7KzkWFNvWLePPVOmjMhmuA4Ig0A+v2mvjpDQDw7tJKkNnTSz0VRLaoKM+OVrVpO3Gg9XPxrUU/7/Hw0wU72t4NWbR68eKuMHZ7bugwDgFcEHwiYcgQX9QaLjufO1VP2aW/Lk8sWbfQYeVgZN6bGmJLWRzw99LTNWbnfbT0s6Nntk2gDEyNougL89Qez4gKQ/SPBa+twFGrj4KjjWdVl8BR5WBk0zHne9sNO5/cFJg8zJ8/gJPIDoQPCBqP2AtGZFuE7dnbtmJ82j/OCrt4q/jbeCCVx8FRzXHDvhdZbUI1cPlV5d2kvFoa/NdG5XGoiUFI41GRG7M20AYqDJGKAfhJv3VjdaaK655mOlnx1qVHyopQDaewTBZSGsNVCa0tTfxdff0jLlgixn4ai++dx00Rmy+PW9jfa7a1yuHKn/txmO8Qw8LH//sIrF34AoReYDYdXUN+imPiD1W66viVqBzd+KT76yEJpBaG44xdff5ZHij2XVtgqvf0v9O7tmqZTOjtLAw9uf63ud28ttq7b7HI5R1Ue+WxMKQHQh84GwaS6zYX1Aenrp/S/MPuf36er1dq3mUuHS3Lf/SOCr8ZbWVmhRqGs7c8/HpQWf3v4uK7d+E3i4/i3fqzgkf31/f6PAw+JtmzmmJnrHWMbmnh7YgwYQMch8IGyay2zo6a5LG6+Iu3JLhazaWmFS857ttb/9LA2bx97Ya1ZbdUTQ6qm+MhmejbeU62wUK4DQhQRXv/O5PLul3DzX+rguH/o9Wbv9iyYDBP1bXvbo5oCPd8LQTBnWu0uTzesmnfc9s8YQgOhE8IGITP1bBvVK8/q7+ju6Sq63mo9wrQmjK/q6Bkp6jJrZCefqqc0VhrrOTtKshrdg0DOA0H008Lh+TI48trEs5Me8bvt+uaPgLBPgvPDuF87tF5/VQ75/Znc5v08XAg8gyjHsgrDxZ80NX0MvylpczJ81YTT9v+zNveY8kCESf4dQ9HrNeHg7xuVvhf4D2h+BrBbs70qxFr2t1gg8rICndN8hE+C4evOjavnh4J4EHkAMIPOBsGpuzQ0rQPHWvl0DjTsvPUsWbNjj1q7d8zZ++dwOt2/QmrL//ZXnNjuNd/mmMln27Sq8zQ2h6LCGrxGIP71RJteO+m6Nm0gZ1rK4Pg9mpdiE8Bbt+qr5YKVjIHYQfCDiG5NZAYqv9u0/OTfTZ/CimQ7XwEPp5R8OyvDZ50KLLr0t595UH4ymFtjToCkcQ0HejkkvVx85boIrPR7P7IieJYQ58NC/q7eaD1Y6BmIHwQeign5Qzhs/QK4d3adRoOEZvLgWWG7d9y+vt7fsjTKv36w13e9rjZGmvnk3laFR739RI3l9u4mdrGOy1kKxMhozn93hzORkde3Q6LGGI/bQ43l4ylAzU8l6flnpGIhdBB+IqSyJ51DKLRf39brflrJ/NZop488Uz5fe3++zm6aVoXmk+BNZufW7Kapqwfo98pMhmQFlP0Kx7op1TNp4TZt1uXaD1aDk8Z+f1+h5SPw2SPF8GvTpGXdOhmzYVek1wHI145K+suQfe33uZw3vuGaxfjQkM6AhOQDRi+ADMcNbgeWjXrpnKt3lhjE58sSb+9zqRbK6NP0Bt3JLuZlyOn+S9/oP/YD80ZCejYKPQOsV/G1f7hmgeAtY9LxLSuOaFD2mXzxd2igA0+dBWRkTrUr/xZgcZ92KtYLxsjc/9ZklGX1mD/nZyN7mMXdISpSX369sNGTmT2DRGmsFAQg/gg/EDG8Flr4KJ/UDffygnuYD1fUD8IGXP2z2fvQmNTDokNTGNDrz/HD0ZwqxJ9egQfmz7opnoejoft1l08fVXgtktTGYZ4bD8/nR39EVZq3+Gb6CA9chsEde+9j0XXFlPVbXwEFv09eQGYD4Q/CBqOTtG35TRZ+edB9dN8RzhovObvFHg0fthGfvDM8eFdo4y/OD1noMO7+oNT1LrCzH9NE5zc708FYo+ubH1W7H51lA6za08m0hrOdjOnbCfaujiQoQE6xdPlhqjp2UV3ZWNvlYrf0JNgBY70FAVNFv/N5agHv2DdEP8qbaVnj2vWhqumxTt6FFptpC3LodPV/jMcNmzfZvWsJ7ewxFr3wTeFi398Smsmb7l3jL8njyVUCrz8uyacMa9fRwvQ9fz7EnfUxaA+LZJCySW8sDCD+CD0SV5hpnaQZi09xL5NnrR5rl1rU2wzUY8eS6kqtmToJpz665Ai3mtD6km1tt1/MxNLo9h5juob6ar/nbEExf3IeOnWh0P3q5Q1Jbnw3eAmlO1pLVcQHEL4ZdEFX8aZzlmt53nTGhhY861OKrFkN/Z+64XJOJCIb1IX3vTwZ4vd6qr/Ana3Gw7ripv9BhENcaCc86jyaPR0TufvEDr7N69DZ16q/n2i7azVUDG38bfAVT3wIABB+IKsF82LkGI831jrhxTF/zSW0Wh/t2m/Uh78+QjN5ut5Tkxgveuay2609tyrodB+TFHQdM5sbqD+KtzsMfVgGq60wTz4JP7eb6+Lf1LmYarI+ApbleIr66zAKAqwSHI5yNlBurq6uTtLQ0qa2tldTU1HAfDiKQfvv3/LALZOVY/RBvboqn7qP1EjoqcV7vLvKX9/Y7i0JNW/dxZ8n30trLbau2NwqEdNjHFHl+23BMhz+KPKbm6mPwtcy8K+v29Dg1K6E1GMH47WXnyJmnd2r0mF0zKZ6solR/nmN/nlMAsa0ugM9vMh+IOi1tPuXPrItv+nV8N9RhBR6aEdD1ZEyGRESOnvi312/9zR2jXp+SfJqpFfGnaLRrx6Nmuqy/s3k8aRDl2WG1udqT30w4R/r2aByweMNMFgCBIPhAVLLrw87bUIcuZKfryTQXZDR3jN7WL/GkwY6VXdF9dQqvzibRoCQQndsnNdr2zr5/NXnfXTok2d4SHkB8YLYL0AR/ZnNogKEf0sFkYFxnnPgqIHWddaKBhxai3jb2TL/vx7XexKLZnNtX7fD9O98ONwFAayD4AJpgFYe6CuVsDmtq8KNXD210nbeOpBr46AyYq0ZkNzvbxXqBa9Gq58J7TQ236OOdP9H9dwAglBh2AZpgx2wOX2uvmOXtv13kzVvrcg0qfK2iq+4eP0B+OLhno2P1NdX3/gBqPACgJQg+gGbYsbqqrynEOqtGV8T1Fvj4WrHW+l1vgUdT9/WDAekEHQBsQfABRECBq68MiwYYPxmS6TPw0cvjB7eXI/XeZ90Ecl9kOwDYhT4fQARpSb+MQH+X3hwAQok+H0AcZlgC/V16cwAIF2a7AAAAWxF8AACA2Ag+Hn30UenTp4+0a9dORowYIVu3bm2tuwIAAPEefKxevVrmzJkj9957r7z77rsyZMgQKSgokIMHD7bG3QEAgHgPPh566CG5/vrr5dprr5Wzzz5bli5dKh06dJAnn3yyNe4OAADEc/Bx4sQJKS0tlfz8/O/uJDHRXC4pKWm0f319vZme43oCAACxK+TBR3V1tZw6dUrS09PdtuvlysrKRvsXFRVJWlqa85SVlRXqQwIAABEk7LNdCgsLpba21nmqqKgI9yEBAIBoaq/evXt3adOmjVRVVblt18sZGRmN9k9OTjYnAAAQH0Ke+UhKSpJhw4ZJcXGxc1tDQ4O5nJeXF+q7AwAAUaZVFpbTabbTpk2T888/Xy644AJZtGiRHD161Mx+AQAA8a1Vgo/JkyfLl19+Kffcc48pMj333HNlw4YNjYpQvXE4vlnnm1kvAABED+tz2/ocj6pVbT///HNmvAAAEKV04kivXr2iK/jQ+pD9+/dLp06dJCEhQaIh0tPpwfpkp6amhvtwYhrPNc91LOL/muc5Vmg4cfjwYcnMzDT9vWwfdmkJPeDmIqZIpIEHwQfPdazh/5rnOtbwP926tF9XVPT5AAAA8YXgAwAA2Irgo4W0QZqu3kujtNbHc20fnmue61jD/3RkibiCUwAAENvIfAAAAFsRfAAAAFsRfAAAAFsRfAAAAFsRfLSS+vp6s6aNdmndsWNHa91NXNq3b59Mnz5dcnJypH379tK3b18z4+jEiRPhPrSY8Oijj0qfPn2kXbt2MmLECNm6dWu4DynmFBUVyfDhw00n59NPP10mTJgge/bsCfdhxYX58+eb9+VZs2aF+1DiGsFHK7nzzjtNi1mE3u7du00b/scee0w++OADWbhwoSxdulTmzZvH091Cq1evNqtSazD37rvvypAhQ6SgoEAOHjzIcxtCGzdulBkzZsjbb78tr776qpw8eVL+8z//06z+jdazbds2874xePBgnuZw06m2CK1XXnnFkZub6/jggw90GrNj+/btPMWtbMGCBY6cnBye5xa64IILHDNmzHBePnXqlCMzM9NRVFTEc9uKDh48aN4rNm7cyPPcSg4fPuzo16+f49VXX3VcdNFFjttvv53nOozIfIRYVVWVXH/99fK///u/0qFDh1DfPHyora2Vrl278vy0gA5blZaWSn5+vttaS3q5pKSE57aV/38V/8OtRzNN48ePd/v/RvhE3MJy0Uz7tV1zzTVy0003yfnnn29qE9D6PvnkE3nkkUfkf/7nf3i6W6C6ulpOnTol6enpbtv1sg51oXXoEKLWH4waNUoGDhzI09wKVq1aZYYRddgFkYHMhx/mzp1rCpSaOumbs34A6nLChYWFrf+Xi+Pn2dUXX3whl156qVxxxRUm4wRE4zfyXbt2mQ9IhF5FRYXcfvvt8swzz5giakQG2qv74csvv5SvvvqqyX3OOOMMufLKK+Wll14yH5IW/SbZpk0bmTp1qjz11FMt/4vFMH+f56SkJPPz/v375eKLL5aRI0fKihUrzBABWjbsokOF//d//2dmX1imTZsmNTU18uKLL/L0htitt95qntc33njDzN5C6K1bt04uv/xy8z7s+r6s79P6nqEzE12vgz0IPkKovLxc6urqnJf1w1FnCuibuU5Z7NWrVyjvLq5pxuOSSy6RYcOGyZ///GfePEJE/08vuOACk8WzhgSys7PNh6RmphC6IdqZM2fK2rVr5fXXX5d+/frx1LYSzUZ/9tlnbtuuvfZayc3NlbvuuouhrjCh5iOE9E3aVceOHc259qEg8Aht4KEZj969e5s6D82YWDIyMkJ4T/FHp9lqpkNrljQIWbRokZn+qW/WCO1Qy8qVK03WQ3t9VFZWmu1paWmmdw1CR59fz1qalJQU6datG4FHGBF8IOpoXwQtMtWTZ1DHIs0tM3nyZBPM3XPPPeYDURvlbdiwoVERKlpmyZIl5lyDaFfLly83RetArGPYBQAA2IoKPQAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAYCuCDwAAIHb6fxi53TRzb2nxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 200\n",
    "x = torch.randn(m, 1) * 2\n",
    "y = 2 * x**2 + 3*x + 5 + torch.randn(m, 1)\n",
    "\n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80db0c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1]), torch.Size([20, 1]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(x, y))\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size= 20 , shuffle=True)\n",
    "\n",
    "for xi, yi in loader:\n",
    "    break\n",
    "xi.shape, yi.shape, xi.dtype, yi.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00049903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.cat([x**2, x, x**0], axis = 1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25d34d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0022],\n",
      "        [2.9422],\n",
      "        [5.0464]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.linalg.pinv(X) @ y\n",
    "\n",
    "print(w)\n",
    "\n",
    "# tensor([[2.0xxx],   ← a ≈ 2\n",
    "#         [3.0xxx],   ← b ≈ 3\n",
    "#         [5.0xxx]])  ← c ≈ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb220252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망\n",
    "\n",
    "W1 = torch.randn((1,32), requires_grad=True) #입력->은닉1\n",
    "B1 = torch.randn((32, ), requires_grad=True)\n",
    "W2 = torch.randn((32,32), requires_grad=True) #은닉1->출력\n",
    "B2 = torch.randn((32, ), requires_grad=True)\n",
    "\n",
    "\n",
    "# (batch,1)  @ (1,32)  = (batch,32)\n",
    "# (batch,32) @ (32,1)  = (batch,1)\n",
    "\n",
    "def model(x):\n",
    "    h = torch.relu(x @ W1 + B1)\n",
    "    y = (h @ W2 + B2)\n",
    "    return y \n",
    "\n",
    "# optimize(최적화)\n",
    "opt = torch.optim.Adam([W1, B1, W2, B2], lr = 0.0001)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e80034fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(33.3628, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(15.9763, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(7.5397, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(9.6812, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(7.7877, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(30.0626, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(6.4394, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(6.2912, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(11.8170, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(29.1360, grad_fn=<MseLossBackward0>)\n",
      "10 tensor(16.1405, grad_fn=<MseLossBackward0>)\n",
      "11 tensor(27.6584, grad_fn=<MseLossBackward0>)\n",
      "12 tensor(9.7710, grad_fn=<MseLossBackward0>)\n",
      "13 tensor(6.7041, grad_fn=<MseLossBackward0>)\n",
      "14 tensor(23.2157, grad_fn=<MseLossBackward0>)\n",
      "15 tensor(12.2015, grad_fn=<MseLossBackward0>)\n",
      "16 tensor(31.2145, grad_fn=<MseLossBackward0>)\n",
      "17 tensor(8.5743, grad_fn=<MseLossBackward0>)\n",
      "18 tensor(5.6799, grad_fn=<MseLossBackward0>)\n",
      "19 tensor(12.5521, grad_fn=<MseLossBackward0>)\n",
      "20 tensor(55.5667, grad_fn=<MseLossBackward0>)\n",
      "21 tensor(8.4123, grad_fn=<MseLossBackward0>)\n",
      "22 tensor(7.7920, grad_fn=<MseLossBackward0>)\n",
      "23 tensor(25.2815, grad_fn=<MseLossBackward0>)\n",
      "24 tensor(13.2463, grad_fn=<MseLossBackward0>)\n",
      "25 tensor(34.8120, grad_fn=<MseLossBackward0>)\n",
      "26 tensor(23.2257, grad_fn=<MseLossBackward0>)\n",
      "27 tensor(24.3402, grad_fn=<MseLossBackward0>)\n",
      "28 tensor(14.7989, grad_fn=<MseLossBackward0>)\n",
      "29 tensor(21.3921, grad_fn=<MseLossBackward0>)\n",
      "30 tensor(8.7116, grad_fn=<MseLossBackward0>)\n",
      "31 tensor(9.4629, grad_fn=<MseLossBackward0>)\n",
      "32 tensor(19.6745, grad_fn=<MseLossBackward0>)\n",
      "33 tensor(29.4598, grad_fn=<MseLossBackward0>)\n",
      "34 tensor(48.2083, grad_fn=<MseLossBackward0>)\n",
      "35 tensor(7.0228, grad_fn=<MseLossBackward0>)\n",
      "36 tensor(42.7672, grad_fn=<MseLossBackward0>)\n",
      "37 tensor(13.1337, grad_fn=<MseLossBackward0>)\n",
      "38 tensor(11.4681, grad_fn=<MseLossBackward0>)\n",
      "39 tensor(8.4101, grad_fn=<MseLossBackward0>)\n",
      "40 tensor(12.9398, grad_fn=<MseLossBackward0>)\n",
      "41 tensor(23.2675, grad_fn=<MseLossBackward0>)\n",
      "42 tensor(10.4338, grad_fn=<MseLossBackward0>)\n",
      "43 tensor(7.2948, grad_fn=<MseLossBackward0>)\n",
      "44 tensor(22.2851, grad_fn=<MseLossBackward0>)\n",
      "45 tensor(9.6276, grad_fn=<MseLossBackward0>)\n",
      "46 tensor(36.2722, grad_fn=<MseLossBackward0>)\n",
      "47 tensor(32.9655, grad_fn=<MseLossBackward0>)\n",
      "48 tensor(5.9821, grad_fn=<MseLossBackward0>)\n",
      "49 tensor(12.3340, grad_fn=<MseLossBackward0>)\n",
      "50 tensor(8.7587, grad_fn=<MseLossBackward0>)\n",
      "51 tensor(9.2970, grad_fn=<MseLossBackward0>)\n",
      "52 tensor(50.1112, grad_fn=<MseLossBackward0>)\n",
      "53 tensor(33.8799, grad_fn=<MseLossBackward0>)\n",
      "54 tensor(7.9031, grad_fn=<MseLossBackward0>)\n",
      "55 tensor(22.0009, grad_fn=<MseLossBackward0>)\n",
      "56 tensor(9.5534, grad_fn=<MseLossBackward0>)\n",
      "57 tensor(6.3567, grad_fn=<MseLossBackward0>)\n",
      "58 tensor(24.6826, grad_fn=<MseLossBackward0>)\n",
      "59 tensor(11.9119, grad_fn=<MseLossBackward0>)\n",
      "60 tensor(6.8964, grad_fn=<MseLossBackward0>)\n",
      "61 tensor(33.9894, grad_fn=<MseLossBackward0>)\n",
      "62 tensor(7.8313, grad_fn=<MseLossBackward0>)\n",
      "63 tensor(6.8178, grad_fn=<MseLossBackward0>)\n",
      "64 tensor(11.4864, grad_fn=<MseLossBackward0>)\n",
      "65 tensor(9.4097, grad_fn=<MseLossBackward0>)\n",
      "66 tensor(16.1289, grad_fn=<MseLossBackward0>)\n",
      "67 tensor(8.2695, grad_fn=<MseLossBackward0>)\n",
      "68 tensor(10.4556, grad_fn=<MseLossBackward0>)\n",
      "69 tensor(11.6236, grad_fn=<MseLossBackward0>)\n",
      "70 tensor(24.2311, grad_fn=<MseLossBackward0>)\n",
      "71 tensor(42.6694, grad_fn=<MseLossBackward0>)\n",
      "72 tensor(7.2595, grad_fn=<MseLossBackward0>)\n",
      "73 tensor(12.4376, grad_fn=<MseLossBackward0>)\n",
      "74 tensor(6.7174, grad_fn=<MseLossBackward0>)\n",
      "75 tensor(26.6122, grad_fn=<MseLossBackward0>)\n",
      "76 tensor(10.5705, grad_fn=<MseLossBackward0>)\n",
      "77 tensor(21.2880, grad_fn=<MseLossBackward0>)\n",
      "78 tensor(7.6445, grad_fn=<MseLossBackward0>)\n",
      "79 tensor(9.9157, grad_fn=<MseLossBackward0>)\n",
      "80 tensor(7.5582, grad_fn=<MseLossBackward0>)\n",
      "81 tensor(23.3944, grad_fn=<MseLossBackward0>)\n",
      "82 tensor(11.0396, grad_fn=<MseLossBackward0>)\n",
      "83 tensor(22.7187, grad_fn=<MseLossBackward0>)\n",
      "84 tensor(15.3128, grad_fn=<MseLossBackward0>)\n",
      "85 tensor(8.5726, grad_fn=<MseLossBackward0>)\n",
      "86 tensor(7.1904, grad_fn=<MseLossBackward0>)\n",
      "87 tensor(10.4158, grad_fn=<MseLossBackward0>)\n",
      "88 tensor(8.7954, grad_fn=<MseLossBackward0>)\n",
      "89 tensor(30.1565, grad_fn=<MseLossBackward0>)\n",
      "90 tensor(10.2315, grad_fn=<MseLossBackward0>)\n",
      "91 tensor(29.5938, grad_fn=<MseLossBackward0>)\n",
      "92 tensor(10.6518, grad_fn=<MseLossBackward0>)\n",
      "93 tensor(9.3485, grad_fn=<MseLossBackward0>)\n",
      "94 tensor(33.3513, grad_fn=<MseLossBackward0>)\n",
      "95 tensor(6.7836, grad_fn=<MseLossBackward0>)\n",
      "96 tensor(27.3080, grad_fn=<MseLossBackward0>)\n",
      "97 tensor(5.1658, grad_fn=<MseLossBackward0>)\n",
      "98 tensor(8.2050, grad_fn=<MseLossBackward0>)\n",
      "99 tensor(11.4859, grad_fn=<MseLossBackward0>)\n",
      "100 tensor(11.4917, grad_fn=<MseLossBackward0>)\n",
      "101 tensor(5.8705, grad_fn=<MseLossBackward0>)\n",
      "102 tensor(6.8113, grad_fn=<MseLossBackward0>)\n",
      "103 tensor(22.0039, grad_fn=<MseLossBackward0>)\n",
      "104 tensor(20.6638, grad_fn=<MseLossBackward0>)\n",
      "105 tensor(10.5933, grad_fn=<MseLossBackward0>)\n",
      "106 tensor(12.9615, grad_fn=<MseLossBackward0>)\n",
      "107 tensor(11.3481, grad_fn=<MseLossBackward0>)\n",
      "108 tensor(12.5530, grad_fn=<MseLossBackward0>)\n",
      "109 tensor(26.3653, grad_fn=<MseLossBackward0>)\n",
      "110 tensor(11.7351, grad_fn=<MseLossBackward0>)\n",
      "111 tensor(22.1737, grad_fn=<MseLossBackward0>)\n",
      "112 tensor(23.2484, grad_fn=<MseLossBackward0>)\n",
      "113 tensor(12.7673, grad_fn=<MseLossBackward0>)\n",
      "114 tensor(11.2845, grad_fn=<MseLossBackward0>)\n",
      "115 tensor(25.5269, grad_fn=<MseLossBackward0>)\n",
      "116 tensor(7.9533, grad_fn=<MseLossBackward0>)\n",
      "117 tensor(7.4762, grad_fn=<MseLossBackward0>)\n",
      "118 tensor(46.9864, grad_fn=<MseLossBackward0>)\n",
      "119 tensor(11.4845, grad_fn=<MseLossBackward0>)\n",
      "120 tensor(10.8301, grad_fn=<MseLossBackward0>)\n",
      "121 tensor(10.0117, grad_fn=<MseLossBackward0>)\n",
      "122 tensor(33.8657, grad_fn=<MseLossBackward0>)\n",
      "123 tensor(21.9077, grad_fn=<MseLossBackward0>)\n",
      "124 tensor(8.9482, grad_fn=<MseLossBackward0>)\n",
      "125 tensor(8.7076, grad_fn=<MseLossBackward0>)\n",
      "126 tensor(28.5235, grad_fn=<MseLossBackward0>)\n",
      "127 tensor(6.2042, grad_fn=<MseLossBackward0>)\n",
      "128 tensor(26.2349, grad_fn=<MseLossBackward0>)\n",
      "129 tensor(12.9183, grad_fn=<MseLossBackward0>)\n",
      "130 tensor(9.0472, grad_fn=<MseLossBackward0>)\n",
      "131 tensor(8.9633, grad_fn=<MseLossBackward0>)\n",
      "132 tensor(12.5155, grad_fn=<MseLossBackward0>)\n",
      "133 tensor(29.8331, grad_fn=<MseLossBackward0>)\n",
      "134 tensor(25.1699, grad_fn=<MseLossBackward0>)\n",
      "135 tensor(10.3111, grad_fn=<MseLossBackward0>)\n",
      "136 tensor(10.4743, grad_fn=<MseLossBackward0>)\n",
      "137 tensor(11.7442, grad_fn=<MseLossBackward0>)\n",
      "138 tensor(9.1735, grad_fn=<MseLossBackward0>)\n",
      "139 tensor(8.1398, grad_fn=<MseLossBackward0>)\n",
      "140 tensor(7.2088, grad_fn=<MseLossBackward0>)\n",
      "141 tensor(20.6619, grad_fn=<MseLossBackward0>)\n",
      "142 tensor(21.9448, grad_fn=<MseLossBackward0>)\n",
      "143 tensor(7.3937, grad_fn=<MseLossBackward0>)\n",
      "144 tensor(7.2845, grad_fn=<MseLossBackward0>)\n",
      "145 tensor(27.6051, grad_fn=<MseLossBackward0>)\n",
      "146 tensor(40.7989, grad_fn=<MseLossBackward0>)\n",
      "147 tensor(27.1234, grad_fn=<MseLossBackward0>)\n",
      "148 tensor(10.7579, grad_fn=<MseLossBackward0>)\n",
      "149 tensor(9.5706, grad_fn=<MseLossBackward0>)\n",
      "150 tensor(11.2874, grad_fn=<MseLossBackward0>)\n",
      "151 tensor(9.9162, grad_fn=<MseLossBackward0>)\n",
      "152 tensor(21.1277, grad_fn=<MseLossBackward0>)\n",
      "153 tensor(6.7295, grad_fn=<MseLossBackward0>)\n",
      "154 tensor(11.6647, grad_fn=<MseLossBackward0>)\n",
      "155 tensor(22.0418, grad_fn=<MseLossBackward0>)\n",
      "156 tensor(23.3185, grad_fn=<MseLossBackward0>)\n",
      "157 tensor(9.5078, grad_fn=<MseLossBackward0>)\n",
      "158 tensor(12.3323, grad_fn=<MseLossBackward0>)\n",
      "159 tensor(7.3412, grad_fn=<MseLossBackward0>)\n",
      "160 tensor(10.8502, grad_fn=<MseLossBackward0>)\n",
      "161 tensor(9.1106, grad_fn=<MseLossBackward0>)\n",
      "162 tensor(20.8054, grad_fn=<MseLossBackward0>)\n",
      "163 tensor(7.5775, grad_fn=<MseLossBackward0>)\n",
      "164 tensor(24.1005, grad_fn=<MseLossBackward0>)\n",
      "165 tensor(5.8001, grad_fn=<MseLossBackward0>)\n",
      "166 tensor(9.7783, grad_fn=<MseLossBackward0>)\n",
      "167 tensor(39.7997, grad_fn=<MseLossBackward0>)\n",
      "168 tensor(19.2021, grad_fn=<MseLossBackward0>)\n",
      "169 tensor(12.2982, grad_fn=<MseLossBackward0>)\n",
      "170 tensor(25.4051, grad_fn=<MseLossBackward0>)\n",
      "171 tensor(16.1805, grad_fn=<MseLossBackward0>)\n",
      "172 tensor(15.9785, grad_fn=<MseLossBackward0>)\n",
      "173 tensor(29.5591, grad_fn=<MseLossBackward0>)\n",
      "174 tensor(11.3345, grad_fn=<MseLossBackward0>)\n",
      "175 tensor(6.9802, grad_fn=<MseLossBackward0>)\n",
      "176 tensor(27.5132, grad_fn=<MseLossBackward0>)\n",
      "177 tensor(24.2205, grad_fn=<MseLossBackward0>)\n",
      "178 tensor(9.4593, grad_fn=<MseLossBackward0>)\n",
      "179 tensor(10.7800, grad_fn=<MseLossBackward0>)\n",
      "180 tensor(8.4962, grad_fn=<MseLossBackward0>)\n",
      "181 tensor(9.6403, grad_fn=<MseLossBackward0>)\n",
      "182 tensor(22.1314, grad_fn=<MseLossBackward0>)\n",
      "183 tensor(24.5180, grad_fn=<MseLossBackward0>)\n",
      "184 tensor(7.6389, grad_fn=<MseLossBackward0>)\n",
      "185 tensor(11.1072, grad_fn=<MseLossBackward0>)\n",
      "186 tensor(8.5661, grad_fn=<MseLossBackward0>)\n",
      "187 tensor(29.6348, grad_fn=<MseLossBackward0>)\n",
      "188 tensor(22.5419, grad_fn=<MseLossBackward0>)\n",
      "189 tensor(9.2879, grad_fn=<MseLossBackward0>)\n",
      "190 tensor(9.2208, grad_fn=<MseLossBackward0>)\n",
      "191 tensor(9.1978, grad_fn=<MseLossBackward0>)\n",
      "192 tensor(9.9328, grad_fn=<MseLossBackward0>)\n",
      "193 tensor(11.3458, grad_fn=<MseLossBackward0>)\n",
      "194 tensor(23.1628, grad_fn=<MseLossBackward0>)\n",
      "195 tensor(22.5056, grad_fn=<MseLossBackward0>)\n",
      "196 tensor(7.1014, grad_fn=<MseLossBackward0>)\n",
      "197 tensor(25.5600, grad_fn=<MseLossBackward0>)\n",
      "198 tensor(13.3651, grad_fn=<MseLossBackward0>)\n",
      "199 tensor(22.3786, grad_fn=<MseLossBackward0>)\n",
      "200 tensor(21.8976, grad_fn=<MseLossBackward0>)\n",
      "201 tensor(17.2105, grad_fn=<MseLossBackward0>)\n",
      "202 tensor(19.6642, grad_fn=<MseLossBackward0>)\n",
      "203 tensor(9.4544, grad_fn=<MseLossBackward0>)\n",
      "204 tensor(10.0483, grad_fn=<MseLossBackward0>)\n",
      "205 tensor(8.7457, grad_fn=<MseLossBackward0>)\n",
      "206 tensor(25.8474, grad_fn=<MseLossBackward0>)\n",
      "207 tensor(6.0715, grad_fn=<MseLossBackward0>)\n",
      "208 tensor(9.0343, grad_fn=<MseLossBackward0>)\n",
      "209 tensor(6.8935, grad_fn=<MseLossBackward0>)\n",
      "210 tensor(11.5142, grad_fn=<MseLossBackward0>)\n",
      "211 tensor(6.9979, grad_fn=<MseLossBackward0>)\n",
      "212 tensor(23.4801, grad_fn=<MseLossBackward0>)\n",
      "213 tensor(22.6400, grad_fn=<MseLossBackward0>)\n",
      "214 tensor(12.5903, grad_fn=<MseLossBackward0>)\n",
      "215 tensor(7.1974, grad_fn=<MseLossBackward0>)\n",
      "216 tensor(11.7588, grad_fn=<MseLossBackward0>)\n",
      "217 tensor(28.1430, grad_fn=<MseLossBackward0>)\n",
      "218 tensor(21.8814, grad_fn=<MseLossBackward0>)\n",
      "219 tensor(10.8968, grad_fn=<MseLossBackward0>)\n",
      "220 tensor(7.6642, grad_fn=<MseLossBackward0>)\n",
      "221 tensor(9.6071, grad_fn=<MseLossBackward0>)\n",
      "222 tensor(12.7698, grad_fn=<MseLossBackward0>)\n",
      "223 tensor(17.8337, grad_fn=<MseLossBackward0>)\n",
      "224 tensor(7.7426, grad_fn=<MseLossBackward0>)\n",
      "225 tensor(10.0307, grad_fn=<MseLossBackward0>)\n",
      "226 tensor(38.6812, grad_fn=<MseLossBackward0>)\n",
      "227 tensor(8.0378, grad_fn=<MseLossBackward0>)\n",
      "228 tensor(11.2585, grad_fn=<MseLossBackward0>)\n",
      "229 tensor(7.0114, grad_fn=<MseLossBackward0>)\n",
      "230 tensor(7.2140, grad_fn=<MseLossBackward0>)\n",
      "231 tensor(10.5048, grad_fn=<MseLossBackward0>)\n",
      "232 tensor(8.7614, grad_fn=<MseLossBackward0>)\n",
      "233 tensor(24.5920, grad_fn=<MseLossBackward0>)\n",
      "234 tensor(27.1478, grad_fn=<MseLossBackward0>)\n",
      "235 tensor(8.3442, grad_fn=<MseLossBackward0>)\n",
      "236 tensor(36.5040, grad_fn=<MseLossBackward0>)\n",
      "237 tensor(9.4964, grad_fn=<MseLossBackward0>)\n",
      "238 tensor(8.1942, grad_fn=<MseLossBackward0>)\n",
      "239 tensor(5.7425, grad_fn=<MseLossBackward0>)\n",
      "240 tensor(7.6597, grad_fn=<MseLossBackward0>)\n",
      "241 tensor(7.6783, grad_fn=<MseLossBackward0>)\n",
      "242 tensor(23.3331, grad_fn=<MseLossBackward0>)\n",
      "243 tensor(9.5295, grad_fn=<MseLossBackward0>)\n",
      "244 tensor(24.0345, grad_fn=<MseLossBackward0>)\n",
      "245 tensor(9.9367, grad_fn=<MseLossBackward0>)\n",
      "246 tensor(37.3060, grad_fn=<MseLossBackward0>)\n",
      "247 tensor(11.1548, grad_fn=<MseLossBackward0>)\n",
      "248 tensor(6.9608, grad_fn=<MseLossBackward0>)\n",
      "249 tensor(10.3212, grad_fn=<MseLossBackward0>)\n",
      "250 tensor(7.0012, grad_fn=<MseLossBackward0>)\n",
      "251 tensor(18.7416, grad_fn=<MseLossBackward0>)\n",
      "252 tensor(7.3879, grad_fn=<MseLossBackward0>)\n",
      "253 tensor(8.3008, grad_fn=<MseLossBackward0>)\n",
      "254 tensor(25.4826, grad_fn=<MseLossBackward0>)\n",
      "255 tensor(8.0179, grad_fn=<MseLossBackward0>)\n",
      "256 tensor(7.2786, grad_fn=<MseLossBackward0>)\n",
      "257 tensor(5.4457, grad_fn=<MseLossBackward0>)\n",
      "258 tensor(24.7014, grad_fn=<MseLossBackward0>)\n",
      "259 tensor(9.3296, grad_fn=<MseLossBackward0>)\n",
      "260 tensor(9.5409, grad_fn=<MseLossBackward0>)\n",
      "261 tensor(22.0980, grad_fn=<MseLossBackward0>)\n",
      "262 tensor(20.4916, grad_fn=<MseLossBackward0>)\n",
      "263 tensor(8.4968, grad_fn=<MseLossBackward0>)\n",
      "264 tensor(19.1936, grad_fn=<MseLossBackward0>)\n",
      "265 tensor(23.0365, grad_fn=<MseLossBackward0>)\n",
      "266 tensor(14.7935, grad_fn=<MseLossBackward0>)\n",
      "267 tensor(9.8866, grad_fn=<MseLossBackward0>)\n",
      "268 tensor(39.1095, grad_fn=<MseLossBackward0>)\n",
      "269 tensor(19.7973, grad_fn=<MseLossBackward0>)\n",
      "270 tensor(7.2327, grad_fn=<MseLossBackward0>)\n",
      "271 tensor(5.8004, grad_fn=<MseLossBackward0>)\n",
      "272 tensor(32.6406, grad_fn=<MseLossBackward0>)\n",
      "273 tensor(21.3132, grad_fn=<MseLossBackward0>)\n",
      "274 tensor(12.3899, grad_fn=<MseLossBackward0>)\n",
      "275 tensor(5.0997, grad_fn=<MseLossBackward0>)\n",
      "276 tensor(8.4734, grad_fn=<MseLossBackward0>)\n",
      "277 tensor(12.5495, grad_fn=<MseLossBackward0>)\n",
      "278 tensor(25.2989, grad_fn=<MseLossBackward0>)\n",
      "279 tensor(19.3845, grad_fn=<MseLossBackward0>)\n",
      "280 tensor(26.4396, grad_fn=<MseLossBackward0>)\n",
      "281 tensor(9.7755, grad_fn=<MseLossBackward0>)\n",
      "282 tensor(9.9601, grad_fn=<MseLossBackward0>)\n",
      "283 tensor(22.4507, grad_fn=<MseLossBackward0>)\n",
      "284 tensor(7.0145, grad_fn=<MseLossBackward0>)\n",
      "285 tensor(27.1923, grad_fn=<MseLossBackward0>)\n",
      "286 tensor(15.2166, grad_fn=<MseLossBackward0>)\n",
      "287 tensor(8.3586, grad_fn=<MseLossBackward0>)\n",
      "288 tensor(8.8897, grad_fn=<MseLossBackward0>)\n",
      "289 tensor(8.5772, grad_fn=<MseLossBackward0>)\n",
      "290 tensor(21.4914, grad_fn=<MseLossBackward0>)\n",
      "291 tensor(7.2583, grad_fn=<MseLossBackward0>)\n",
      "292 tensor(10.4203, grad_fn=<MseLossBackward0>)\n",
      "293 tensor(10.8943, grad_fn=<MseLossBackward0>)\n",
      "294 tensor(23.8726, grad_fn=<MseLossBackward0>)\n",
      "295 tensor(8.0180, grad_fn=<MseLossBackward0>)\n",
      "296 tensor(17.7031, grad_fn=<MseLossBackward0>)\n",
      "297 tensor(7.7471, grad_fn=<MseLossBackward0>)\n",
      "298 tensor(12.1841, grad_fn=<MseLossBackward0>)\n",
      "299 tensor(8.0041, grad_fn=<MseLossBackward0>)\n",
      "300 tensor(9.0001, grad_fn=<MseLossBackward0>)\n",
      "301 tensor(14.5808, grad_fn=<MseLossBackward0>)\n",
      "302 tensor(26.0005, grad_fn=<MseLossBackward0>)\n",
      "303 tensor(22.1828, grad_fn=<MseLossBackward0>)\n",
      "304 tensor(8.2492, grad_fn=<MseLossBackward0>)\n",
      "305 tensor(10.0528, grad_fn=<MseLossBackward0>)\n",
      "306 tensor(8.9562, grad_fn=<MseLossBackward0>)\n",
      "307 tensor(8.5158, grad_fn=<MseLossBackward0>)\n",
      "308 tensor(6.1730, grad_fn=<MseLossBackward0>)\n",
      "309 tensor(8.2241, grad_fn=<MseLossBackward0>)\n",
      "310 tensor(15.7859, grad_fn=<MseLossBackward0>)\n",
      "311 tensor(7.8480, grad_fn=<MseLossBackward0>)\n",
      "312 tensor(6.4738, grad_fn=<MseLossBackward0>)\n",
      "313 tensor(25.2158, grad_fn=<MseLossBackward0>)\n",
      "314 tensor(7.1638, grad_fn=<MseLossBackward0>)\n",
      "315 tensor(24.7861, grad_fn=<MseLossBackward0>)\n",
      "316 tensor(6.6348, grad_fn=<MseLossBackward0>)\n",
      "317 tensor(8.1665, grad_fn=<MseLossBackward0>)\n",
      "318 tensor(7.0668, grad_fn=<MseLossBackward0>)\n",
      "319 tensor(10.6009, grad_fn=<MseLossBackward0>)\n",
      "320 tensor(5.9339, grad_fn=<MseLossBackward0>)\n",
      "321 tensor(9.6114, grad_fn=<MseLossBackward0>)\n",
      "322 tensor(5.4427, grad_fn=<MseLossBackward0>)\n",
      "323 tensor(6.5937, grad_fn=<MseLossBackward0>)\n",
      "324 tensor(9.9976, grad_fn=<MseLossBackward0>)\n",
      "325 tensor(8.0026, grad_fn=<MseLossBackward0>)\n",
      "326 tensor(6.9281, grad_fn=<MseLossBackward0>)\n",
      "327 tensor(17.0024, grad_fn=<MseLossBackward0>)\n",
      "328 tensor(9.9311, grad_fn=<MseLossBackward0>)\n",
      "329 tensor(15.8704, grad_fn=<MseLossBackward0>)\n",
      "330 tensor(31.6877, grad_fn=<MseLossBackward0>)\n",
      "331 tensor(7.1505, grad_fn=<MseLossBackward0>)\n",
      "332 tensor(7.3505, grad_fn=<MseLossBackward0>)\n",
      "333 tensor(20.6148, grad_fn=<MseLossBackward0>)\n",
      "334 tensor(10.1444, grad_fn=<MseLossBackward0>)\n",
      "335 tensor(17.2729, grad_fn=<MseLossBackward0>)\n",
      "336 tensor(6.8581, grad_fn=<MseLossBackward0>)\n",
      "337 tensor(6.7505, grad_fn=<MseLossBackward0>)\n",
      "338 tensor(9.1558, grad_fn=<MseLossBackward0>)\n",
      "339 tensor(8.6661, grad_fn=<MseLossBackward0>)\n",
      "340 tensor(7.2338, grad_fn=<MseLossBackward0>)\n",
      "341 tensor(5.8659, grad_fn=<MseLossBackward0>)\n",
      "342 tensor(6.0973, grad_fn=<MseLossBackward0>)\n",
      "343 tensor(16.3506, grad_fn=<MseLossBackward0>)\n",
      "344 tensor(9.7667, grad_fn=<MseLossBackward0>)\n",
      "345 tensor(6.1116, grad_fn=<MseLossBackward0>)\n",
      "346 tensor(11.3522, grad_fn=<MseLossBackward0>)\n",
      "347 tensor(5.6570, grad_fn=<MseLossBackward0>)\n",
      "348 tensor(8.8526, grad_fn=<MseLossBackward0>)\n",
      "349 tensor(18.5365, grad_fn=<MseLossBackward0>)\n",
      "350 tensor(25.3328, grad_fn=<MseLossBackward0>)\n",
      "351 tensor(25.8201, grad_fn=<MseLossBackward0>)\n",
      "352 tensor(8.7061, grad_fn=<MseLossBackward0>)\n",
      "353 tensor(10.1639, grad_fn=<MseLossBackward0>)\n",
      "354 tensor(8.1471, grad_fn=<MseLossBackward0>)\n",
      "355 tensor(7.5136, grad_fn=<MseLossBackward0>)\n",
      "356 tensor(9.7738, grad_fn=<MseLossBackward0>)\n",
      "357 tensor(24.5208, grad_fn=<MseLossBackward0>)\n",
      "358 tensor(19.0816, grad_fn=<MseLossBackward0>)\n",
      "359 tensor(9.1747, grad_fn=<MseLossBackward0>)\n",
      "360 tensor(18.5968, grad_fn=<MseLossBackward0>)\n",
      "361 tensor(20.7549, grad_fn=<MseLossBackward0>)\n",
      "362 tensor(10.3551, grad_fn=<MseLossBackward0>)\n",
      "363 tensor(7.8218, grad_fn=<MseLossBackward0>)\n",
      "364 tensor(6.7382, grad_fn=<MseLossBackward0>)\n",
      "365 tensor(17.3621, grad_fn=<MseLossBackward0>)\n",
      "366 tensor(7.6643, grad_fn=<MseLossBackward0>)\n",
      "367 tensor(9.3509, grad_fn=<MseLossBackward0>)\n",
      "368 tensor(5.8063, grad_fn=<MseLossBackward0>)\n",
      "369 tensor(5.8285, grad_fn=<MseLossBackward0>)\n",
      "370 tensor(21.6768, grad_fn=<MseLossBackward0>)\n",
      "371 tensor(10.1295, grad_fn=<MseLossBackward0>)\n",
      "372 tensor(10.8341, grad_fn=<MseLossBackward0>)\n",
      "373 tensor(8.3127, grad_fn=<MseLossBackward0>)\n",
      "374 tensor(9.7819, grad_fn=<MseLossBackward0>)\n",
      "375 tensor(9.4501, grad_fn=<MseLossBackward0>)\n",
      "376 tensor(5.8805, grad_fn=<MseLossBackward0>)\n",
      "377 tensor(15.0006, grad_fn=<MseLossBackward0>)\n",
      "378 tensor(9.3054, grad_fn=<MseLossBackward0>)\n",
      "379 tensor(10.7664, grad_fn=<MseLossBackward0>)\n",
      "380 tensor(10.1906, grad_fn=<MseLossBackward0>)\n",
      "381 tensor(7.2151, grad_fn=<MseLossBackward0>)\n",
      "382 tensor(9.1489, grad_fn=<MseLossBackward0>)\n",
      "383 tensor(7.3663, grad_fn=<MseLossBackward0>)\n",
      "384 tensor(5.9554, grad_fn=<MseLossBackward0>)\n",
      "385 tensor(8.2743, grad_fn=<MseLossBackward0>)\n",
      "386 tensor(8.3638, grad_fn=<MseLossBackward0>)\n",
      "387 tensor(14.7260, grad_fn=<MseLossBackward0>)\n",
      "388 tensor(16.5098, grad_fn=<MseLossBackward0>)\n",
      "389 tensor(6.8424, grad_fn=<MseLossBackward0>)\n",
      "390 tensor(6.7755, grad_fn=<MseLossBackward0>)\n",
      "391 tensor(32.3358, grad_fn=<MseLossBackward0>)\n",
      "392 tensor(8.3655, grad_fn=<MseLossBackward0>)\n",
      "393 tensor(19.5872, grad_fn=<MseLossBackward0>)\n",
      "394 tensor(9.1887, grad_fn=<MseLossBackward0>)\n",
      "395 tensor(31.0834, grad_fn=<MseLossBackward0>)\n",
      "396 tensor(14.9125, grad_fn=<MseLossBackward0>)\n",
      "397 tensor(14.3048, grad_fn=<MseLossBackward0>)\n",
      "398 tensor(9.4803, grad_fn=<MseLossBackward0>)\n",
      "399 tensor(22.5409, grad_fn=<MseLossBackward0>)\n",
      "400 tensor(18.9859, grad_fn=<MseLossBackward0>)\n",
      "401 tensor(16.5929, grad_fn=<MseLossBackward0>)\n",
      "402 tensor(20.3815, grad_fn=<MseLossBackward0>)\n",
      "403 tensor(17.1156, grad_fn=<MseLossBackward0>)\n",
      "404 tensor(9.0540, grad_fn=<MseLossBackward0>)\n",
      "405 tensor(8.6037, grad_fn=<MseLossBackward0>)\n",
      "406 tensor(8.7585, grad_fn=<MseLossBackward0>)\n",
      "407 tensor(6.1232, grad_fn=<MseLossBackward0>)\n",
      "408 tensor(7.7548, grad_fn=<MseLossBackward0>)\n",
      "409 tensor(7.5215, grad_fn=<MseLossBackward0>)\n",
      "410 tensor(9.5253, grad_fn=<MseLossBackward0>)\n",
      "411 tensor(5.7665, grad_fn=<MseLossBackward0>)\n",
      "412 tensor(9.6305, grad_fn=<MseLossBackward0>)\n",
      "413 tensor(23.3276, grad_fn=<MseLossBackward0>)\n",
      "414 tensor(7.5307, grad_fn=<MseLossBackward0>)\n",
      "415 tensor(12.0392, grad_fn=<MseLossBackward0>)\n",
      "416 tensor(7.7747, grad_fn=<MseLossBackward0>)\n",
      "417 tensor(6.4208, grad_fn=<MseLossBackward0>)\n",
      "418 tensor(8.2966, grad_fn=<MseLossBackward0>)\n",
      "419 tensor(6.3361, grad_fn=<MseLossBackward0>)\n",
      "420 tensor(23.7866, grad_fn=<MseLossBackward0>)\n",
      "421 tensor(8.6592, grad_fn=<MseLossBackward0>)\n",
      "422 tensor(20.2727, grad_fn=<MseLossBackward0>)\n",
      "423 tensor(6.3068, grad_fn=<MseLossBackward0>)\n",
      "424 tensor(16.9293, grad_fn=<MseLossBackward0>)\n",
      "425 tensor(7.2049, grad_fn=<MseLossBackward0>)\n",
      "426 tensor(8.0125, grad_fn=<MseLossBackward0>)\n",
      "427 tensor(8.0063, grad_fn=<MseLossBackward0>)\n",
      "428 tensor(16.8711, grad_fn=<MseLossBackward0>)\n",
      "429 tensor(3.9288, grad_fn=<MseLossBackward0>)\n",
      "430 tensor(7.1943, grad_fn=<MseLossBackward0>)\n",
      "431 tensor(7.0846, grad_fn=<MseLossBackward0>)\n",
      "432 tensor(7.7311, grad_fn=<MseLossBackward0>)\n",
      "433 tensor(27.1693, grad_fn=<MseLossBackward0>)\n",
      "434 tensor(19.6058, grad_fn=<MseLossBackward0>)\n",
      "435 tensor(7.6652, grad_fn=<MseLossBackward0>)\n",
      "436 tensor(16.1454, grad_fn=<MseLossBackward0>)\n",
      "437 tensor(8.0083, grad_fn=<MseLossBackward0>)\n",
      "438 tensor(11.4919, grad_fn=<MseLossBackward0>)\n",
      "439 tensor(9.1161, grad_fn=<MseLossBackward0>)\n",
      "440 tensor(7.2192, grad_fn=<MseLossBackward0>)\n",
      "441 tensor(13.4456, grad_fn=<MseLossBackward0>)\n",
      "442 tensor(16.0606, grad_fn=<MseLossBackward0>)\n",
      "443 tensor(4.1201, grad_fn=<MseLossBackward0>)\n",
      "444 tensor(7.7001, grad_fn=<MseLossBackward0>)\n",
      "445 tensor(7.2054, grad_fn=<MseLossBackward0>)\n",
      "446 tensor(8.7067, grad_fn=<MseLossBackward0>)\n",
      "447 tensor(10.7278, grad_fn=<MseLossBackward0>)\n",
      "448 tensor(16.5638, grad_fn=<MseLossBackward0>)\n",
      "449 tensor(17.8750, grad_fn=<MseLossBackward0>)\n",
      "450 tensor(10.4995, grad_fn=<MseLossBackward0>)\n",
      "451 tensor(17.6892, grad_fn=<MseLossBackward0>)\n",
      "452 tensor(6.8595, grad_fn=<MseLossBackward0>)\n",
      "453 tensor(5.1007, grad_fn=<MseLossBackward0>)\n",
      "454 tensor(7.5130, grad_fn=<MseLossBackward0>)\n",
      "455 tensor(18.3849, grad_fn=<MseLossBackward0>)\n",
      "456 tensor(7.3562, grad_fn=<MseLossBackward0>)\n",
      "457 tensor(9.7341, grad_fn=<MseLossBackward0>)\n",
      "458 tensor(6.5121, grad_fn=<MseLossBackward0>)\n",
      "459 tensor(7.3856, grad_fn=<MseLossBackward0>)\n",
      "460 tensor(7.6082, grad_fn=<MseLossBackward0>)\n",
      "461 tensor(6.0027, grad_fn=<MseLossBackward0>)\n",
      "462 tensor(21.1450, grad_fn=<MseLossBackward0>)\n",
      "463 tensor(8.1240, grad_fn=<MseLossBackward0>)\n",
      "464 tensor(13.5865, grad_fn=<MseLossBackward0>)\n",
      "465 tensor(15.6201, grad_fn=<MseLossBackward0>)\n",
      "466 tensor(6.2743, grad_fn=<MseLossBackward0>)\n",
      "467 tensor(14.3085, grad_fn=<MseLossBackward0>)\n",
      "468 tensor(28.0011, grad_fn=<MseLossBackward0>)\n",
      "469 tensor(14.6000, grad_fn=<MseLossBackward0>)\n",
      "470 tensor(8.5330, grad_fn=<MseLossBackward0>)\n",
      "471 tensor(10.0717, grad_fn=<MseLossBackward0>)\n",
      "472 tensor(16.7291, grad_fn=<MseLossBackward0>)\n",
      "473 tensor(9.1288, grad_fn=<MseLossBackward0>)\n",
      "474 tensor(9.2792, grad_fn=<MseLossBackward0>)\n",
      "475 tensor(17.7971, grad_fn=<MseLossBackward0>)\n",
      "476 tensor(21.4026, grad_fn=<MseLossBackward0>)\n",
      "477 tensor(21.0193, grad_fn=<MseLossBackward0>)\n",
      "478 tensor(12.4466, grad_fn=<MseLossBackward0>)\n",
      "479 tensor(30.0578, grad_fn=<MseLossBackward0>)\n",
      "480 tensor(9.6930, grad_fn=<MseLossBackward0>)\n",
      "481 tensor(20.9740, grad_fn=<MseLossBackward0>)\n",
      "482 tensor(7.3422, grad_fn=<MseLossBackward0>)\n",
      "483 tensor(23.7272, grad_fn=<MseLossBackward0>)\n",
      "484 tensor(9.4252, grad_fn=<MseLossBackward0>)\n",
      "485 tensor(7.5174, grad_fn=<MseLossBackward0>)\n",
      "486 tensor(30.7732, grad_fn=<MseLossBackward0>)\n",
      "487 tensor(8.6189, grad_fn=<MseLossBackward0>)\n",
      "488 tensor(7.8830, grad_fn=<MseLossBackward0>)\n",
      "489 tensor(7.7854, grad_fn=<MseLossBackward0>)\n",
      "490 tensor(15.6453, grad_fn=<MseLossBackward0>)\n",
      "491 tensor(21.2794, grad_fn=<MseLossBackward0>)\n",
      "492 tensor(9.1782, grad_fn=<MseLossBackward0>)\n",
      "493 tensor(8.0388, grad_fn=<MseLossBackward0>)\n",
      "494 tensor(12.0841, grad_fn=<MseLossBackward0>)\n",
      "495 tensor(13.9187, grad_fn=<MseLossBackward0>)\n",
      "496 tensor(18.0897, grad_fn=<MseLossBackward0>)\n",
      "497 tensor(8.0009, grad_fn=<MseLossBackward0>)\n",
      "498 tensor(19.1888, grad_fn=<MseLossBackward0>)\n",
      "499 tensor(10.5350, grad_fn=<MseLossBackward0>)\n",
      "500 tensor(7.2924, grad_fn=<MseLossBackward0>)\n",
      "501 tensor(6.6634, grad_fn=<MseLossBackward0>)\n",
      "502 tensor(6.4945, grad_fn=<MseLossBackward0>)\n",
      "503 tensor(21.9787, grad_fn=<MseLossBackward0>)\n",
      "504 tensor(14.6853, grad_fn=<MseLossBackward0>)\n",
      "505 tensor(16.0231, grad_fn=<MseLossBackward0>)\n",
      "506 tensor(20.7566, grad_fn=<MseLossBackward0>)\n",
      "507 tensor(5.3680, grad_fn=<MseLossBackward0>)\n",
      "508 tensor(19.9034, grad_fn=<MseLossBackward0>)\n",
      "509 tensor(12.4295, grad_fn=<MseLossBackward0>)\n",
      "510 tensor(4.1882, grad_fn=<MseLossBackward0>)\n",
      "511 tensor(7.2983, grad_fn=<MseLossBackward0>)\n",
      "512 tensor(12.7925, grad_fn=<MseLossBackward0>)\n",
      "513 tensor(6.9557, grad_fn=<MseLossBackward0>)\n",
      "514 tensor(16.6947, grad_fn=<MseLossBackward0>)\n",
      "515 tensor(16.7238, grad_fn=<MseLossBackward0>)\n",
      "516 tensor(12.9049, grad_fn=<MseLossBackward0>)\n",
      "517 tensor(9.9770, grad_fn=<MseLossBackward0>)\n",
      "518 tensor(14.7837, grad_fn=<MseLossBackward0>)\n",
      "519 tensor(7.9778, grad_fn=<MseLossBackward0>)\n",
      "520 tensor(20.5638, grad_fn=<MseLossBackward0>)\n",
      "521 tensor(9.4058, grad_fn=<MseLossBackward0>)\n",
      "522 tensor(7.4983, grad_fn=<MseLossBackward0>)\n",
      "523 tensor(8.0506, grad_fn=<MseLossBackward0>)\n",
      "524 tensor(7.9047, grad_fn=<MseLossBackward0>)\n",
      "525 tensor(8.8455, grad_fn=<MseLossBackward0>)\n",
      "526 tensor(6.9280, grad_fn=<MseLossBackward0>)\n",
      "527 tensor(20.0571, grad_fn=<MseLossBackward0>)\n",
      "528 tensor(14.6269, grad_fn=<MseLossBackward0>)\n",
      "529 tensor(8.6240, grad_fn=<MseLossBackward0>)\n",
      "530 tensor(8.8890, grad_fn=<MseLossBackward0>)\n",
      "531 tensor(9.5986, grad_fn=<MseLossBackward0>)\n",
      "532 tensor(10.3996, grad_fn=<MseLossBackward0>)\n",
      "533 tensor(5.2705, grad_fn=<MseLossBackward0>)\n",
      "534 tensor(9.8848, grad_fn=<MseLossBackward0>)\n",
      "535 tensor(6.7502, grad_fn=<MseLossBackward0>)\n",
      "536 tensor(6.8326, grad_fn=<MseLossBackward0>)\n",
      "537 tensor(17.2047, grad_fn=<MseLossBackward0>)\n",
      "538 tensor(9.8248, grad_fn=<MseLossBackward0>)\n",
      "539 tensor(6.2858, grad_fn=<MseLossBackward0>)\n",
      "540 tensor(7.7874, grad_fn=<MseLossBackward0>)\n",
      "541 tensor(10.4804, grad_fn=<MseLossBackward0>)\n",
      "542 tensor(26.6732, grad_fn=<MseLossBackward0>)\n",
      "543 tensor(7.3522, grad_fn=<MseLossBackward0>)\n",
      "544 tensor(5.3472, grad_fn=<MseLossBackward0>)\n",
      "545 tensor(8.6053, grad_fn=<MseLossBackward0>)\n",
      "546 tensor(11.7538, grad_fn=<MseLossBackward0>)\n",
      "547 tensor(8.9386, grad_fn=<MseLossBackward0>)\n",
      "548 tensor(7.1227, grad_fn=<MseLossBackward0>)\n",
      "549 tensor(13.4446, grad_fn=<MseLossBackward0>)\n",
      "550 tensor(22.2980, grad_fn=<MseLossBackward0>)\n",
      "551 tensor(6.7142, grad_fn=<MseLossBackward0>)\n",
      "552 tensor(24.9311, grad_fn=<MseLossBackward0>)\n",
      "553 tensor(8.6180, grad_fn=<MseLossBackward0>)\n",
      "554 tensor(7.7508, grad_fn=<MseLossBackward0>)\n",
      "555 tensor(10.5958, grad_fn=<MseLossBackward0>)\n",
      "556 tensor(8.3663, grad_fn=<MseLossBackward0>)\n",
      "557 tensor(16.0384, grad_fn=<MseLossBackward0>)\n",
      "558 tensor(9.7462, grad_fn=<MseLossBackward0>)\n",
      "559 tensor(16.1061, grad_fn=<MseLossBackward0>)\n",
      "560 tensor(17.2361, grad_fn=<MseLossBackward0>)\n",
      "561 tensor(5.0795, grad_fn=<MseLossBackward0>)\n",
      "562 tensor(7.9957, grad_fn=<MseLossBackward0>)\n",
      "563 tensor(8.5648, grad_fn=<MseLossBackward0>)\n",
      "564 tensor(6.9753, grad_fn=<MseLossBackward0>)\n",
      "565 tensor(5.5761, grad_fn=<MseLossBackward0>)\n",
      "566 tensor(10.2584, grad_fn=<MseLossBackward0>)\n",
      "567 tensor(7.2732, grad_fn=<MseLossBackward0>)\n",
      "568 tensor(9.9194, grad_fn=<MseLossBackward0>)\n",
      "569 tensor(18.2474, grad_fn=<MseLossBackward0>)\n",
      "570 tensor(7.0875, grad_fn=<MseLossBackward0>)\n",
      "571 tensor(20.5645, grad_fn=<MseLossBackward0>)\n",
      "572 tensor(5.3623, grad_fn=<MseLossBackward0>)\n",
      "573 tensor(18.8451, grad_fn=<MseLossBackward0>)\n",
      "574 tensor(17.7240, grad_fn=<MseLossBackward0>)\n",
      "575 tensor(9.5250, grad_fn=<MseLossBackward0>)\n",
      "576 tensor(13.4653, grad_fn=<MseLossBackward0>)\n",
      "577 tensor(6.7736, grad_fn=<MseLossBackward0>)\n",
      "578 tensor(7.3203, grad_fn=<MseLossBackward0>)\n",
      "579 tensor(12.6786, grad_fn=<MseLossBackward0>)\n",
      "580 tensor(8.5739, grad_fn=<MseLossBackward0>)\n",
      "581 tensor(21.2055, grad_fn=<MseLossBackward0>)\n",
      "582 tensor(4.9072, grad_fn=<MseLossBackward0>)\n",
      "583 tensor(5.1206, grad_fn=<MseLossBackward0>)\n",
      "584 tensor(6.9395, grad_fn=<MseLossBackward0>)\n",
      "585 tensor(6.0289, grad_fn=<MseLossBackward0>)\n",
      "586 tensor(22.0077, grad_fn=<MseLossBackward0>)\n",
      "587 tensor(35.3575, grad_fn=<MseLossBackward0>)\n",
      "588 tensor(5.1891, grad_fn=<MseLossBackward0>)\n",
      "589 tensor(16.9411, grad_fn=<MseLossBackward0>)\n",
      "590 tensor(6.8366, grad_fn=<MseLossBackward0>)\n",
      "591 tensor(6.4873, grad_fn=<MseLossBackward0>)\n",
      "592 tensor(6.8167, grad_fn=<MseLossBackward0>)\n",
      "593 tensor(10.8594, grad_fn=<MseLossBackward0>)\n",
      "594 tensor(20.9908, grad_fn=<MseLossBackward0>)\n",
      "595 tensor(11.8748, grad_fn=<MseLossBackward0>)\n",
      "596 tensor(8.8592, grad_fn=<MseLossBackward0>)\n",
      "597 tensor(4.8550, grad_fn=<MseLossBackward0>)\n",
      "598 tensor(7.1052, grad_fn=<MseLossBackward0>)\n",
      "599 tensor(7.6587, grad_fn=<MseLossBackward0>)\n",
      "600 tensor(5.3942, grad_fn=<MseLossBackward0>)\n",
      "601 tensor(9.2325, grad_fn=<MseLossBackward0>)\n",
      "602 tensor(9.8517, grad_fn=<MseLossBackward0>)\n",
      "603 tensor(19.5847, grad_fn=<MseLossBackward0>)\n",
      "604 tensor(8.0925, grad_fn=<MseLossBackward0>)\n",
      "605 tensor(14.8606, grad_fn=<MseLossBackward0>)\n",
      "606 tensor(18.3463, grad_fn=<MseLossBackward0>)\n",
      "607 tensor(5.8524, grad_fn=<MseLossBackward0>)\n",
      "608 tensor(6.4981, grad_fn=<MseLossBackward0>)\n",
      "609 tensor(19.1822, grad_fn=<MseLossBackward0>)\n",
      "610 tensor(20.8414, grad_fn=<MseLossBackward0>)\n",
      "611 tensor(16.8049, grad_fn=<MseLossBackward0>)\n",
      "612 tensor(17.0515, grad_fn=<MseLossBackward0>)\n",
      "613 tensor(5.7509, grad_fn=<MseLossBackward0>)\n",
      "614 tensor(14.4681, grad_fn=<MseLossBackward0>)\n",
      "615 tensor(25.3818, grad_fn=<MseLossBackward0>)\n",
      "616 tensor(15.7277, grad_fn=<MseLossBackward0>)\n",
      "617 tensor(12.5061, grad_fn=<MseLossBackward0>)\n",
      "618 tensor(4.0438, grad_fn=<MseLossBackward0>)\n",
      "619 tensor(5.0683, grad_fn=<MseLossBackward0>)\n",
      "620 tensor(7.9010, grad_fn=<MseLossBackward0>)\n",
      "621 tensor(8.0938, grad_fn=<MseLossBackward0>)\n",
      "622 tensor(12.1122, grad_fn=<MseLossBackward0>)\n",
      "623 tensor(9.1312, grad_fn=<MseLossBackward0>)\n",
      "624 tensor(13.6058, grad_fn=<MseLossBackward0>)\n",
      "625 tensor(7.4800, grad_fn=<MseLossBackward0>)\n",
      "626 tensor(5.7906, grad_fn=<MseLossBackward0>)\n",
      "627 tensor(5.1374, grad_fn=<MseLossBackward0>)\n",
      "628 tensor(15.1393, grad_fn=<MseLossBackward0>)\n",
      "629 tensor(5.9419, grad_fn=<MseLossBackward0>)\n",
      "630 tensor(16.7056, grad_fn=<MseLossBackward0>)\n",
      "631 tensor(7.7305, grad_fn=<MseLossBackward0>)\n",
      "632 tensor(5.4877, grad_fn=<MseLossBackward0>)\n",
      "633 tensor(13.4776, grad_fn=<MseLossBackward0>)\n",
      "634 tensor(5.7317, grad_fn=<MseLossBackward0>)\n",
      "635 tensor(7.2638, grad_fn=<MseLossBackward0>)\n",
      "636 tensor(12.9558, grad_fn=<MseLossBackward0>)\n",
      "637 tensor(8.8774, grad_fn=<MseLossBackward0>)\n",
      "638 tensor(5.2278, grad_fn=<MseLossBackward0>)\n",
      "639 tensor(11.7741, grad_fn=<MseLossBackward0>)\n",
      "640 tensor(7.1680, grad_fn=<MseLossBackward0>)\n",
      "641 tensor(18.2308, grad_fn=<MseLossBackward0>)\n",
      "642 tensor(6.8321, grad_fn=<MseLossBackward0>)\n",
      "643 tensor(6.3226, grad_fn=<MseLossBackward0>)\n",
      "644 tensor(13.6253, grad_fn=<MseLossBackward0>)\n",
      "645 tensor(8.2637, grad_fn=<MseLossBackward0>)\n",
      "646 tensor(10.0910, grad_fn=<MseLossBackward0>)\n",
      "647 tensor(18.2234, grad_fn=<MseLossBackward0>)\n",
      "648 tensor(6.9661, grad_fn=<MseLossBackward0>)\n",
      "649 tensor(16.5757, grad_fn=<MseLossBackward0>)\n",
      "650 tensor(6.6986, grad_fn=<MseLossBackward0>)\n",
      "651 tensor(17.0193, grad_fn=<MseLossBackward0>)\n",
      "652 tensor(6.0054, grad_fn=<MseLossBackward0>)\n",
      "653 tensor(8.0502, grad_fn=<MseLossBackward0>)\n",
      "654 tensor(7.2577, grad_fn=<MseLossBackward0>)\n",
      "655 tensor(9.3947, grad_fn=<MseLossBackward0>)\n",
      "656 tensor(9.4162, grad_fn=<MseLossBackward0>)\n",
      "657 tensor(6.9622, grad_fn=<MseLossBackward0>)\n",
      "658 tensor(10.3886, grad_fn=<MseLossBackward0>)\n",
      "659 tensor(8.3007, grad_fn=<MseLossBackward0>)\n",
      "660 tensor(7.6046, grad_fn=<MseLossBackward0>)\n",
      "661 tensor(10.2053, grad_fn=<MseLossBackward0>)\n",
      "662 tensor(15.5290, grad_fn=<MseLossBackward0>)\n",
      "663 tensor(7.4510, grad_fn=<MseLossBackward0>)\n",
      "664 tensor(6.6545, grad_fn=<MseLossBackward0>)\n",
      "665 tensor(6.9764, grad_fn=<MseLossBackward0>)\n",
      "666 tensor(14.5584, grad_fn=<MseLossBackward0>)\n",
      "667 tensor(16.9395, grad_fn=<MseLossBackward0>)\n",
      "668 tensor(26.5700, grad_fn=<MseLossBackward0>)\n",
      "669 tensor(18.4048, grad_fn=<MseLossBackward0>)\n",
      "670 tensor(11.7112, grad_fn=<MseLossBackward0>)\n",
      "671 tensor(8.5514, grad_fn=<MseLossBackward0>)\n",
      "672 tensor(10.6153, grad_fn=<MseLossBackward0>)\n",
      "673 tensor(5.9628, grad_fn=<MseLossBackward0>)\n",
      "674 tensor(26.7670, grad_fn=<MseLossBackward0>)\n",
      "675 tensor(16.1220, grad_fn=<MseLossBackward0>)\n",
      "676 tensor(5.1599, grad_fn=<MseLossBackward0>)\n",
      "677 tensor(12.1073, grad_fn=<MseLossBackward0>)\n",
      "678 tensor(17.0422, grad_fn=<MseLossBackward0>)\n",
      "679 tensor(8.0232, grad_fn=<MseLossBackward0>)\n",
      "680 tensor(18.2185, grad_fn=<MseLossBackward0>)\n",
      "681 tensor(20.7507, grad_fn=<MseLossBackward0>)\n",
      "682 tensor(6.1063, grad_fn=<MseLossBackward0>)\n",
      "683 tensor(10.9716, grad_fn=<MseLossBackward0>)\n",
      "684 tensor(4.5622, grad_fn=<MseLossBackward0>)\n",
      "685 tensor(9.4777, grad_fn=<MseLossBackward0>)\n",
      "686 tensor(10.1479, grad_fn=<MseLossBackward0>)\n",
      "687 tensor(8.8499, grad_fn=<MseLossBackward0>)\n",
      "688 tensor(5.8850, grad_fn=<MseLossBackward0>)\n",
      "689 tensor(5.6638, grad_fn=<MseLossBackward0>)\n",
      "690 tensor(8.5704, grad_fn=<MseLossBackward0>)\n",
      "691 tensor(8.3809, grad_fn=<MseLossBackward0>)\n",
      "692 tensor(6.6106, grad_fn=<MseLossBackward0>)\n",
      "693 tensor(10.1633, grad_fn=<MseLossBackward0>)\n",
      "694 tensor(5.9441, grad_fn=<MseLossBackward0>)\n",
      "695 tensor(23.1326, grad_fn=<MseLossBackward0>)\n",
      "696 tensor(3.4935, grad_fn=<MseLossBackward0>)\n",
      "697 tensor(8.6705, grad_fn=<MseLossBackward0>)\n",
      "698 tensor(6.6727, grad_fn=<MseLossBackward0>)\n",
      "699 tensor(8.6864, grad_fn=<MseLossBackward0>)\n",
      "700 tensor(18.8469, grad_fn=<MseLossBackward0>)\n",
      "701 tensor(5.3834, grad_fn=<MseLossBackward0>)\n",
      "702 tensor(5.3135, grad_fn=<MseLossBackward0>)\n",
      "703 tensor(5.0989, grad_fn=<MseLossBackward0>)\n",
      "704 tensor(6.3201, grad_fn=<MseLossBackward0>)\n",
      "705 tensor(28.0423, grad_fn=<MseLossBackward0>)\n",
      "706 tensor(8.6889, grad_fn=<MseLossBackward0>)\n",
      "707 tensor(8.6821, grad_fn=<MseLossBackward0>)\n",
      "708 tensor(15.1205, grad_fn=<MseLossBackward0>)\n",
      "709 tensor(8.6630, grad_fn=<MseLossBackward0>)\n",
      "710 tensor(10.1282, grad_fn=<MseLossBackward0>)\n",
      "711 tensor(5.6465, grad_fn=<MseLossBackward0>)\n",
      "712 tensor(13.6989, grad_fn=<MseLossBackward0>)\n",
      "713 tensor(6.6805, grad_fn=<MseLossBackward0>)\n",
      "714 tensor(6.0069, grad_fn=<MseLossBackward0>)\n",
      "715 tensor(9.0907, grad_fn=<MseLossBackward0>)\n",
      "716 tensor(6.6187, grad_fn=<MseLossBackward0>)\n",
      "717 tensor(6.1931, grad_fn=<MseLossBackward0>)\n",
      "718 tensor(13.1780, grad_fn=<MseLossBackward0>)\n",
      "719 tensor(16.1966, grad_fn=<MseLossBackward0>)\n",
      "720 tensor(6.9554, grad_fn=<MseLossBackward0>)\n",
      "721 tensor(7.3871, grad_fn=<MseLossBackward0>)\n",
      "722 tensor(15.0112, grad_fn=<MseLossBackward0>)\n",
      "723 tensor(15.8578, grad_fn=<MseLossBackward0>)\n",
      "724 tensor(14.8738, grad_fn=<MseLossBackward0>)\n",
      "725 tensor(16.3221, grad_fn=<MseLossBackward0>)\n",
      "726 tensor(9.6944, grad_fn=<MseLossBackward0>)\n",
      "727 tensor(8.2076, grad_fn=<MseLossBackward0>)\n",
      "728 tensor(15.4079, grad_fn=<MseLossBackward0>)\n",
      "729 tensor(15.8743, grad_fn=<MseLossBackward0>)\n",
      "730 tensor(5.3787, grad_fn=<MseLossBackward0>)\n",
      "731 tensor(7.4321, grad_fn=<MseLossBackward0>)\n",
      "732 tensor(18.0306, grad_fn=<MseLossBackward0>)\n",
      "733 tensor(16.9510, grad_fn=<MseLossBackward0>)\n",
      "734 tensor(15.7792, grad_fn=<MseLossBackward0>)\n",
      "735 tensor(18.1517, grad_fn=<MseLossBackward0>)\n",
      "736 tensor(5.0581, grad_fn=<MseLossBackward0>)\n",
      "737 tensor(7.3332, grad_fn=<MseLossBackward0>)\n",
      "738 tensor(5.5288, grad_fn=<MseLossBackward0>)\n",
      "739 tensor(13.0202, grad_fn=<MseLossBackward0>)\n",
      "740 tensor(8.0039, grad_fn=<MseLossBackward0>)\n",
      "741 tensor(8.4867, grad_fn=<MseLossBackward0>)\n",
      "742 tensor(5.8096, grad_fn=<MseLossBackward0>)\n",
      "743 tensor(17.1303, grad_fn=<MseLossBackward0>)\n",
      "744 tensor(6.7710, grad_fn=<MseLossBackward0>)\n",
      "745 tensor(8.8384, grad_fn=<MseLossBackward0>)\n",
      "746 tensor(12.4589, grad_fn=<MseLossBackward0>)\n",
      "747 tensor(6.3070, grad_fn=<MseLossBackward0>)\n",
      "748 tensor(7.5683, grad_fn=<MseLossBackward0>)\n",
      "749 tensor(7.6675, grad_fn=<MseLossBackward0>)\n",
      "750 tensor(8.7394, grad_fn=<MseLossBackward0>)\n",
      "751 tensor(7.9959, grad_fn=<MseLossBackward0>)\n",
      "752 tensor(6.5009, grad_fn=<MseLossBackward0>)\n",
      "753 tensor(4.8176, grad_fn=<MseLossBackward0>)\n",
      "754 tensor(12.6329, grad_fn=<MseLossBackward0>)\n",
      "755 tensor(11.5783, grad_fn=<MseLossBackward0>)\n",
      "756 tensor(9.2119, grad_fn=<MseLossBackward0>)\n",
      "757 tensor(5.6356, grad_fn=<MseLossBackward0>)\n",
      "758 tensor(6.0797, grad_fn=<MseLossBackward0>)\n",
      "759 tensor(12.0059, grad_fn=<MseLossBackward0>)\n",
      "760 tensor(5.8047, grad_fn=<MseLossBackward0>)\n",
      "761 tensor(6.5709, grad_fn=<MseLossBackward0>)\n",
      "762 tensor(7.0246, grad_fn=<MseLossBackward0>)\n",
      "763 tensor(11.1539, grad_fn=<MseLossBackward0>)\n",
      "764 tensor(16.9493, grad_fn=<MseLossBackward0>)\n",
      "765 tensor(16.3897, grad_fn=<MseLossBackward0>)\n",
      "766 tensor(6.4518, grad_fn=<MseLossBackward0>)\n",
      "767 tensor(4.8347, grad_fn=<MseLossBackward0>)\n",
      "768 tensor(8.7279, grad_fn=<MseLossBackward0>)\n",
      "769 tensor(7.7660, grad_fn=<MseLossBackward0>)\n",
      "770 tensor(5.0452, grad_fn=<MseLossBackward0>)\n",
      "771 tensor(8.3879, grad_fn=<MseLossBackward0>)\n",
      "772 tensor(6.9990, grad_fn=<MseLossBackward0>)\n",
      "773 tensor(7.7621, grad_fn=<MseLossBackward0>)\n",
      "774 tensor(18.6153, grad_fn=<MseLossBackward0>)\n",
      "775 tensor(7.1813, grad_fn=<MseLossBackward0>)\n",
      "776 tensor(19.0251, grad_fn=<MseLossBackward0>)\n",
      "777 tensor(7.0988, grad_fn=<MseLossBackward0>)\n",
      "778 tensor(14.2235, grad_fn=<MseLossBackward0>)\n",
      "779 tensor(10.8333, grad_fn=<MseLossBackward0>)\n",
      "780 tensor(6.8091, grad_fn=<MseLossBackward0>)\n",
      "781 tensor(19.2388, grad_fn=<MseLossBackward0>)\n",
      "782 tensor(6.9025, grad_fn=<MseLossBackward0>)\n",
      "783 tensor(6.9803, grad_fn=<MseLossBackward0>)\n",
      "784 tensor(6.9300, grad_fn=<MseLossBackward0>)\n",
      "785 tensor(24.1308, grad_fn=<MseLossBackward0>)\n",
      "786 tensor(4.8678, grad_fn=<MseLossBackward0>)\n",
      "787 tensor(5.8153, grad_fn=<MseLossBackward0>)\n",
      "788 tensor(6.4950, grad_fn=<MseLossBackward0>)\n",
      "789 tensor(6.6401, grad_fn=<MseLossBackward0>)\n",
      "790 tensor(13.3188, grad_fn=<MseLossBackward0>)\n",
      "791 tensor(5.2990, grad_fn=<MseLossBackward0>)\n",
      "792 tensor(6.6359, grad_fn=<MseLossBackward0>)\n",
      "793 tensor(14.6211, grad_fn=<MseLossBackward0>)\n",
      "794 tensor(7.7193, grad_fn=<MseLossBackward0>)\n",
      "795 tensor(6.7953, grad_fn=<MseLossBackward0>)\n",
      "796 tensor(11.5733, grad_fn=<MseLossBackward0>)\n",
      "797 tensor(11.2970, grad_fn=<MseLossBackward0>)\n",
      "798 tensor(6.5697, grad_fn=<MseLossBackward0>)\n",
      "799 tensor(6.5577, grad_fn=<MseLossBackward0>)\n",
      "800 tensor(6.1942, grad_fn=<MseLossBackward0>)\n",
      "801 tensor(9.3965, grad_fn=<MseLossBackward0>)\n",
      "802 tensor(4.5289, grad_fn=<MseLossBackward0>)\n",
      "803 tensor(14.2574, grad_fn=<MseLossBackward0>)\n",
      "804 tensor(17.0762, grad_fn=<MseLossBackward0>)\n",
      "805 tensor(5.8896, grad_fn=<MseLossBackward0>)\n",
      "806 tensor(4.8248, grad_fn=<MseLossBackward0>)\n",
      "807 tensor(7.5561, grad_fn=<MseLossBackward0>)\n",
      "808 tensor(7.8450, grad_fn=<MseLossBackward0>)\n",
      "809 tensor(8.2076, grad_fn=<MseLossBackward0>)\n",
      "810 tensor(7.2192, grad_fn=<MseLossBackward0>)\n",
      "811 tensor(6.7343, grad_fn=<MseLossBackward0>)\n",
      "812 tensor(5.7501, grad_fn=<MseLossBackward0>)\n",
      "813 tensor(5.1161, grad_fn=<MseLossBackward0>)\n",
      "814 tensor(8.1514, grad_fn=<MseLossBackward0>)\n",
      "815 tensor(5.6977, grad_fn=<MseLossBackward0>)\n",
      "816 tensor(17.3738, grad_fn=<MseLossBackward0>)\n",
      "817 tensor(7.5516, grad_fn=<MseLossBackward0>)\n",
      "818 tensor(4.4390, grad_fn=<MseLossBackward0>)\n",
      "819 tensor(11.1980, grad_fn=<MseLossBackward0>)\n",
      "820 tensor(5.4907, grad_fn=<MseLossBackward0>)\n",
      "821 tensor(14.7759, grad_fn=<MseLossBackward0>)\n",
      "822 tensor(10.9706, grad_fn=<MseLossBackward0>)\n",
      "823 tensor(12.9650, grad_fn=<MseLossBackward0>)\n",
      "824 tensor(16.9701, grad_fn=<MseLossBackward0>)\n",
      "825 tensor(13.2947, grad_fn=<MseLossBackward0>)\n",
      "826 tensor(11.6663, grad_fn=<MseLossBackward0>)\n",
      "827 tensor(6.4718, grad_fn=<MseLossBackward0>)\n",
      "828 tensor(4.9375, grad_fn=<MseLossBackward0>)\n",
      "829 tensor(6.2370, grad_fn=<MseLossBackward0>)\n",
      "830 tensor(5.3452, grad_fn=<MseLossBackward0>)\n",
      "831 tensor(4.4223, grad_fn=<MseLossBackward0>)\n",
      "832 tensor(13.7718, grad_fn=<MseLossBackward0>)\n",
      "833 tensor(10.2272, grad_fn=<MseLossBackward0>)\n",
      "834 tensor(15.2398, grad_fn=<MseLossBackward0>)\n",
      "835 tensor(8.8006, grad_fn=<MseLossBackward0>)\n",
      "836 tensor(19.7389, grad_fn=<MseLossBackward0>)\n",
      "837 tensor(23.0965, grad_fn=<MseLossBackward0>)\n",
      "838 tensor(24.6173, grad_fn=<MseLossBackward0>)\n",
      "839 tensor(19.4936, grad_fn=<MseLossBackward0>)\n",
      "840 tensor(4.8854, grad_fn=<MseLossBackward0>)\n",
      "841 tensor(14.6724, grad_fn=<MseLossBackward0>)\n",
      "842 tensor(4.7866, grad_fn=<MseLossBackward0>)\n",
      "843 tensor(26.1871, grad_fn=<MseLossBackward0>)\n",
      "844 tensor(7.1743, grad_fn=<MseLossBackward0>)\n",
      "845 tensor(7.0312, grad_fn=<MseLossBackward0>)\n",
      "846 tensor(5.3513, grad_fn=<MseLossBackward0>)\n",
      "847 tensor(13.8514, grad_fn=<MseLossBackward0>)\n",
      "848 tensor(11.9381, grad_fn=<MseLossBackward0>)\n",
      "849 tensor(5.1066, grad_fn=<MseLossBackward0>)\n",
      "850 tensor(8.7091, grad_fn=<MseLossBackward0>)\n",
      "851 tensor(5.0386, grad_fn=<MseLossBackward0>)\n",
      "852 tensor(10.8234, grad_fn=<MseLossBackward0>)\n",
      "853 tensor(3.8510, grad_fn=<MseLossBackward0>)\n",
      "854 tensor(4.9465, grad_fn=<MseLossBackward0>)\n",
      "855 tensor(6.3836, grad_fn=<MseLossBackward0>)\n",
      "856 tensor(4.8511, grad_fn=<MseLossBackward0>)\n",
      "857 tensor(3.8822, grad_fn=<MseLossBackward0>)\n",
      "858 tensor(5.2262, grad_fn=<MseLossBackward0>)\n",
      "859 tensor(4.3999, grad_fn=<MseLossBackward0>)\n",
      "860 tensor(4.6179, grad_fn=<MseLossBackward0>)\n",
      "861 tensor(5.6397, grad_fn=<MseLossBackward0>)\n",
      "862 tensor(5.9829, grad_fn=<MseLossBackward0>)\n",
      "863 tensor(6.6153, grad_fn=<MseLossBackward0>)\n",
      "864 tensor(6.1282, grad_fn=<MseLossBackward0>)\n",
      "865 tensor(14.0225, grad_fn=<MseLossBackward0>)\n",
      "866 tensor(15.9527, grad_fn=<MseLossBackward0>)\n",
      "867 tensor(14.6302, grad_fn=<MseLossBackward0>)\n",
      "868 tensor(15.5725, grad_fn=<MseLossBackward0>)\n",
      "869 tensor(6.4983, grad_fn=<MseLossBackward0>)\n",
      "870 tensor(5.6766, grad_fn=<MseLossBackward0>)\n",
      "871 tensor(6.6643, grad_fn=<MseLossBackward0>)\n",
      "872 tensor(13.5125, grad_fn=<MseLossBackward0>)\n",
      "873 tensor(7.3447, grad_fn=<MseLossBackward0>)\n",
      "874 tensor(5.0046, grad_fn=<MseLossBackward0>)\n",
      "875 tensor(5.1796, grad_fn=<MseLossBackward0>)\n",
      "876 tensor(30.4818, grad_fn=<MseLossBackward0>)\n",
      "877 tensor(4.2874, grad_fn=<MseLossBackward0>)\n",
      "878 tensor(4.7571, grad_fn=<MseLossBackward0>)\n",
      "879 tensor(6.5835, grad_fn=<MseLossBackward0>)\n",
      "880 tensor(11.3964, grad_fn=<MseLossBackward0>)\n",
      "881 tensor(5.5120, grad_fn=<MseLossBackward0>)\n",
      "882 tensor(7.9692, grad_fn=<MseLossBackward0>)\n",
      "883 tensor(14.3166, grad_fn=<MseLossBackward0>)\n",
      "884 tensor(5.4755, grad_fn=<MseLossBackward0>)\n",
      "885 tensor(9.6484, grad_fn=<MseLossBackward0>)\n",
      "886 tensor(4.6198, grad_fn=<MseLossBackward0>)\n",
      "887 tensor(5.5716, grad_fn=<MseLossBackward0>)\n",
      "888 tensor(7.3341, grad_fn=<MseLossBackward0>)\n",
      "889 tensor(7.2045, grad_fn=<MseLossBackward0>)\n",
      "890 tensor(7.8374, grad_fn=<MseLossBackward0>)\n",
      "891 tensor(13.4281, grad_fn=<MseLossBackward0>)\n",
      "892 tensor(13.6706, grad_fn=<MseLossBackward0>)\n",
      "893 tensor(14.5109, grad_fn=<MseLossBackward0>)\n",
      "894 tensor(14.1296, grad_fn=<MseLossBackward0>)\n",
      "895 tensor(5.3657, grad_fn=<MseLossBackward0>)\n",
      "896 tensor(4.7976, grad_fn=<MseLossBackward0>)\n",
      "897 tensor(15.2338, grad_fn=<MseLossBackward0>)\n",
      "898 tensor(5.8971, grad_fn=<MseLossBackward0>)\n",
      "899 tensor(7.1702, grad_fn=<MseLossBackward0>)\n",
      "900 tensor(12.4335, grad_fn=<MseLossBackward0>)\n",
      "901 tensor(6.7856, grad_fn=<MseLossBackward0>)\n",
      "902 tensor(14.2646, grad_fn=<MseLossBackward0>)\n",
      "903 tensor(3.8534, grad_fn=<MseLossBackward0>)\n",
      "904 tensor(5.0684, grad_fn=<MseLossBackward0>)\n",
      "905 tensor(5.4968, grad_fn=<MseLossBackward0>)\n",
      "906 tensor(5.9925, grad_fn=<MseLossBackward0>)\n",
      "907 tensor(16.9987, grad_fn=<MseLossBackward0>)\n",
      "908 tensor(9.6567, grad_fn=<MseLossBackward0>)\n",
      "909 tensor(4.7182, grad_fn=<MseLossBackward0>)\n",
      "910 tensor(14.7073, grad_fn=<MseLossBackward0>)\n",
      "911 tensor(15.4417, grad_fn=<MseLossBackward0>)\n",
      "912 tensor(6.5055, grad_fn=<MseLossBackward0>)\n",
      "913 tensor(13.5831, grad_fn=<MseLossBackward0>)\n",
      "914 tensor(4.9056, grad_fn=<MseLossBackward0>)\n",
      "915 tensor(14.7547, grad_fn=<MseLossBackward0>)\n",
      "916 tensor(13.6544, grad_fn=<MseLossBackward0>)\n",
      "917 tensor(12.2772, grad_fn=<MseLossBackward0>)\n",
      "918 tensor(5.0749, grad_fn=<MseLossBackward0>)\n",
      "919 tensor(3.8731, grad_fn=<MseLossBackward0>)\n",
      "920 tensor(14.6617, grad_fn=<MseLossBackward0>)\n",
      "921 tensor(5.0329, grad_fn=<MseLossBackward0>)\n",
      "922 tensor(11.5211, grad_fn=<MseLossBackward0>)\n",
      "923 tensor(9.2573, grad_fn=<MseLossBackward0>)\n",
      "924 tensor(13.7395, grad_fn=<MseLossBackward0>)\n",
      "925 tensor(12.5314, grad_fn=<MseLossBackward0>)\n",
      "926 tensor(5.5684, grad_fn=<MseLossBackward0>)\n",
      "927 tensor(5.1347, grad_fn=<MseLossBackward0>)\n",
      "928 tensor(4.5031, grad_fn=<MseLossBackward0>)\n",
      "929 tensor(13.1677, grad_fn=<MseLossBackward0>)\n",
      "930 tensor(6.6616, grad_fn=<MseLossBackward0>)\n",
      "931 tensor(6.3485, grad_fn=<MseLossBackward0>)\n",
      "932 tensor(13.3553, grad_fn=<MseLossBackward0>)\n",
      "933 tensor(18.8591, grad_fn=<MseLossBackward0>)\n",
      "934 tensor(7.8247, grad_fn=<MseLossBackward0>)\n",
      "935 tensor(13.0844, grad_fn=<MseLossBackward0>)\n",
      "936 tensor(6.7236, grad_fn=<MseLossBackward0>)\n",
      "937 tensor(6.9088, grad_fn=<MseLossBackward0>)\n",
      "938 tensor(14.1481, grad_fn=<MseLossBackward0>)\n",
      "939 tensor(6.4125, grad_fn=<MseLossBackward0>)\n",
      "940 tensor(5.8971, grad_fn=<MseLossBackward0>)\n",
      "941 tensor(11.8844, grad_fn=<MseLossBackward0>)\n",
      "942 tensor(8.0518, grad_fn=<MseLossBackward0>)\n",
      "943 tensor(10.7714, grad_fn=<MseLossBackward0>)\n",
      "944 tensor(7.0613, grad_fn=<MseLossBackward0>)\n",
      "945 tensor(10.1968, grad_fn=<MseLossBackward0>)\n",
      "946 tensor(12.1251, grad_fn=<MseLossBackward0>)\n",
      "947 tensor(7.1382, grad_fn=<MseLossBackward0>)\n",
      "948 tensor(8.7098, grad_fn=<MseLossBackward0>)\n",
      "949 tensor(5.9902, grad_fn=<MseLossBackward0>)\n",
      "950 tensor(13.3638, grad_fn=<MseLossBackward0>)\n",
      "951 tensor(13.0271, grad_fn=<MseLossBackward0>)\n",
      "952 tensor(3.7731, grad_fn=<MseLossBackward0>)\n",
      "953 tensor(4.5187, grad_fn=<MseLossBackward0>)\n",
      "954 tensor(6.2790, grad_fn=<MseLossBackward0>)\n",
      "955 tensor(11.2125, grad_fn=<MseLossBackward0>)\n",
      "956 tensor(6.1488, grad_fn=<MseLossBackward0>)\n",
      "957 tensor(12.5525, grad_fn=<MseLossBackward0>)\n",
      "958 tensor(5.2561, grad_fn=<MseLossBackward0>)\n",
      "959 tensor(3.4159, grad_fn=<MseLossBackward0>)\n",
      "960 tensor(13.4796, grad_fn=<MseLossBackward0>)\n",
      "961 tensor(5.8230, grad_fn=<MseLossBackward0>)\n",
      "962 tensor(13.0431, grad_fn=<MseLossBackward0>)\n",
      "963 tensor(3.4981, grad_fn=<MseLossBackward0>)\n",
      "964 tensor(8.8377, grad_fn=<MseLossBackward0>)\n",
      "965 tensor(4.2826, grad_fn=<MseLossBackward0>)\n",
      "966 tensor(8.3358, grad_fn=<MseLossBackward0>)\n",
      "967 tensor(6.3942, grad_fn=<MseLossBackward0>)\n",
      "968 tensor(8.1229, grad_fn=<MseLossBackward0>)\n",
      "969 tensor(5.6142, grad_fn=<MseLossBackward0>)\n",
      "970 tensor(6.3046, grad_fn=<MseLossBackward0>)\n",
      "971 tensor(9.0043, grad_fn=<MseLossBackward0>)\n",
      "972 tensor(9.6502, grad_fn=<MseLossBackward0>)\n",
      "973 tensor(18.8918, grad_fn=<MseLossBackward0>)\n",
      "974 tensor(6.3245, grad_fn=<MseLossBackward0>)\n",
      "975 tensor(4.2877, grad_fn=<MseLossBackward0>)\n",
      "976 tensor(14.0966, grad_fn=<MseLossBackward0>)\n",
      "977 tensor(10.5485, grad_fn=<MseLossBackward0>)\n",
      "978 tensor(4.2859, grad_fn=<MseLossBackward0>)\n",
      "979 tensor(5.8960, grad_fn=<MseLossBackward0>)\n",
      "980 tensor(13.2668, grad_fn=<MseLossBackward0>)\n",
      "981 tensor(15.0196, grad_fn=<MseLossBackward0>)\n",
      "982 tensor(6.3882, grad_fn=<MseLossBackward0>)\n",
      "983 tensor(4.6011, grad_fn=<MseLossBackward0>)\n",
      "984 tensor(5.3725, grad_fn=<MseLossBackward0>)\n",
      "985 tensor(5.1793, grad_fn=<MseLossBackward0>)\n",
      "986 tensor(3.6092, grad_fn=<MseLossBackward0>)\n",
      "987 tensor(6.0293, grad_fn=<MseLossBackward0>)\n",
      "988 tensor(2.8212, grad_fn=<MseLossBackward0>)\n",
      "989 tensor(8.4973, grad_fn=<MseLossBackward0>)\n",
      "990 tensor(10.8242, grad_fn=<MseLossBackward0>)\n",
      "991 tensor(17.8006, grad_fn=<MseLossBackward0>)\n",
      "992 tensor(4.4226, grad_fn=<MseLossBackward0>)\n",
      "993 tensor(19.0466, grad_fn=<MseLossBackward0>)\n",
      "994 tensor(7.4798, grad_fn=<MseLossBackward0>)\n",
      "995 tensor(8.7613, grad_fn=<MseLossBackward0>)\n",
      "996 tensor(5.2403, grad_fn=<MseLossBackward0>)\n",
      "997 tensor(6.2471, grad_fn=<MseLossBackward0>)\n",
      "998 tensor(5.3354, grad_fn=<MseLossBackward0>)\n",
      "999 tensor(4.9482, grad_fn=<MseLossBackward0>)\n",
      "1000 tensor(3.4440, grad_fn=<MseLossBackward0>)\n",
      "1001 tensor(16.7312, grad_fn=<MseLossBackward0>)\n",
      "1002 tensor(5.7687, grad_fn=<MseLossBackward0>)\n",
      "1003 tensor(4.8018, grad_fn=<MseLossBackward0>)\n",
      "1004 tensor(10.3441, grad_fn=<MseLossBackward0>)\n",
      "1005 tensor(10.2927, grad_fn=<MseLossBackward0>)\n",
      "1006 tensor(8.3582, grad_fn=<MseLossBackward0>)\n",
      "1007 tensor(16.2198, grad_fn=<MseLossBackward0>)\n",
      "1008 tensor(6.2017, grad_fn=<MseLossBackward0>)\n",
      "1009 tensor(6.6049, grad_fn=<MseLossBackward0>)\n",
      "1010 tensor(4.2402, grad_fn=<MseLossBackward0>)\n",
      "1011 tensor(6.9057, grad_fn=<MseLossBackward0>)\n",
      "1012 tensor(5.1802, grad_fn=<MseLossBackward0>)\n",
      "1013 tensor(8.5793, grad_fn=<MseLossBackward0>)\n",
      "1014 tensor(7.9634, grad_fn=<MseLossBackward0>)\n",
      "1015 tensor(4.0770, grad_fn=<MseLossBackward0>)\n",
      "1016 tensor(11.6813, grad_fn=<MseLossBackward0>)\n",
      "1017 tensor(17.2343, grad_fn=<MseLossBackward0>)\n",
      "1018 tensor(6.5160, grad_fn=<MseLossBackward0>)\n",
      "1019 tensor(13.7622, grad_fn=<MseLossBackward0>)\n",
      "1020 tensor(6.3625, grad_fn=<MseLossBackward0>)\n",
      "1021 tensor(3.9269, grad_fn=<MseLossBackward0>)\n",
      "1022 tensor(5.0926, grad_fn=<MseLossBackward0>)\n",
      "1023 tensor(5.4761, grad_fn=<MseLossBackward0>)\n",
      "1024 tensor(15.8955, grad_fn=<MseLossBackward0>)\n",
      "1025 tensor(7.9657, grad_fn=<MseLossBackward0>)\n",
      "1026 tensor(6.7020, grad_fn=<MseLossBackward0>)\n",
      "1027 tensor(5.7084, grad_fn=<MseLossBackward0>)\n",
      "1028 tensor(5.3144, grad_fn=<MseLossBackward0>)\n",
      "1029 tensor(5.8982, grad_fn=<MseLossBackward0>)\n",
      "1030 tensor(10.3106, grad_fn=<MseLossBackward0>)\n",
      "1031 tensor(6.2633, grad_fn=<MseLossBackward0>)\n",
      "1032 tensor(12.7799, grad_fn=<MseLossBackward0>)\n",
      "1033 tensor(10.2514, grad_fn=<MseLossBackward0>)\n",
      "1034 tensor(4.8536, grad_fn=<MseLossBackward0>)\n",
      "1035 tensor(7.2542, grad_fn=<MseLossBackward0>)\n",
      "1036 tensor(5.5680, grad_fn=<MseLossBackward0>)\n",
      "1037 tensor(4.1381, grad_fn=<MseLossBackward0>)\n",
      "1038 tensor(4.7883, grad_fn=<MseLossBackward0>)\n",
      "1039 tensor(19.6582, grad_fn=<MseLossBackward0>)\n",
      "1040 tensor(5.2030, grad_fn=<MseLossBackward0>)\n",
      "1041 tensor(6.3579, grad_fn=<MseLossBackward0>)\n",
      "1042 tensor(9.5888, grad_fn=<MseLossBackward0>)\n",
      "1043 tensor(5.3887, grad_fn=<MseLossBackward0>)\n",
      "1044 tensor(9.0309, grad_fn=<MseLossBackward0>)\n",
      "1045 tensor(5.0196, grad_fn=<MseLossBackward0>)\n",
      "1046 tensor(6.6044, grad_fn=<MseLossBackward0>)\n",
      "1047 tensor(6.8450, grad_fn=<MseLossBackward0>)\n",
      "1048 tensor(9.7677, grad_fn=<MseLossBackward0>)\n",
      "1049 tensor(4.2536, grad_fn=<MseLossBackward0>)\n",
      "1050 tensor(5.3542, grad_fn=<MseLossBackward0>)\n",
      "1051 tensor(13.3965, grad_fn=<MseLossBackward0>)\n",
      "1052 tensor(4.9828, grad_fn=<MseLossBackward0>)\n",
      "1053 tensor(7.5950, grad_fn=<MseLossBackward0>)\n",
      "1054 tensor(14.3738, grad_fn=<MseLossBackward0>)\n",
      "1055 tensor(14.6729, grad_fn=<MseLossBackward0>)\n",
      "1056 tensor(11.9892, grad_fn=<MseLossBackward0>)\n",
      "1057 tensor(12.4638, grad_fn=<MseLossBackward0>)\n",
      "1058 tensor(5.2098, grad_fn=<MseLossBackward0>)\n",
      "1059 tensor(5.5324, grad_fn=<MseLossBackward0>)\n",
      "1060 tensor(7.2313, grad_fn=<MseLossBackward0>)\n",
      "1061 tensor(6.3165, grad_fn=<MseLossBackward0>)\n",
      "1062 tensor(5.8964, grad_fn=<MseLossBackward0>)\n",
      "1063 tensor(8.8578, grad_fn=<MseLossBackward0>)\n",
      "1064 tensor(5.0960, grad_fn=<MseLossBackward0>)\n",
      "1065 tensor(13.4118, grad_fn=<MseLossBackward0>)\n",
      "1066 tensor(6.3694, grad_fn=<MseLossBackward0>)\n",
      "1067 tensor(14.7156, grad_fn=<MseLossBackward0>)\n",
      "1068 tensor(8.2276, grad_fn=<MseLossBackward0>)\n",
      "1069 tensor(15.3306, grad_fn=<MseLossBackward0>)\n",
      "1070 tensor(5.7033, grad_fn=<MseLossBackward0>)\n",
      "1071 tensor(3.9708, grad_fn=<MseLossBackward0>)\n",
      "1072 tensor(16.7544, grad_fn=<MseLossBackward0>)\n",
      "1073 tensor(6.4677, grad_fn=<MseLossBackward0>)\n",
      "1074 tensor(3.0143, grad_fn=<MseLossBackward0>)\n",
      "1075 tensor(19.5863, grad_fn=<MseLossBackward0>)\n",
      "1076 tensor(10.0795, grad_fn=<MseLossBackward0>)\n",
      "1077 tensor(3.8737, grad_fn=<MseLossBackward0>)\n",
      "1078 tensor(11.5626, grad_fn=<MseLossBackward0>)\n",
      "1079 tensor(4.4816, grad_fn=<MseLossBackward0>)\n",
      "1080 tensor(5.5194, grad_fn=<MseLossBackward0>)\n",
      "1081 tensor(5.6624, grad_fn=<MseLossBackward0>)\n",
      "1082 tensor(4.3098, grad_fn=<MseLossBackward0>)\n",
      "1083 tensor(4.7763, grad_fn=<MseLossBackward0>)\n",
      "1084 tensor(5.1410, grad_fn=<MseLossBackward0>)\n",
      "1085 tensor(6.7584, grad_fn=<MseLossBackward0>)\n",
      "1086 tensor(20.3333, grad_fn=<MseLossBackward0>)\n",
      "1087 tensor(5.0766, grad_fn=<MseLossBackward0>)\n",
      "1088 tensor(10.4572, grad_fn=<MseLossBackward0>)\n",
      "1089 tensor(10.5165, grad_fn=<MseLossBackward0>)\n",
      "1090 tensor(6.6757, grad_fn=<MseLossBackward0>)\n",
      "1091 tensor(10.0267, grad_fn=<MseLossBackward0>)\n",
      "1092 tensor(9.5873, grad_fn=<MseLossBackward0>)\n",
      "1093 tensor(5.4189, grad_fn=<MseLossBackward0>)\n",
      "1094 tensor(10.2887, grad_fn=<MseLossBackward0>)\n",
      "1095 tensor(6.3137, grad_fn=<MseLossBackward0>)\n",
      "1096 tensor(6.1311, grad_fn=<MseLossBackward0>)\n",
      "1097 tensor(11.8857, grad_fn=<MseLossBackward0>)\n",
      "1098 tensor(6.6049, grad_fn=<MseLossBackward0>)\n",
      "1099 tensor(4.3522, grad_fn=<MseLossBackward0>)\n",
      "1100 tensor(5.1725, grad_fn=<MseLossBackward0>)\n",
      "1101 tensor(6.7452, grad_fn=<MseLossBackward0>)\n",
      "1102 tensor(9.9145, grad_fn=<MseLossBackward0>)\n",
      "1103 tensor(14.1437, grad_fn=<MseLossBackward0>)\n",
      "1104 tensor(5.7810, grad_fn=<MseLossBackward0>)\n",
      "1105 tensor(13.5573, grad_fn=<MseLossBackward0>)\n",
      "1106 tensor(5.1809, grad_fn=<MseLossBackward0>)\n",
      "1107 tensor(14.9976, grad_fn=<MseLossBackward0>)\n",
      "1108 tensor(18.4655, grad_fn=<MseLossBackward0>)\n",
      "1109 tensor(5.4888, grad_fn=<MseLossBackward0>)\n",
      "1110 tensor(12.6772, grad_fn=<MseLossBackward0>)\n",
      "1111 tensor(12.6271, grad_fn=<MseLossBackward0>)\n",
      "1112 tensor(10.0148, grad_fn=<MseLossBackward0>)\n",
      "1113 tensor(5.8085, grad_fn=<MseLossBackward0>)\n",
      "1114 tensor(7.8637, grad_fn=<MseLossBackward0>)\n",
      "1115 tensor(12.7132, grad_fn=<MseLossBackward0>)\n",
      "1116 tensor(11.8740, grad_fn=<MseLossBackward0>)\n",
      "1117 tensor(9.1938, grad_fn=<MseLossBackward0>)\n",
      "1118 tensor(13.7082, grad_fn=<MseLossBackward0>)\n",
      "1119 tensor(5.9924, grad_fn=<MseLossBackward0>)\n",
      "1120 tensor(7.7962, grad_fn=<MseLossBackward0>)\n",
      "1121 tensor(4.4058, grad_fn=<MseLossBackward0>)\n",
      "1122 tensor(13.2990, grad_fn=<MseLossBackward0>)\n",
      "1123 tensor(5.5194, grad_fn=<MseLossBackward0>)\n",
      "1124 tensor(6.2566, grad_fn=<MseLossBackward0>)\n",
      "1125 tensor(4.0255, grad_fn=<MseLossBackward0>)\n",
      "1126 tensor(7.9518, grad_fn=<MseLossBackward0>)\n",
      "1127 tensor(5.5297, grad_fn=<MseLossBackward0>)\n",
      "1128 tensor(5.1352, grad_fn=<MseLossBackward0>)\n",
      "1129 tensor(6.3347, grad_fn=<MseLossBackward0>)\n",
      "1130 tensor(6.9273, grad_fn=<MseLossBackward0>)\n",
      "1131 tensor(4.7133, grad_fn=<MseLossBackward0>)\n",
      "1132 tensor(4.4349, grad_fn=<MseLossBackward0>)\n",
      "1133 tensor(9.0025, grad_fn=<MseLossBackward0>)\n",
      "1134 tensor(3.7538, grad_fn=<MseLossBackward0>)\n",
      "1135 tensor(6.3318, grad_fn=<MseLossBackward0>)\n",
      "1136 tensor(6.3473, grad_fn=<MseLossBackward0>)\n",
      "1137 tensor(4.4803, grad_fn=<MseLossBackward0>)\n",
      "1138 tensor(6.8027, grad_fn=<MseLossBackward0>)\n",
      "1139 tensor(4.5566, grad_fn=<MseLossBackward0>)\n",
      "1140 tensor(5.7707, grad_fn=<MseLossBackward0>)\n",
      "1141 tensor(7.3814, grad_fn=<MseLossBackward0>)\n",
      "1142 tensor(7.2702, grad_fn=<MseLossBackward0>)\n",
      "1143 tensor(6.5374, grad_fn=<MseLossBackward0>)\n",
      "1144 tensor(5.6941, grad_fn=<MseLossBackward0>)\n",
      "1145 tensor(4.2195, grad_fn=<MseLossBackward0>)\n",
      "1146 tensor(3.6534, grad_fn=<MseLossBackward0>)\n",
      "1147 tensor(6.9276, grad_fn=<MseLossBackward0>)\n",
      "1148 tensor(5.5550, grad_fn=<MseLossBackward0>)\n",
      "1149 tensor(5.3988, grad_fn=<MseLossBackward0>)\n",
      "1150 tensor(5.4884, grad_fn=<MseLossBackward0>)\n",
      "1151 tensor(20.1449, grad_fn=<MseLossBackward0>)\n",
      "1152 tensor(6.9489, grad_fn=<MseLossBackward0>)\n",
      "1153 tensor(4.0593, grad_fn=<MseLossBackward0>)\n",
      "1154 tensor(12.4692, grad_fn=<MseLossBackward0>)\n",
      "1155 tensor(12.8175, grad_fn=<MseLossBackward0>)\n",
      "1156 tensor(7.2962, grad_fn=<MseLossBackward0>)\n",
      "1157 tensor(4.9172, grad_fn=<MseLossBackward0>)\n",
      "1158 tensor(5.0229, grad_fn=<MseLossBackward0>)\n",
      "1159 tensor(6.0683, grad_fn=<MseLossBackward0>)\n",
      "1160 tensor(5.5706, grad_fn=<MseLossBackward0>)\n",
      "1161 tensor(6.5260, grad_fn=<MseLossBackward0>)\n",
      "1162 tensor(7.8890, grad_fn=<MseLossBackward0>)\n",
      "1163 tensor(5.4146, grad_fn=<MseLossBackward0>)\n",
      "1164 tensor(4.0302, grad_fn=<MseLossBackward0>)\n",
      "1165 tensor(10.0879, grad_fn=<MseLossBackward0>)\n",
      "1166 tensor(7.2097, grad_fn=<MseLossBackward0>)\n",
      "1167 tensor(3.6616, grad_fn=<MseLossBackward0>)\n",
      "1168 tensor(12.1657, grad_fn=<MseLossBackward0>)\n",
      "1169 tensor(5.2309, grad_fn=<MseLossBackward0>)\n",
      "1170 tensor(16.9426, grad_fn=<MseLossBackward0>)\n",
      "1171 tensor(5.9856, grad_fn=<MseLossBackward0>)\n",
      "1172 tensor(5.6978, grad_fn=<MseLossBackward0>)\n",
      "1173 tensor(5.5814, grad_fn=<MseLossBackward0>)\n",
      "1174 tensor(12.0272, grad_fn=<MseLossBackward0>)\n",
      "1175 tensor(11.0822, grad_fn=<MseLossBackward0>)\n",
      "1176 tensor(4.4411, grad_fn=<MseLossBackward0>)\n",
      "1177 tensor(3.6768, grad_fn=<MseLossBackward0>)\n",
      "1178 tensor(5.9809, grad_fn=<MseLossBackward0>)\n",
      "1179 tensor(5.1469, grad_fn=<MseLossBackward0>)\n",
      "1180 tensor(11.8819, grad_fn=<MseLossBackward0>)\n",
      "1181 tensor(5.5231, grad_fn=<MseLossBackward0>)\n",
      "1182 tensor(7.3554, grad_fn=<MseLossBackward0>)\n",
      "1183 tensor(8.2240, grad_fn=<MseLossBackward0>)\n",
      "1184 tensor(11.4428, grad_fn=<MseLossBackward0>)\n",
      "1185 tensor(11.5911, grad_fn=<MseLossBackward0>)\n",
      "1186 tensor(3.2533, grad_fn=<MseLossBackward0>)\n",
      "1187 tensor(3.7089, grad_fn=<MseLossBackward0>)\n",
      "1188 tensor(16.1142, grad_fn=<MseLossBackward0>)\n",
      "1189 tensor(21.0076, grad_fn=<MseLossBackward0>)\n",
      "1190 tensor(5.5293, grad_fn=<MseLossBackward0>)\n",
      "1191 tensor(9.9943, grad_fn=<MseLossBackward0>)\n",
      "1192 tensor(3.5825, grad_fn=<MseLossBackward0>)\n",
      "1193 tensor(4.7966, grad_fn=<MseLossBackward0>)\n",
      "1194 tensor(5.0223, grad_fn=<MseLossBackward0>)\n",
      "1195 tensor(4.9965, grad_fn=<MseLossBackward0>)\n",
      "1196 tensor(6.2974, grad_fn=<MseLossBackward0>)\n",
      "1197 tensor(9.2536, grad_fn=<MseLossBackward0>)\n",
      "1198 tensor(5.6295, grad_fn=<MseLossBackward0>)\n",
      "1199 tensor(13.9375, grad_fn=<MseLossBackward0>)\n",
      "1200 tensor(12.9869, grad_fn=<MseLossBackward0>)\n",
      "1201 tensor(5.9648, grad_fn=<MseLossBackward0>)\n",
      "1202 tensor(8.4409, grad_fn=<MseLossBackward0>)\n",
      "1203 tensor(13.0418, grad_fn=<MseLossBackward0>)\n",
      "1204 tensor(6.9469, grad_fn=<MseLossBackward0>)\n",
      "1205 tensor(6.1486, grad_fn=<MseLossBackward0>)\n",
      "1206 tensor(3.0856, grad_fn=<MseLossBackward0>)\n",
      "1207 tensor(3.8415, grad_fn=<MseLossBackward0>)\n",
      "1208 tensor(11.5943, grad_fn=<MseLossBackward0>)\n",
      "1209 tensor(11.5790, grad_fn=<MseLossBackward0>)\n",
      "1210 tensor(9.7770, grad_fn=<MseLossBackward0>)\n",
      "1211 tensor(4.1573, grad_fn=<MseLossBackward0>)\n",
      "1212 tensor(3.9244, grad_fn=<MseLossBackward0>)\n",
      "1213 tensor(6.4020, grad_fn=<MseLossBackward0>)\n",
      "1214 tensor(4.7694, grad_fn=<MseLossBackward0>)\n",
      "1215 tensor(5.4692, grad_fn=<MseLossBackward0>)\n",
      "1216 tensor(6.8614, grad_fn=<MseLossBackward0>)\n",
      "1217 tensor(6.6593, grad_fn=<MseLossBackward0>)\n",
      "1218 tensor(3.7194, grad_fn=<MseLossBackward0>)\n",
      "1219 tensor(12.4842, grad_fn=<MseLossBackward0>)\n",
      "1220 tensor(3.5353, grad_fn=<MseLossBackward0>)\n",
      "1221 tensor(4.2673, grad_fn=<MseLossBackward0>)\n",
      "1222 tensor(6.2516, grad_fn=<MseLossBackward0>)\n",
      "1223 tensor(7.3051, grad_fn=<MseLossBackward0>)\n",
      "1224 tensor(12.4125, grad_fn=<MseLossBackward0>)\n",
      "1225 tensor(6.7822, grad_fn=<MseLossBackward0>)\n",
      "1226 tensor(12.9456, grad_fn=<MseLossBackward0>)\n",
      "1227 tensor(3.0943, grad_fn=<MseLossBackward0>)\n",
      "1228 tensor(8.3579, grad_fn=<MseLossBackward0>)\n",
      "1229 tensor(10.9813, grad_fn=<MseLossBackward0>)\n",
      "1230 tensor(4.8715, grad_fn=<MseLossBackward0>)\n",
      "1231 tensor(3.0809, grad_fn=<MseLossBackward0>)\n",
      "1232 tensor(3.8489, grad_fn=<MseLossBackward0>)\n",
      "1233 tensor(5.8641, grad_fn=<MseLossBackward0>)\n",
      "1234 tensor(5.3574, grad_fn=<MseLossBackward0>)\n",
      "1235 tensor(16.5041, grad_fn=<MseLossBackward0>)\n",
      "1236 tensor(8.0255, grad_fn=<MseLossBackward0>)\n",
      "1237 tensor(5.1899, grad_fn=<MseLossBackward0>)\n",
      "1238 tensor(12.5668, grad_fn=<MseLossBackward0>)\n",
      "1239 tensor(10.0231, grad_fn=<MseLossBackward0>)\n",
      "1240 tensor(5.1672, grad_fn=<MseLossBackward0>)\n",
      "1241 tensor(9.3948, grad_fn=<MseLossBackward0>)\n",
      "1242 tensor(3.3495, grad_fn=<MseLossBackward0>)\n",
      "1243 tensor(12.1418, grad_fn=<MseLossBackward0>)\n",
      "1244 tensor(12.4847, grad_fn=<MseLossBackward0>)\n",
      "1245 tensor(5.1182, grad_fn=<MseLossBackward0>)\n",
      "1246 tensor(4.0521, grad_fn=<MseLossBackward0>)\n",
      "1247 tensor(6.7713, grad_fn=<MseLossBackward0>)\n",
      "1248 tensor(13.8543, grad_fn=<MseLossBackward0>)\n",
      "1249 tensor(2.6571, grad_fn=<MseLossBackward0>)\n",
      "1250 tensor(10.6269, grad_fn=<MseLossBackward0>)\n",
      "1251 tensor(3.4018, grad_fn=<MseLossBackward0>)\n",
      "1252 tensor(5.1192, grad_fn=<MseLossBackward0>)\n",
      "1253 tensor(5.6442, grad_fn=<MseLossBackward0>)\n",
      "1254 tensor(4.6610, grad_fn=<MseLossBackward0>)\n",
      "1255 tensor(3.8809, grad_fn=<MseLossBackward0>)\n",
      "1256 tensor(6.7386, grad_fn=<MseLossBackward0>)\n",
      "1257 tensor(11.1030, grad_fn=<MseLossBackward0>)\n",
      "1258 tensor(5.9422, grad_fn=<MseLossBackward0>)\n",
      "1259 tensor(6.0320, grad_fn=<MseLossBackward0>)\n",
      "1260 tensor(4.4861, grad_fn=<MseLossBackward0>)\n",
      "1261 tensor(5.8725, grad_fn=<MseLossBackward0>)\n",
      "1262 tensor(4.4237, grad_fn=<MseLossBackward0>)\n",
      "1263 tensor(3.8686, grad_fn=<MseLossBackward0>)\n",
      "1264 tensor(3.7993, grad_fn=<MseLossBackward0>)\n",
      "1265 tensor(5.4453, grad_fn=<MseLossBackward0>)\n",
      "1266 tensor(4.0964, grad_fn=<MseLossBackward0>)\n",
      "1267 tensor(4.4970, grad_fn=<MseLossBackward0>)\n",
      "1268 tensor(5.6936, grad_fn=<MseLossBackward0>)\n",
      "1269 tensor(11.4280, grad_fn=<MseLossBackward0>)\n",
      "1270 tensor(10.2678, grad_fn=<MseLossBackward0>)\n",
      "1271 tensor(4.4429, grad_fn=<MseLossBackward0>)\n",
      "1272 tensor(12.4848, grad_fn=<MseLossBackward0>)\n",
      "1273 tensor(8.3834, grad_fn=<MseLossBackward0>)\n",
      "1274 tensor(8.3819, grad_fn=<MseLossBackward0>)\n",
      "1275 tensor(4.1610, grad_fn=<MseLossBackward0>)\n",
      "1276 tensor(4.9346, grad_fn=<MseLossBackward0>)\n",
      "1277 tensor(3.9161, grad_fn=<MseLossBackward0>)\n",
      "1278 tensor(8.2349, grad_fn=<MseLossBackward0>)\n",
      "1279 tensor(4.2001, grad_fn=<MseLossBackward0>)\n",
      "1280 tensor(8.9010, grad_fn=<MseLossBackward0>)\n",
      "1281 tensor(6.1017, grad_fn=<MseLossBackward0>)\n",
      "1282 tensor(14.8943, grad_fn=<MseLossBackward0>)\n",
      "1283 tensor(5.1145, grad_fn=<MseLossBackward0>)\n",
      "1284 tensor(16.0292, grad_fn=<MseLossBackward0>)\n",
      "1285 tensor(6.6681, grad_fn=<MseLossBackward0>)\n",
      "1286 tensor(3.5927, grad_fn=<MseLossBackward0>)\n",
      "1287 tensor(5.1678, grad_fn=<MseLossBackward0>)\n",
      "1288 tensor(4.2313, grad_fn=<MseLossBackward0>)\n",
      "1289 tensor(11.9666, grad_fn=<MseLossBackward0>)\n",
      "1290 tensor(5.8338, grad_fn=<MseLossBackward0>)\n",
      "1291 tensor(8.0269, grad_fn=<MseLossBackward0>)\n",
      "1292 tensor(4.6279, grad_fn=<MseLossBackward0>)\n",
      "1293 tensor(8.2716, grad_fn=<MseLossBackward0>)\n",
      "1294 tensor(4.1860, grad_fn=<MseLossBackward0>)\n",
      "1295 tensor(3.0030, grad_fn=<MseLossBackward0>)\n",
      "1296 tensor(5.5735, grad_fn=<MseLossBackward0>)\n",
      "1297 tensor(13.2291, grad_fn=<MseLossBackward0>)\n",
      "1298 tensor(3.3762, grad_fn=<MseLossBackward0>)\n",
      "1299 tensor(5.5165, grad_fn=<MseLossBackward0>)\n",
      "1300 tensor(3.1711, grad_fn=<MseLossBackward0>)\n",
      "1301 tensor(3.9163, grad_fn=<MseLossBackward0>)\n",
      "1302 tensor(4.3033, grad_fn=<MseLossBackward0>)\n",
      "1303 tensor(4.2158, grad_fn=<MseLossBackward0>)\n",
      "1304 tensor(10.1825, grad_fn=<MseLossBackward0>)\n",
      "1305 tensor(6.0206, grad_fn=<MseLossBackward0>)\n",
      "1306 tensor(8.6658, grad_fn=<MseLossBackward0>)\n",
      "1307 tensor(4.1625, grad_fn=<MseLossBackward0>)\n",
      "1308 tensor(10.4952, grad_fn=<MseLossBackward0>)\n",
      "1309 tensor(7.4742, grad_fn=<MseLossBackward0>)\n",
      "1310 tensor(10.5813, grad_fn=<MseLossBackward0>)\n",
      "1311 tensor(2.8491, grad_fn=<MseLossBackward0>)\n",
      "1312 tensor(4.8840, grad_fn=<MseLossBackward0>)\n",
      "1313 tensor(15.0255, grad_fn=<MseLossBackward0>)\n",
      "1314 tensor(3.7023, grad_fn=<MseLossBackward0>)\n",
      "1315 tensor(4.5138, grad_fn=<MseLossBackward0>)\n",
      "1316 tensor(5.0629, grad_fn=<MseLossBackward0>)\n",
      "1317 tensor(6.4013, grad_fn=<MseLossBackward0>)\n",
      "1318 tensor(7.2057, grad_fn=<MseLossBackward0>)\n",
      "1319 tensor(10.4387, grad_fn=<MseLossBackward0>)\n",
      "1320 tensor(3.4763, grad_fn=<MseLossBackward0>)\n",
      "1321 tensor(19.6995, grad_fn=<MseLossBackward0>)\n",
      "1322 tensor(3.1186, grad_fn=<MseLossBackward0>)\n",
      "1323 tensor(8.8881, grad_fn=<MseLossBackward0>)\n",
      "1324 tensor(3.9463, grad_fn=<MseLossBackward0>)\n",
      "1325 tensor(8.1092, grad_fn=<MseLossBackward0>)\n",
      "1326 tensor(4.6423, grad_fn=<MseLossBackward0>)\n",
      "1327 tensor(8.6742, grad_fn=<MseLossBackward0>)\n",
      "1328 tensor(8.5048, grad_fn=<MseLossBackward0>)\n",
      "1329 tensor(10.5354, grad_fn=<MseLossBackward0>)\n",
      "1330 tensor(5.3629, grad_fn=<MseLossBackward0>)\n",
      "1331 tensor(13.8378, grad_fn=<MseLossBackward0>)\n",
      "1332 tensor(10.1547, grad_fn=<MseLossBackward0>)\n",
      "1333 tensor(18.1858, grad_fn=<MseLossBackward0>)\n",
      "1334 tensor(4.9775, grad_fn=<MseLossBackward0>)\n",
      "1335 tensor(5.8838, grad_fn=<MseLossBackward0>)\n",
      "1336 tensor(3.2535, grad_fn=<MseLossBackward0>)\n",
      "1337 tensor(4.2797, grad_fn=<MseLossBackward0>)\n",
      "1338 tensor(7.8217, grad_fn=<MseLossBackward0>)\n",
      "1339 tensor(2.4268, grad_fn=<MseLossBackward0>)\n",
      "1340 tensor(4.4797, grad_fn=<MseLossBackward0>)\n",
      "1341 tensor(3.0205, grad_fn=<MseLossBackward0>)\n",
      "1342 tensor(11.9688, grad_fn=<MseLossBackward0>)\n",
      "1343 tensor(4.3361, grad_fn=<MseLossBackward0>)\n",
      "1344 tensor(5.7468, grad_fn=<MseLossBackward0>)\n",
      "1345 tensor(10.2991, grad_fn=<MseLossBackward0>)\n",
      "1346 tensor(3.6107, grad_fn=<MseLossBackward0>)\n",
      "1347 tensor(3.3704, grad_fn=<MseLossBackward0>)\n",
      "1348 tensor(5.4409, grad_fn=<MseLossBackward0>)\n",
      "1349 tensor(4.3762, grad_fn=<MseLossBackward0>)\n",
      "1350 tensor(3.6981, grad_fn=<MseLossBackward0>)\n",
      "1351 tensor(3.1680, grad_fn=<MseLossBackward0>)\n",
      "1352 tensor(7.7365, grad_fn=<MseLossBackward0>)\n",
      "1353 tensor(4.0205, grad_fn=<MseLossBackward0>)\n",
      "1354 tensor(4.5389, grad_fn=<MseLossBackward0>)\n",
      "1355 tensor(4.8439, grad_fn=<MseLossBackward0>)\n",
      "1356 tensor(5.7045, grad_fn=<MseLossBackward0>)\n",
      "1357 tensor(3.2399, grad_fn=<MseLossBackward0>)\n",
      "1358 tensor(6.1222, grad_fn=<MseLossBackward0>)\n",
      "1359 tensor(14.2186, grad_fn=<MseLossBackward0>)\n",
      "1360 tensor(5.8437, grad_fn=<MseLossBackward0>)\n",
      "1361 tensor(11.1704, grad_fn=<MseLossBackward0>)\n",
      "1362 tensor(4.1747, grad_fn=<MseLossBackward0>)\n",
      "1363 tensor(3.7064, grad_fn=<MseLossBackward0>)\n",
      "1364 tensor(6.2597, grad_fn=<MseLossBackward0>)\n",
      "1365 tensor(5.7894, grad_fn=<MseLossBackward0>)\n",
      "1366 tensor(5.3586, grad_fn=<MseLossBackward0>)\n",
      "1367 tensor(4.0006, grad_fn=<MseLossBackward0>)\n",
      "1368 tensor(4.6024, grad_fn=<MseLossBackward0>)\n",
      "1369 tensor(5.8622, grad_fn=<MseLossBackward0>)\n",
      "1370 tensor(3.5295, grad_fn=<MseLossBackward0>)\n",
      "1371 tensor(10.1415, grad_fn=<MseLossBackward0>)\n",
      "1372 tensor(6.2773, grad_fn=<MseLossBackward0>)\n",
      "1373 tensor(5.3901, grad_fn=<MseLossBackward0>)\n",
      "1374 tensor(5.2979, grad_fn=<MseLossBackward0>)\n",
      "1375 tensor(7.0335, grad_fn=<MseLossBackward0>)\n",
      "1376 tensor(5.1832, grad_fn=<MseLossBackward0>)\n",
      "1377 tensor(3.0903, grad_fn=<MseLossBackward0>)\n",
      "1378 tensor(3.5815, grad_fn=<MseLossBackward0>)\n",
      "1379 tensor(21.1797, grad_fn=<MseLossBackward0>)\n",
      "1380 tensor(4.8024, grad_fn=<MseLossBackward0>)\n",
      "1381 tensor(5.7994, grad_fn=<MseLossBackward0>)\n",
      "1382 tensor(4.2315, grad_fn=<MseLossBackward0>)\n",
      "1383 tensor(16.4530, grad_fn=<MseLossBackward0>)\n",
      "1384 tensor(6.3640, grad_fn=<MseLossBackward0>)\n",
      "1385 tensor(10.5250, grad_fn=<MseLossBackward0>)\n",
      "1386 tensor(4.6337, grad_fn=<MseLossBackward0>)\n",
      "1387 tensor(3.6501, grad_fn=<MseLossBackward0>)\n",
      "1388 tensor(4.2331, grad_fn=<MseLossBackward0>)\n",
      "1389 tensor(9.8683, grad_fn=<MseLossBackward0>)\n",
      "1390 tensor(9.2903, grad_fn=<MseLossBackward0>)\n",
      "1391 tensor(5.8884, grad_fn=<MseLossBackward0>)\n",
      "1392 tensor(5.5903, grad_fn=<MseLossBackward0>)\n",
      "1393 tensor(8.7358, grad_fn=<MseLossBackward0>)\n",
      "1394 tensor(6.3918, grad_fn=<MseLossBackward0>)\n",
      "1395 tensor(5.1910, grad_fn=<MseLossBackward0>)\n",
      "1396 tensor(10.0468, grad_fn=<MseLossBackward0>)\n",
      "1397 tensor(5.1879, grad_fn=<MseLossBackward0>)\n",
      "1398 tensor(10.9719, grad_fn=<MseLossBackward0>)\n",
      "1399 tensor(4.1934, grad_fn=<MseLossBackward0>)\n",
      "1400 tensor(3.2816, grad_fn=<MseLossBackward0>)\n",
      "1401 tensor(8.9775, grad_fn=<MseLossBackward0>)\n",
      "1402 tensor(6.8931, grad_fn=<MseLossBackward0>)\n",
      "1403 tensor(4.2602, grad_fn=<MseLossBackward0>)\n",
      "1404 tensor(7.5646, grad_fn=<MseLossBackward0>)\n",
      "1405 tensor(5.3188, grad_fn=<MseLossBackward0>)\n",
      "1406 tensor(11.5047, grad_fn=<MseLossBackward0>)\n",
      "1407 tensor(2.8381, grad_fn=<MseLossBackward0>)\n",
      "1408 tensor(7.8587, grad_fn=<MseLossBackward0>)\n",
      "1409 tensor(5.0319, grad_fn=<MseLossBackward0>)\n",
      "1410 tensor(6.3137, grad_fn=<MseLossBackward0>)\n",
      "1411 tensor(4.6974, grad_fn=<MseLossBackward0>)\n",
      "1412 tensor(4.7049, grad_fn=<MseLossBackward0>)\n",
      "1413 tensor(6.7537, grad_fn=<MseLossBackward0>)\n",
      "1414 tensor(2.4427, grad_fn=<MseLossBackward0>)\n",
      "1415 tensor(14.1115, grad_fn=<MseLossBackward0>)\n",
      "1416 tensor(5.3408, grad_fn=<MseLossBackward0>)\n",
      "1417 tensor(6.3452, grad_fn=<MseLossBackward0>)\n",
      "1418 tensor(13.1810, grad_fn=<MseLossBackward0>)\n",
      "1419 tensor(6.6319, grad_fn=<MseLossBackward0>)\n",
      "1420 tensor(4.8373, grad_fn=<MseLossBackward0>)\n",
      "1421 tensor(4.1029, grad_fn=<MseLossBackward0>)\n",
      "1422 tensor(6.0691, grad_fn=<MseLossBackward0>)\n",
      "1423 tensor(5.1564, grad_fn=<MseLossBackward0>)\n",
      "1424 tensor(6.7319, grad_fn=<MseLossBackward0>)\n",
      "1425 tensor(3.5668, grad_fn=<MseLossBackward0>)\n",
      "1426 tensor(10.1724, grad_fn=<MseLossBackward0>)\n",
      "1427 tensor(11.2179, grad_fn=<MseLossBackward0>)\n",
      "1428 tensor(3.3094, grad_fn=<MseLossBackward0>)\n",
      "1429 tensor(8.7069, grad_fn=<MseLossBackward0>)\n",
      "1430 tensor(5.9142, grad_fn=<MseLossBackward0>)\n",
      "1431 tensor(12.3907, grad_fn=<MseLossBackward0>)\n",
      "1432 tensor(3.3348, grad_fn=<MseLossBackward0>)\n",
      "1433 tensor(5.9218, grad_fn=<MseLossBackward0>)\n",
      "1434 tensor(3.3809, grad_fn=<MseLossBackward0>)\n",
      "1435 tensor(4.8293, grad_fn=<MseLossBackward0>)\n",
      "1436 tensor(2.8884, grad_fn=<MseLossBackward0>)\n",
      "1437 tensor(4.6518, grad_fn=<MseLossBackward0>)\n",
      "1438 tensor(6.9192, grad_fn=<MseLossBackward0>)\n",
      "1439 tensor(4.3830, grad_fn=<MseLossBackward0>)\n",
      "1440 tensor(3.2169, grad_fn=<MseLossBackward0>)\n",
      "1441 tensor(7.3378, grad_fn=<MseLossBackward0>)\n",
      "1442 tensor(13.3879, grad_fn=<MseLossBackward0>)\n",
      "1443 tensor(9.8829, grad_fn=<MseLossBackward0>)\n",
      "1444 tensor(5.5659, grad_fn=<MseLossBackward0>)\n",
      "1445 tensor(4.3153, grad_fn=<MseLossBackward0>)\n",
      "1446 tensor(4.1090, grad_fn=<MseLossBackward0>)\n",
      "1447 tensor(6.5682, grad_fn=<MseLossBackward0>)\n",
      "1448 tensor(6.5040, grad_fn=<MseLossBackward0>)\n",
      "1449 tensor(7.3707, grad_fn=<MseLossBackward0>)\n",
      "1450 tensor(3.4163, grad_fn=<MseLossBackward0>)\n",
      "1451 tensor(2.6826, grad_fn=<MseLossBackward0>)\n",
      "1452 tensor(6.6673, grad_fn=<MseLossBackward0>)\n",
      "1453 tensor(4.6078, grad_fn=<MseLossBackward0>)\n",
      "1454 tensor(5.7293, grad_fn=<MseLossBackward0>)\n",
      "1455 tensor(5.1857, grad_fn=<MseLossBackward0>)\n",
      "1456 tensor(11.9575, grad_fn=<MseLossBackward0>)\n",
      "1457 tensor(4.5757, grad_fn=<MseLossBackward0>)\n",
      "1458 tensor(2.6204, grad_fn=<MseLossBackward0>)\n",
      "1459 tensor(3.9184, grad_fn=<MseLossBackward0>)\n",
      "1460 tensor(3.4611, grad_fn=<MseLossBackward0>)\n",
      "1461 tensor(6.5409, grad_fn=<MseLossBackward0>)\n",
      "1462 tensor(7.6351, grad_fn=<MseLossBackward0>)\n",
      "1463 tensor(5.2863, grad_fn=<MseLossBackward0>)\n",
      "1464 tensor(1.7535, grad_fn=<MseLossBackward0>)\n",
      "1465 tensor(4.9789, grad_fn=<MseLossBackward0>)\n",
      "1466 tensor(12.5572, grad_fn=<MseLossBackward0>)\n",
      "1467 tensor(9.6539, grad_fn=<MseLossBackward0>)\n",
      "1468 tensor(7.9033, grad_fn=<MseLossBackward0>)\n",
      "1469 tensor(3.4873, grad_fn=<MseLossBackward0>)\n",
      "1470 tensor(4.0972, grad_fn=<MseLossBackward0>)\n",
      "1471 tensor(3.5400, grad_fn=<MseLossBackward0>)\n",
      "1472 tensor(3.7204, grad_fn=<MseLossBackward0>)\n",
      "1473 tensor(12.4674, grad_fn=<MseLossBackward0>)\n",
      "1474 tensor(13.1219, grad_fn=<MseLossBackward0>)\n",
      "1475 tensor(8.4041, grad_fn=<MseLossBackward0>)\n",
      "1476 tensor(5.4155, grad_fn=<MseLossBackward0>)\n",
      "1477 tensor(7.4476, grad_fn=<MseLossBackward0>)\n",
      "1478 tensor(7.3651, grad_fn=<MseLossBackward0>)\n",
      "1479 tensor(3.2820, grad_fn=<MseLossBackward0>)\n",
      "1480 tensor(5.1752, grad_fn=<MseLossBackward0>)\n",
      "1481 tensor(3.7040, grad_fn=<MseLossBackward0>)\n",
      "1482 tensor(5.7701, grad_fn=<MseLossBackward0>)\n",
      "1483 tensor(3.9080, grad_fn=<MseLossBackward0>)\n",
      "1484 tensor(2.8285, grad_fn=<MseLossBackward0>)\n",
      "1485 tensor(5.1736, grad_fn=<MseLossBackward0>)\n",
      "1486 tensor(4.7138, grad_fn=<MseLossBackward0>)\n",
      "1487 tensor(4.8123, grad_fn=<MseLossBackward0>)\n",
      "1488 tensor(4.5434, grad_fn=<MseLossBackward0>)\n",
      "1489 tensor(5.8568, grad_fn=<MseLossBackward0>)\n",
      "1490 tensor(4.3533, grad_fn=<MseLossBackward0>)\n",
      "1491 tensor(4.5490, grad_fn=<MseLossBackward0>)\n",
      "1492 tensor(3.1547, grad_fn=<MseLossBackward0>)\n",
      "1493 tensor(9.8712, grad_fn=<MseLossBackward0>)\n",
      "1494 tensor(2.4446, grad_fn=<MseLossBackward0>)\n",
      "1495 tensor(3.6341, grad_fn=<MseLossBackward0>)\n",
      "1496 tensor(7.6331, grad_fn=<MseLossBackward0>)\n",
      "1497 tensor(15.8762, grad_fn=<MseLossBackward0>)\n",
      "1498 tensor(13.3316, grad_fn=<MseLossBackward0>)\n",
      "1499 tensor(14.9735, grad_fn=<MseLossBackward0>)\n",
      "1500 tensor(5.7180, grad_fn=<MseLossBackward0>)\n",
      "1501 tensor(4.9312, grad_fn=<MseLossBackward0>)\n",
      "1502 tensor(8.5771, grad_fn=<MseLossBackward0>)\n",
      "1503 tensor(3.9993, grad_fn=<MseLossBackward0>)\n",
      "1504 tensor(6.8120, grad_fn=<MseLossBackward0>)\n",
      "1505 tensor(4.5096, grad_fn=<MseLossBackward0>)\n",
      "1506 tensor(3.6692, grad_fn=<MseLossBackward0>)\n",
      "1507 tensor(2.8459, grad_fn=<MseLossBackward0>)\n",
      "1508 tensor(3.4881, grad_fn=<MseLossBackward0>)\n",
      "1509 tensor(7.0241, grad_fn=<MseLossBackward0>)\n",
      "1510 tensor(3.3166, grad_fn=<MseLossBackward0>)\n",
      "1511 tensor(4.3792, grad_fn=<MseLossBackward0>)\n",
      "1512 tensor(5.4263, grad_fn=<MseLossBackward0>)\n",
      "1513 tensor(8.9618, grad_fn=<MseLossBackward0>)\n",
      "1514 tensor(2.7210, grad_fn=<MseLossBackward0>)\n",
      "1515 tensor(4.8783, grad_fn=<MseLossBackward0>)\n",
      "1516 tensor(3.1838, grad_fn=<MseLossBackward0>)\n",
      "1517 tensor(4.9697, grad_fn=<MseLossBackward0>)\n",
      "1518 tensor(4.2388, grad_fn=<MseLossBackward0>)\n",
      "1519 tensor(6.5274, grad_fn=<MseLossBackward0>)\n",
      "1520 tensor(3.8636, grad_fn=<MseLossBackward0>)\n",
      "1521 tensor(6.9901, grad_fn=<MseLossBackward0>)\n",
      "1522 tensor(3.5347, grad_fn=<MseLossBackward0>)\n",
      "1523 tensor(3.5798, grad_fn=<MseLossBackward0>)\n",
      "1524 tensor(8.6947, grad_fn=<MseLossBackward0>)\n",
      "1525 tensor(4.8697, grad_fn=<MseLossBackward0>)\n",
      "1526 tensor(4.8161, grad_fn=<MseLossBackward0>)\n",
      "1527 tensor(9.2766, grad_fn=<MseLossBackward0>)\n",
      "1528 tensor(2.8652, grad_fn=<MseLossBackward0>)\n",
      "1529 tensor(4.9366, grad_fn=<MseLossBackward0>)\n",
      "1530 tensor(3.4997, grad_fn=<MseLossBackward0>)\n",
      "1531 tensor(2.2112, grad_fn=<MseLossBackward0>)\n",
      "1532 tensor(3.7201, grad_fn=<MseLossBackward0>)\n",
      "1533 tensor(6.0081, grad_fn=<MseLossBackward0>)\n",
      "1534 tensor(4.8946, grad_fn=<MseLossBackward0>)\n",
      "1535 tensor(12.4276, grad_fn=<MseLossBackward0>)\n",
      "1536 tensor(2.0674, grad_fn=<MseLossBackward0>)\n",
      "1537 tensor(3.9319, grad_fn=<MseLossBackward0>)\n",
      "1538 tensor(8.4284, grad_fn=<MseLossBackward0>)\n",
      "1539 tensor(3.2714, grad_fn=<MseLossBackward0>)\n",
      "1540 tensor(3.4798, grad_fn=<MseLossBackward0>)\n",
      "1541 tensor(3.6714, grad_fn=<MseLossBackward0>)\n",
      "1542 tensor(3.0246, grad_fn=<MseLossBackward0>)\n",
      "1543 tensor(5.1294, grad_fn=<MseLossBackward0>)\n",
      "1544 tensor(3.4876, grad_fn=<MseLossBackward0>)\n",
      "1545 tensor(2.8523, grad_fn=<MseLossBackward0>)\n",
      "1546 tensor(4.9485, grad_fn=<MseLossBackward0>)\n",
      "1547 tensor(5.3790, grad_fn=<MseLossBackward0>)\n",
      "1548 tensor(3.2091, grad_fn=<MseLossBackward0>)\n",
      "1549 tensor(5.3892, grad_fn=<MseLossBackward0>)\n",
      "1550 tensor(9.7781, grad_fn=<MseLossBackward0>)\n",
      "1551 tensor(8.7377, grad_fn=<MseLossBackward0>)\n",
      "1552 tensor(6.9315, grad_fn=<MseLossBackward0>)\n",
      "1553 tensor(4.8061, grad_fn=<MseLossBackward0>)\n",
      "1554 tensor(3.9446, grad_fn=<MseLossBackward0>)\n",
      "1555 tensor(9.3670, grad_fn=<MseLossBackward0>)\n",
      "1556 tensor(6.9159, grad_fn=<MseLossBackward0>)\n",
      "1557 tensor(5.1924, grad_fn=<MseLossBackward0>)\n",
      "1558 tensor(9.6092, grad_fn=<MseLossBackward0>)\n",
      "1559 tensor(3.8145, grad_fn=<MseLossBackward0>)\n",
      "1560 tensor(9.4972, grad_fn=<MseLossBackward0>)\n",
      "1561 tensor(6.2808, grad_fn=<MseLossBackward0>)\n",
      "1562 tensor(11.9600, grad_fn=<MseLossBackward0>)\n",
      "1563 tensor(3.3621, grad_fn=<MseLossBackward0>)\n",
      "1564 tensor(4.8526, grad_fn=<MseLossBackward0>)\n",
      "1565 tensor(7.6858, grad_fn=<MseLossBackward0>)\n",
      "1566 tensor(5.6019, grad_fn=<MseLossBackward0>)\n",
      "1567 tensor(5.7340, grad_fn=<MseLossBackward0>)\n",
      "1568 tensor(10.2373, grad_fn=<MseLossBackward0>)\n",
      "1569 tensor(4.8401, grad_fn=<MseLossBackward0>)\n",
      "1570 tensor(4.0533, grad_fn=<MseLossBackward0>)\n",
      "1571 tensor(10.2246, grad_fn=<MseLossBackward0>)\n",
      "1572 tensor(8.5562, grad_fn=<MseLossBackward0>)\n",
      "1573 tensor(5.6208, grad_fn=<MseLossBackward0>)\n",
      "1574 tensor(9.9183, grad_fn=<MseLossBackward0>)\n",
      "1575 tensor(9.6765, grad_fn=<MseLossBackward0>)\n",
      "1576 tensor(3.6502, grad_fn=<MseLossBackward0>)\n",
      "1577 tensor(3.0624, grad_fn=<MseLossBackward0>)\n",
      "1578 tensor(4.7251, grad_fn=<MseLossBackward0>)\n",
      "1579 tensor(8.2619, grad_fn=<MseLossBackward0>)\n",
      "1580 tensor(9.9793, grad_fn=<MseLossBackward0>)\n",
      "1581 tensor(3.6168, grad_fn=<MseLossBackward0>)\n",
      "1582 tensor(5.3201, grad_fn=<MseLossBackward0>)\n",
      "1583 tensor(3.4485, grad_fn=<MseLossBackward0>)\n",
      "1584 tensor(2.9232, grad_fn=<MseLossBackward0>)\n",
      "1585 tensor(3.4304, grad_fn=<MseLossBackward0>)\n",
      "1586 tensor(3.0768, grad_fn=<MseLossBackward0>)\n",
      "1587 tensor(3.9167, grad_fn=<MseLossBackward0>)\n",
      "1588 tensor(7.3304, grad_fn=<MseLossBackward0>)\n",
      "1589 tensor(7.1228, grad_fn=<MseLossBackward0>)\n",
      "1590 tensor(3.3403, grad_fn=<MseLossBackward0>)\n",
      "1591 tensor(3.2965, grad_fn=<MseLossBackward0>)\n",
      "1592 tensor(2.4392, grad_fn=<MseLossBackward0>)\n",
      "1593 tensor(7.9282, grad_fn=<MseLossBackward0>)\n",
      "1594 tensor(2.3941, grad_fn=<MseLossBackward0>)\n",
      "1595 tensor(2.9592, grad_fn=<MseLossBackward0>)\n",
      "1596 tensor(4.8312, grad_fn=<MseLossBackward0>)\n",
      "1597 tensor(5.6928, grad_fn=<MseLossBackward0>)\n",
      "1598 tensor(6.2103, grad_fn=<MseLossBackward0>)\n",
      "1599 tensor(6.8972, grad_fn=<MseLossBackward0>)\n",
      "1600 tensor(3.7551, grad_fn=<MseLossBackward0>)\n",
      "1601 tensor(8.8496, grad_fn=<MseLossBackward0>)\n",
      "1602 tensor(2.8330, grad_fn=<MseLossBackward0>)\n",
      "1603 tensor(4.3106, grad_fn=<MseLossBackward0>)\n",
      "1604 tensor(9.2605, grad_fn=<MseLossBackward0>)\n",
      "1605 tensor(2.5734, grad_fn=<MseLossBackward0>)\n",
      "1606 tensor(8.0615, grad_fn=<MseLossBackward0>)\n",
      "1607 tensor(6.5175, grad_fn=<MseLossBackward0>)\n",
      "1608 tensor(2.3594, grad_fn=<MseLossBackward0>)\n",
      "1609 tensor(4.9530, grad_fn=<MseLossBackward0>)\n",
      "1610 tensor(4.3127, grad_fn=<MseLossBackward0>)\n",
      "1611 tensor(5.0548, grad_fn=<MseLossBackward0>)\n",
      "1612 tensor(4.9852, grad_fn=<MseLossBackward0>)\n",
      "1613 tensor(2.8679, grad_fn=<MseLossBackward0>)\n",
      "1614 tensor(3.7707, grad_fn=<MseLossBackward0>)\n",
      "1615 tensor(5.1875, grad_fn=<MseLossBackward0>)\n",
      "1616 tensor(4.0003, grad_fn=<MseLossBackward0>)\n",
      "1617 tensor(8.8916, grad_fn=<MseLossBackward0>)\n",
      "1618 tensor(4.9171, grad_fn=<MseLossBackward0>)\n",
      "1619 tensor(10.1257, grad_fn=<MseLossBackward0>)\n",
      "1620 tensor(6.3029, grad_fn=<MseLossBackward0>)\n",
      "1621 tensor(5.2603, grad_fn=<MseLossBackward0>)\n",
      "1622 tensor(7.5210, grad_fn=<MseLossBackward0>)\n",
      "1623 tensor(3.6341, grad_fn=<MseLossBackward0>)\n",
      "1624 tensor(7.5312, grad_fn=<MseLossBackward0>)\n",
      "1625 tensor(3.3510, grad_fn=<MseLossBackward0>)\n",
      "1626 tensor(3.6524, grad_fn=<MseLossBackward0>)\n",
      "1627 tensor(3.9801, grad_fn=<MseLossBackward0>)\n",
      "1628 tensor(6.1305, grad_fn=<MseLossBackward0>)\n",
      "1629 tensor(2.1088, grad_fn=<MseLossBackward0>)\n",
      "1630 tensor(3.5078, grad_fn=<MseLossBackward0>)\n",
      "1631 tensor(5.5598, grad_fn=<MseLossBackward0>)\n",
      "1632 tensor(4.0815, grad_fn=<MseLossBackward0>)\n",
      "1633 tensor(3.5838, grad_fn=<MseLossBackward0>)\n",
      "1634 tensor(8.9744, grad_fn=<MseLossBackward0>)\n",
      "1635 tensor(8.9671, grad_fn=<MseLossBackward0>)\n",
      "1636 tensor(3.6256, grad_fn=<MseLossBackward0>)\n",
      "1637 tensor(4.3664, grad_fn=<MseLossBackward0>)\n",
      "1638 tensor(4.7148, grad_fn=<MseLossBackward0>)\n",
      "1639 tensor(4.9212, grad_fn=<MseLossBackward0>)\n",
      "1640 tensor(2.9069, grad_fn=<MseLossBackward0>)\n",
      "1641 tensor(3.1113, grad_fn=<MseLossBackward0>)\n",
      "1642 tensor(4.4998, grad_fn=<MseLossBackward0>)\n",
      "1643 tensor(4.0439, grad_fn=<MseLossBackward0>)\n",
      "1644 tensor(10.4776, grad_fn=<MseLossBackward0>)\n",
      "1645 tensor(6.9783, grad_fn=<MseLossBackward0>)\n",
      "1646 tensor(15.1072, grad_fn=<MseLossBackward0>)\n",
      "1647 tensor(3.5040, grad_fn=<MseLossBackward0>)\n",
      "1648 tensor(3.2932, grad_fn=<MseLossBackward0>)\n",
      "1649 tensor(6.9994, grad_fn=<MseLossBackward0>)\n",
      "1650 tensor(3.4551, grad_fn=<MseLossBackward0>)\n",
      "1651 tensor(2.5122, grad_fn=<MseLossBackward0>)\n",
      "1652 tensor(5.0448, grad_fn=<MseLossBackward0>)\n",
      "1653 tensor(3.1857, grad_fn=<MseLossBackward0>)\n",
      "1654 tensor(5.2911, grad_fn=<MseLossBackward0>)\n",
      "1655 tensor(3.1029, grad_fn=<MseLossBackward0>)\n",
      "1656 tensor(6.9499, grad_fn=<MseLossBackward0>)\n",
      "1657 tensor(2.8902, grad_fn=<MseLossBackward0>)\n",
      "1658 tensor(7.9571, grad_fn=<MseLossBackward0>)\n",
      "1659 tensor(3.8018, grad_fn=<MseLossBackward0>)\n",
      "1660 tensor(10.2742, grad_fn=<MseLossBackward0>)\n",
      "1661 tensor(4.9339, grad_fn=<MseLossBackward0>)\n",
      "1662 tensor(10.4294, grad_fn=<MseLossBackward0>)\n",
      "1663 tensor(8.3003, grad_fn=<MseLossBackward0>)\n",
      "1664 tensor(6.0213, grad_fn=<MseLossBackward0>)\n",
      "1665 tensor(10.0655, grad_fn=<MseLossBackward0>)\n",
      "1666 tensor(4.1167, grad_fn=<MseLossBackward0>)\n",
      "1667 tensor(9.3316, grad_fn=<MseLossBackward0>)\n",
      "1668 tensor(5.6294, grad_fn=<MseLossBackward0>)\n",
      "1669 tensor(4.7256, grad_fn=<MseLossBackward0>)\n",
      "1670 tensor(5.6882, grad_fn=<MseLossBackward0>)\n",
      "1671 tensor(7.4585, grad_fn=<MseLossBackward0>)\n",
      "1672 tensor(4.1594, grad_fn=<MseLossBackward0>)\n",
      "1673 tensor(5.0216, grad_fn=<MseLossBackward0>)\n",
      "1674 tensor(2.9144, grad_fn=<MseLossBackward0>)\n",
      "1675 tensor(5.5217, grad_fn=<MseLossBackward0>)\n",
      "1676 tensor(4.1341, grad_fn=<MseLossBackward0>)\n",
      "1677 tensor(4.9885, grad_fn=<MseLossBackward0>)\n",
      "1678 tensor(2.9027, grad_fn=<MseLossBackward0>)\n",
      "1679 tensor(3.8856, grad_fn=<MseLossBackward0>)\n",
      "1680 tensor(2.6075, grad_fn=<MseLossBackward0>)\n",
      "1681 tensor(3.4836, grad_fn=<MseLossBackward0>)\n",
      "1682 tensor(5.4989, grad_fn=<MseLossBackward0>)\n",
      "1683 tensor(6.7934, grad_fn=<MseLossBackward0>)\n",
      "1684 tensor(7.5227, grad_fn=<MseLossBackward0>)\n",
      "1685 tensor(2.6337, grad_fn=<MseLossBackward0>)\n",
      "1686 tensor(10.6715, grad_fn=<MseLossBackward0>)\n",
      "1687 tensor(3.1727, grad_fn=<MseLossBackward0>)\n",
      "1688 tensor(2.3327, grad_fn=<MseLossBackward0>)\n",
      "1689 tensor(3.6862, grad_fn=<MseLossBackward0>)\n",
      "1690 tensor(7.7811, grad_fn=<MseLossBackward0>)\n",
      "1691 tensor(3.4673, grad_fn=<MseLossBackward0>)\n",
      "1692 tensor(4.6368, grad_fn=<MseLossBackward0>)\n",
      "1693 tensor(5.8670, grad_fn=<MseLossBackward0>)\n",
      "1694 tensor(2.6037, grad_fn=<MseLossBackward0>)\n",
      "1695 tensor(4.0169, grad_fn=<MseLossBackward0>)\n",
      "1696 tensor(3.0503, grad_fn=<MseLossBackward0>)\n",
      "1697 tensor(5.5445, grad_fn=<MseLossBackward0>)\n",
      "1698 tensor(4.8772, grad_fn=<MseLossBackward0>)\n",
      "1699 tensor(3.7517, grad_fn=<MseLossBackward0>)\n",
      "1700 tensor(3.8761, grad_fn=<MseLossBackward0>)\n",
      "1701 tensor(4.1462, grad_fn=<MseLossBackward0>)\n",
      "1702 tensor(3.2148, grad_fn=<MseLossBackward0>)\n",
      "1703 tensor(9.0097, grad_fn=<MseLossBackward0>)\n",
      "1704 tensor(5.2721, grad_fn=<MseLossBackward0>)\n",
      "1705 tensor(8.3087, grad_fn=<MseLossBackward0>)\n",
      "1706 tensor(2.6761, grad_fn=<MseLossBackward0>)\n",
      "1707 tensor(8.1893, grad_fn=<MseLossBackward0>)\n",
      "1708 tensor(3.4676, grad_fn=<MseLossBackward0>)\n",
      "1709 tensor(3.6948, grad_fn=<MseLossBackward0>)\n",
      "1710 tensor(2.9945, grad_fn=<MseLossBackward0>)\n",
      "1711 tensor(6.0175, grad_fn=<MseLossBackward0>)\n",
      "1712 tensor(3.1855, grad_fn=<MseLossBackward0>)\n",
      "1713 tensor(6.0184, grad_fn=<MseLossBackward0>)\n",
      "1714 tensor(3.4288, grad_fn=<MseLossBackward0>)\n",
      "1715 tensor(1.7487, grad_fn=<MseLossBackward0>)\n",
      "1716 tensor(9.3443, grad_fn=<MseLossBackward0>)\n",
      "1717 tensor(2.4788, grad_fn=<MseLossBackward0>)\n",
      "1718 tensor(4.6187, grad_fn=<MseLossBackward0>)\n",
      "1719 tensor(4.1048, grad_fn=<MseLossBackward0>)\n",
      "1720 tensor(2.8055, grad_fn=<MseLossBackward0>)\n",
      "1721 tensor(2.7534, grad_fn=<MseLossBackward0>)\n",
      "1722 tensor(2.3662, grad_fn=<MseLossBackward0>)\n",
      "1723 tensor(9.4841, grad_fn=<MseLossBackward0>)\n",
      "1724 tensor(8.3611, grad_fn=<MseLossBackward0>)\n",
      "1725 tensor(4.4551, grad_fn=<MseLossBackward0>)\n",
      "1726 tensor(3.8708, grad_fn=<MseLossBackward0>)\n",
      "1727 tensor(4.4544, grad_fn=<MseLossBackward0>)\n",
      "1728 tensor(10.2216, grad_fn=<MseLossBackward0>)\n",
      "1729 tensor(4.5772, grad_fn=<MseLossBackward0>)\n",
      "1730 tensor(5.5360, grad_fn=<MseLossBackward0>)\n",
      "1731 tensor(1.8565, grad_fn=<MseLossBackward0>)\n",
      "1732 tensor(3.4863, grad_fn=<MseLossBackward0>)\n",
      "1733 tensor(4.0281, grad_fn=<MseLossBackward0>)\n",
      "1734 tensor(2.5266, grad_fn=<MseLossBackward0>)\n",
      "1735 tensor(2.5749, grad_fn=<MseLossBackward0>)\n",
      "1736 tensor(3.9837, grad_fn=<MseLossBackward0>)\n",
      "1737 tensor(3.9694, grad_fn=<MseLossBackward0>)\n",
      "1738 tensor(6.9492, grad_fn=<MseLossBackward0>)\n",
      "1739 tensor(3.9961, grad_fn=<MseLossBackward0>)\n",
      "1740 tensor(2.3955, grad_fn=<MseLossBackward0>)\n",
      "1741 tensor(10.0235, grad_fn=<MseLossBackward0>)\n",
      "1742 tensor(10.2775, grad_fn=<MseLossBackward0>)\n",
      "1743 tensor(4.5723, grad_fn=<MseLossBackward0>)\n",
      "1744 tensor(8.3625, grad_fn=<MseLossBackward0>)\n",
      "1745 tensor(4.0880, grad_fn=<MseLossBackward0>)\n",
      "1746 tensor(5.3531, grad_fn=<MseLossBackward0>)\n",
      "1747 tensor(5.2252, grad_fn=<MseLossBackward0>)\n",
      "1748 tensor(11.2602, grad_fn=<MseLossBackward0>)\n",
      "1749 tensor(2.8416, grad_fn=<MseLossBackward0>)\n",
      "1750 tensor(3.6769, grad_fn=<MseLossBackward0>)\n",
      "1751 tensor(2.6627, grad_fn=<MseLossBackward0>)\n",
      "1752 tensor(3.8171, grad_fn=<MseLossBackward0>)\n",
      "1753 tensor(7.3395, grad_fn=<MseLossBackward0>)\n",
      "1754 tensor(4.9257, grad_fn=<MseLossBackward0>)\n",
      "1755 tensor(3.2287, grad_fn=<MseLossBackward0>)\n",
      "1756 tensor(4.0055, grad_fn=<MseLossBackward0>)\n",
      "1757 tensor(3.1282, grad_fn=<MseLossBackward0>)\n",
      "1758 tensor(9.9302, grad_fn=<MseLossBackward0>)\n",
      "1759 tensor(10.3690, grad_fn=<MseLossBackward0>)\n",
      "1760 tensor(3.3229, grad_fn=<MseLossBackward0>)\n",
      "1761 tensor(4.7903, grad_fn=<MseLossBackward0>)\n",
      "1762 tensor(5.9357, grad_fn=<MseLossBackward0>)\n",
      "1763 tensor(5.7173, grad_fn=<MseLossBackward0>)\n",
      "1764 tensor(2.6902, grad_fn=<MseLossBackward0>)\n",
      "1765 tensor(3.2094, grad_fn=<MseLossBackward0>)\n",
      "1766 tensor(9.4661, grad_fn=<MseLossBackward0>)\n",
      "1767 tensor(4.5669, grad_fn=<MseLossBackward0>)\n",
      "1768 tensor(5.8270, grad_fn=<MseLossBackward0>)\n",
      "1769 tensor(8.7289, grad_fn=<MseLossBackward0>)\n",
      "1770 tensor(4.0457, grad_fn=<MseLossBackward0>)\n",
      "1771 tensor(8.3812, grad_fn=<MseLossBackward0>)\n",
      "1772 tensor(3.1024, grad_fn=<MseLossBackward0>)\n",
      "1773 tensor(5.4932, grad_fn=<MseLossBackward0>)\n",
      "1774 tensor(4.4479, grad_fn=<MseLossBackward0>)\n",
      "1775 tensor(3.8735, grad_fn=<MseLossBackward0>)\n",
      "1776 tensor(2.6527, grad_fn=<MseLossBackward0>)\n",
      "1777 tensor(5.1363, grad_fn=<MseLossBackward0>)\n",
      "1778 tensor(3.9190, grad_fn=<MseLossBackward0>)\n",
      "1779 tensor(3.6389, grad_fn=<MseLossBackward0>)\n",
      "1780 tensor(3.3418, grad_fn=<MseLossBackward0>)\n",
      "1781 tensor(5.8234, grad_fn=<MseLossBackward0>)\n",
      "1782 tensor(7.9057, grad_fn=<MseLossBackward0>)\n",
      "1783 tensor(2.0755, grad_fn=<MseLossBackward0>)\n",
      "1784 tensor(5.8590, grad_fn=<MseLossBackward0>)\n",
      "1785 tensor(2.4259, grad_fn=<MseLossBackward0>)\n",
      "1786 tensor(2.2980, grad_fn=<MseLossBackward0>)\n",
      "1787 tensor(4.4063, grad_fn=<MseLossBackward0>)\n",
      "1788 tensor(7.5258, grad_fn=<MseLossBackward0>)\n",
      "1789 tensor(4.6529, grad_fn=<MseLossBackward0>)\n",
      "1790 tensor(4.4912, grad_fn=<MseLossBackward0>)\n",
      "1791 tensor(8.1729, grad_fn=<MseLossBackward0>)\n",
      "1792 tensor(3.5405, grad_fn=<MseLossBackward0>)\n",
      "1793 tensor(8.6401, grad_fn=<MseLossBackward0>)\n",
      "1794 tensor(3.1726, grad_fn=<MseLossBackward0>)\n",
      "1795 tensor(5.0718, grad_fn=<MseLossBackward0>)\n",
      "1796 tensor(6.8475, grad_fn=<MseLossBackward0>)\n",
      "1797 tensor(7.5306, grad_fn=<MseLossBackward0>)\n",
      "1798 tensor(6.8685, grad_fn=<MseLossBackward0>)\n",
      "1799 tensor(3.6257, grad_fn=<MseLossBackward0>)\n",
      "1800 tensor(2.3977, grad_fn=<MseLossBackward0>)\n",
      "1801 tensor(2.6616, grad_fn=<MseLossBackward0>)\n",
      "1802 tensor(1.7038, grad_fn=<MseLossBackward0>)\n",
      "1803 tensor(2.4708, grad_fn=<MseLossBackward0>)\n",
      "1804 tensor(12.6229, grad_fn=<MseLossBackward0>)\n",
      "1805 tensor(6.8745, grad_fn=<MseLossBackward0>)\n",
      "1806 tensor(2.7751, grad_fn=<MseLossBackward0>)\n",
      "1807 tensor(5.4923, grad_fn=<MseLossBackward0>)\n",
      "1808 tensor(5.1635, grad_fn=<MseLossBackward0>)\n",
      "1809 tensor(5.7290, grad_fn=<MseLossBackward0>)\n",
      "1810 tensor(5.3770, grad_fn=<MseLossBackward0>)\n",
      "1811 tensor(5.7931, grad_fn=<MseLossBackward0>)\n",
      "1812 tensor(4.5773, grad_fn=<MseLossBackward0>)\n",
      "1813 tensor(2.6633, grad_fn=<MseLossBackward0>)\n",
      "1814 tensor(4.2246, grad_fn=<MseLossBackward0>)\n",
      "1815 tensor(7.5522, grad_fn=<MseLossBackward0>)\n",
      "1816 tensor(5.3046, grad_fn=<MseLossBackward0>)\n",
      "1817 tensor(3.8649, grad_fn=<MseLossBackward0>)\n",
      "1818 tensor(2.3224, grad_fn=<MseLossBackward0>)\n",
      "1819 tensor(3.3396, grad_fn=<MseLossBackward0>)\n",
      "1820 tensor(11.2355, grad_fn=<MseLossBackward0>)\n",
      "1821 tensor(2.8296, grad_fn=<MseLossBackward0>)\n",
      "1822 tensor(4.4570, grad_fn=<MseLossBackward0>)\n",
      "1823 tensor(2.1590, grad_fn=<MseLossBackward0>)\n",
      "1824 tensor(3.1841, grad_fn=<MseLossBackward0>)\n",
      "1825 tensor(9.2867, grad_fn=<MseLossBackward0>)\n",
      "1826 tensor(2.5299, grad_fn=<MseLossBackward0>)\n",
      "1827 tensor(8.0868, grad_fn=<MseLossBackward0>)\n",
      "1828 tensor(4.5430, grad_fn=<MseLossBackward0>)\n",
      "1829 tensor(9.5244, grad_fn=<MseLossBackward0>)\n",
      "1830 tensor(6.5812, grad_fn=<MseLossBackward0>)\n",
      "1831 tensor(3.0798, grad_fn=<MseLossBackward0>)\n",
      "1832 tensor(7.6760, grad_fn=<MseLossBackward0>)\n",
      "1833 tensor(6.6997, grad_fn=<MseLossBackward0>)\n",
      "1834 tensor(3.6416, grad_fn=<MseLossBackward0>)\n",
      "1835 tensor(3.5968, grad_fn=<MseLossBackward0>)\n",
      "1836 tensor(4.2171, grad_fn=<MseLossBackward0>)\n",
      "1837 tensor(2.7793, grad_fn=<MseLossBackward0>)\n",
      "1838 tensor(7.8230, grad_fn=<MseLossBackward0>)\n",
      "1839 tensor(4.5166, grad_fn=<MseLossBackward0>)\n",
      "1840 tensor(3.5878, grad_fn=<MseLossBackward0>)\n",
      "1841 tensor(3.5559, grad_fn=<MseLossBackward0>)\n",
      "1842 tensor(1.8641, grad_fn=<MseLossBackward0>)\n",
      "1843 tensor(3.2407, grad_fn=<MseLossBackward0>)\n",
      "1844 tensor(4.0960, grad_fn=<MseLossBackward0>)\n",
      "1845 tensor(6.3954, grad_fn=<MseLossBackward0>)\n",
      "1846 tensor(2.8472, grad_fn=<MseLossBackward0>)\n",
      "1847 tensor(6.9835, grad_fn=<MseLossBackward0>)\n",
      "1848 tensor(3.1181, grad_fn=<MseLossBackward0>)\n",
      "1849 tensor(10.0241, grad_fn=<MseLossBackward0>)\n",
      "1850 tensor(2.0091, grad_fn=<MseLossBackward0>)\n",
      "1851 tensor(2.7661, grad_fn=<MseLossBackward0>)\n",
      "1852 tensor(11.9272, grad_fn=<MseLossBackward0>)\n",
      "1853 tensor(2.8167, grad_fn=<MseLossBackward0>)\n",
      "1854 tensor(4.0177, grad_fn=<MseLossBackward0>)\n",
      "1855 tensor(3.5914, grad_fn=<MseLossBackward0>)\n",
      "1856 tensor(7.3672, grad_fn=<MseLossBackward0>)\n",
      "1857 tensor(5.6794, grad_fn=<MseLossBackward0>)\n",
      "1858 tensor(3.1393, grad_fn=<MseLossBackward0>)\n",
      "1859 tensor(4.4494, grad_fn=<MseLossBackward0>)\n",
      "1860 tensor(2.2679, grad_fn=<MseLossBackward0>)\n",
      "1861 tensor(2.5368, grad_fn=<MseLossBackward0>)\n",
      "1862 tensor(10.6641, grad_fn=<MseLossBackward0>)\n",
      "1863 tensor(5.5766, grad_fn=<MseLossBackward0>)\n",
      "1864 tensor(4.8017, grad_fn=<MseLossBackward0>)\n",
      "1865 tensor(3.2726, grad_fn=<MseLossBackward0>)\n",
      "1866 tensor(2.1104, grad_fn=<MseLossBackward0>)\n",
      "1867 tensor(2.0173, grad_fn=<MseLossBackward0>)\n",
      "1868 tensor(7.8223, grad_fn=<MseLossBackward0>)\n",
      "1869 tensor(4.7789, grad_fn=<MseLossBackward0>)\n",
      "1870 tensor(3.1207, grad_fn=<MseLossBackward0>)\n",
      "1871 tensor(1.6808, grad_fn=<MseLossBackward0>)\n",
      "1872 tensor(9.2992, grad_fn=<MseLossBackward0>)\n",
      "1873 tensor(4.5820, grad_fn=<MseLossBackward0>)\n",
      "1874 tensor(5.7628, grad_fn=<MseLossBackward0>)\n",
      "1875 tensor(7.9235, grad_fn=<MseLossBackward0>)\n",
      "1876 tensor(2.2726, grad_fn=<MseLossBackward0>)\n",
      "1877 tensor(4.8614, grad_fn=<MseLossBackward0>)\n",
      "1878 tensor(8.4146, grad_fn=<MseLossBackward0>)\n",
      "1879 tensor(5.2647, grad_fn=<MseLossBackward0>)\n",
      "1880 tensor(2.6277, grad_fn=<MseLossBackward0>)\n",
      "1881 tensor(3.3521, grad_fn=<MseLossBackward0>)\n",
      "1882 tensor(6.3838, grad_fn=<MseLossBackward0>)\n",
      "1883 tensor(6.7673, grad_fn=<MseLossBackward0>)\n",
      "1884 tensor(2.2752, grad_fn=<MseLossBackward0>)\n",
      "1885 tensor(4.2015, grad_fn=<MseLossBackward0>)\n",
      "1886 tensor(4.5828, grad_fn=<MseLossBackward0>)\n",
      "1887 tensor(2.2718, grad_fn=<MseLossBackward0>)\n",
      "1888 tensor(2.9126, grad_fn=<MseLossBackward0>)\n",
      "1889 tensor(3.1935, grad_fn=<MseLossBackward0>)\n",
      "1890 tensor(2.9832, grad_fn=<MseLossBackward0>)\n",
      "1891 tensor(3.0087, grad_fn=<MseLossBackward0>)\n",
      "1892 tensor(4.1263, grad_fn=<MseLossBackward0>)\n",
      "1893 tensor(6.3085, grad_fn=<MseLossBackward0>)\n",
      "1894 tensor(4.4548, grad_fn=<MseLossBackward0>)\n",
      "1895 tensor(1.7534, grad_fn=<MseLossBackward0>)\n",
      "1896 tensor(3.0816, grad_fn=<MseLossBackward0>)\n",
      "1897 tensor(3.0198, grad_fn=<MseLossBackward0>)\n",
      "1898 tensor(6.4167, grad_fn=<MseLossBackward0>)\n",
      "1899 tensor(3.2029, grad_fn=<MseLossBackward0>)\n",
      "1900 tensor(7.2511, grad_fn=<MseLossBackward0>)\n",
      "1901 tensor(2.5948, grad_fn=<MseLossBackward0>)\n",
      "1902 tensor(4.9995, grad_fn=<MseLossBackward0>)\n",
      "1903 tensor(3.0830, grad_fn=<MseLossBackward0>)\n",
      "1904 tensor(6.7431, grad_fn=<MseLossBackward0>)\n",
      "1905 tensor(7.5507, grad_fn=<MseLossBackward0>)\n",
      "1906 tensor(3.3732, grad_fn=<MseLossBackward0>)\n",
      "1907 tensor(2.6972, grad_fn=<MseLossBackward0>)\n",
      "1908 tensor(5.0644, grad_fn=<MseLossBackward0>)\n",
      "1909 tensor(6.8130, grad_fn=<MseLossBackward0>)\n",
      "1910 tensor(3.0799, grad_fn=<MseLossBackward0>)\n",
      "1911 tensor(6.0186, grad_fn=<MseLossBackward0>)\n",
      "1912 tensor(4.2158, grad_fn=<MseLossBackward0>)\n",
      "1913 tensor(3.2687, grad_fn=<MseLossBackward0>)\n",
      "1914 tensor(2.5304, grad_fn=<MseLossBackward0>)\n",
      "1915 tensor(2.9116, grad_fn=<MseLossBackward0>)\n",
      "1916 tensor(2.4648, grad_fn=<MseLossBackward0>)\n",
      "1917 tensor(2.6030, grad_fn=<MseLossBackward0>)\n",
      "1918 tensor(3.9218, grad_fn=<MseLossBackward0>)\n",
      "1919 tensor(9.0778, grad_fn=<MseLossBackward0>)\n",
      "1920 tensor(3.0795, grad_fn=<MseLossBackward0>)\n",
      "1921 tensor(2.2711, grad_fn=<MseLossBackward0>)\n",
      "1922 tensor(2.6213, grad_fn=<MseLossBackward0>)\n",
      "1923 tensor(2.6739, grad_fn=<MseLossBackward0>)\n",
      "1924 tensor(2.8913, grad_fn=<MseLossBackward0>)\n",
      "1925 tensor(2.3259, grad_fn=<MseLossBackward0>)\n",
      "1926 tensor(2.8372, grad_fn=<MseLossBackward0>)\n",
      "1927 tensor(7.3887, grad_fn=<MseLossBackward0>)\n",
      "1928 tensor(3.1885, grad_fn=<MseLossBackward0>)\n",
      "1929 tensor(2.8488, grad_fn=<MseLossBackward0>)\n",
      "1930 tensor(4.1439, grad_fn=<MseLossBackward0>)\n",
      "1931 tensor(2.4980, grad_fn=<MseLossBackward0>)\n",
      "1932 tensor(2.4925, grad_fn=<MseLossBackward0>)\n",
      "1933 tensor(2.5174, grad_fn=<MseLossBackward0>)\n",
      "1934 tensor(4.0272, grad_fn=<MseLossBackward0>)\n",
      "1935 tensor(4.1346, grad_fn=<MseLossBackward0>)\n",
      "1936 tensor(3.3508, grad_fn=<MseLossBackward0>)\n",
      "1937 tensor(3.9375, grad_fn=<MseLossBackward0>)\n",
      "1938 tensor(3.5351, grad_fn=<MseLossBackward0>)\n",
      "1939 tensor(3.9934, grad_fn=<MseLossBackward0>)\n",
      "1940 tensor(6.8146, grad_fn=<MseLossBackward0>)\n",
      "1941 tensor(2.6236, grad_fn=<MseLossBackward0>)\n",
      "1942 tensor(1.8056, grad_fn=<MseLossBackward0>)\n",
      "1943 tensor(2.8786, grad_fn=<MseLossBackward0>)\n",
      "1944 tensor(2.5566, grad_fn=<MseLossBackward0>)\n",
      "1945 tensor(3.3619, grad_fn=<MseLossBackward0>)\n",
      "1946 tensor(7.2926, grad_fn=<MseLossBackward0>)\n",
      "1947 tensor(3.5378, grad_fn=<MseLossBackward0>)\n",
      "1948 tensor(2.9838, grad_fn=<MseLossBackward0>)\n",
      "1949 tensor(2.9727, grad_fn=<MseLossBackward0>)\n",
      "1950 tensor(3.8451, grad_fn=<MseLossBackward0>)\n",
      "1951 tensor(9.6538, grad_fn=<MseLossBackward0>)\n",
      "1952 tensor(5.5495, grad_fn=<MseLossBackward0>)\n",
      "1953 tensor(3.3622, grad_fn=<MseLossBackward0>)\n",
      "1954 tensor(3.6491, grad_fn=<MseLossBackward0>)\n",
      "1955 tensor(3.4100, grad_fn=<MseLossBackward0>)\n",
      "1956 tensor(8.9007, grad_fn=<MseLossBackward0>)\n",
      "1957 tensor(2.8848, grad_fn=<MseLossBackward0>)\n",
      "1958 tensor(2.9254, grad_fn=<MseLossBackward0>)\n",
      "1959 tensor(4.4046, grad_fn=<MseLossBackward0>)\n",
      "1960 tensor(3.1558, grad_fn=<MseLossBackward0>)\n",
      "1961 tensor(4.9124, grad_fn=<MseLossBackward0>)\n",
      "1962 tensor(4.8594, grad_fn=<MseLossBackward0>)\n",
      "1963 tensor(7.0209, grad_fn=<MseLossBackward0>)\n",
      "1964 tensor(4.9291, grad_fn=<MseLossBackward0>)\n",
      "1965 tensor(4.5585, grad_fn=<MseLossBackward0>)\n",
      "1966 tensor(4.8509, grad_fn=<MseLossBackward0>)\n",
      "1967 tensor(6.6567, grad_fn=<MseLossBackward0>)\n",
      "1968 tensor(4.3133, grad_fn=<MseLossBackward0>)\n",
      "1969 tensor(10.1297, grad_fn=<MseLossBackward0>)\n",
      "1970 tensor(2.3621, grad_fn=<MseLossBackward0>)\n",
      "1971 tensor(4.5587, grad_fn=<MseLossBackward0>)\n",
      "1972 tensor(2.8597, grad_fn=<MseLossBackward0>)\n",
      "1973 tensor(2.6164, grad_fn=<MseLossBackward0>)\n",
      "1974 tensor(3.2217, grad_fn=<MseLossBackward0>)\n",
      "1975 tensor(3.4124, grad_fn=<MseLossBackward0>)\n",
      "1976 tensor(1.5462, grad_fn=<MseLossBackward0>)\n",
      "1977 tensor(3.5911, grad_fn=<MseLossBackward0>)\n",
      "1978 tensor(6.9495, grad_fn=<MseLossBackward0>)\n",
      "1979 tensor(2.7616, grad_fn=<MseLossBackward0>)\n",
      "1980 tensor(2.1568, grad_fn=<MseLossBackward0>)\n",
      "1981 tensor(2.4149, grad_fn=<MseLossBackward0>)\n",
      "1982 tensor(7.2650, grad_fn=<MseLossBackward0>)\n",
      "1983 tensor(4.1083, grad_fn=<MseLossBackward0>)\n",
      "1984 tensor(2.6834, grad_fn=<MseLossBackward0>)\n",
      "1985 tensor(2.9086, grad_fn=<MseLossBackward0>)\n",
      "1986 tensor(3.6386, grad_fn=<MseLossBackward0>)\n",
      "1987 tensor(3.2899, grad_fn=<MseLossBackward0>)\n",
      "1988 tensor(9.7893, grad_fn=<MseLossBackward0>)\n",
      "1989 tensor(4.6725, grad_fn=<MseLossBackward0>)\n",
      "1990 tensor(3.2782, grad_fn=<MseLossBackward0>)\n",
      "1991 tensor(3.3715, grad_fn=<MseLossBackward0>)\n",
      "1992 tensor(3.5498, grad_fn=<MseLossBackward0>)\n",
      "1993 tensor(3.3766, grad_fn=<MseLossBackward0>)\n",
      "1994 tensor(2.7313, grad_fn=<MseLossBackward0>)\n",
      "1995 tensor(3.2589, grad_fn=<MseLossBackward0>)\n",
      "1996 tensor(3.3245, grad_fn=<MseLossBackward0>)\n",
      "1997 tensor(5.5454, grad_fn=<MseLossBackward0>)\n",
      "1998 tensor(4.7722, grad_fn=<MseLossBackward0>)\n",
      "1999 tensor(2.5757, grad_fn=<MseLossBackward0>)\n",
      "2000 tensor(9.4031, grad_fn=<MseLossBackward0>)\n",
      "2001 tensor(5.5032, grad_fn=<MseLossBackward0>)\n",
      "2002 tensor(2.7652, grad_fn=<MseLossBackward0>)\n",
      "2003 tensor(2.1464, grad_fn=<MseLossBackward0>)\n",
      "2004 tensor(4.9586, grad_fn=<MseLossBackward0>)\n",
      "2005 tensor(4.0939, grad_fn=<MseLossBackward0>)\n",
      "2006 tensor(9.0665, grad_fn=<MseLossBackward0>)\n",
      "2007 tensor(1.8831, grad_fn=<MseLossBackward0>)\n",
      "2008 tensor(2.4769, grad_fn=<MseLossBackward0>)\n",
      "2009 tensor(3.8819, grad_fn=<MseLossBackward0>)\n",
      "2010 tensor(4.4251, grad_fn=<MseLossBackward0>)\n",
      "2011 tensor(4.9383, grad_fn=<MseLossBackward0>)\n",
      "2012 tensor(1.8002, grad_fn=<MseLossBackward0>)\n",
      "2013 tensor(2.8513, grad_fn=<MseLossBackward0>)\n",
      "2014 tensor(4.2149, grad_fn=<MseLossBackward0>)\n",
      "2015 tensor(5.5765, grad_fn=<MseLossBackward0>)\n",
      "2016 tensor(8.4621, grad_fn=<MseLossBackward0>)\n",
      "2017 tensor(5.6437, grad_fn=<MseLossBackward0>)\n",
      "2018 tensor(2.1409, grad_fn=<MseLossBackward0>)\n",
      "2019 tensor(5.3253, grad_fn=<MseLossBackward0>)\n",
      "2020 tensor(7.1717, grad_fn=<MseLossBackward0>)\n",
      "2021 tensor(4.7643, grad_fn=<MseLossBackward0>)\n",
      "2022 tensor(3.0453, grad_fn=<MseLossBackward0>)\n",
      "2023 tensor(2.7626, grad_fn=<MseLossBackward0>)\n",
      "2024 tensor(4.7215, grad_fn=<MseLossBackward0>)\n",
      "2025 tensor(8.6535, grad_fn=<MseLossBackward0>)\n",
      "2026 tensor(3.5949, grad_fn=<MseLossBackward0>)\n",
      "2027 tensor(6.0078, grad_fn=<MseLossBackward0>)\n",
      "2028 tensor(6.4636, grad_fn=<MseLossBackward0>)\n",
      "2029 tensor(1.9022, grad_fn=<MseLossBackward0>)\n",
      "2030 tensor(3.9514, grad_fn=<MseLossBackward0>)\n",
      "2031 tensor(8.3484, grad_fn=<MseLossBackward0>)\n",
      "2032 tensor(2.6900, grad_fn=<MseLossBackward0>)\n",
      "2033 tensor(3.7783, grad_fn=<MseLossBackward0>)\n",
      "2034 tensor(3.3034, grad_fn=<MseLossBackward0>)\n",
      "2035 tensor(3.8673, grad_fn=<MseLossBackward0>)\n",
      "2036 tensor(2.0198, grad_fn=<MseLossBackward0>)\n",
      "2037 tensor(2.7701, grad_fn=<MseLossBackward0>)\n",
      "2038 tensor(4.4457, grad_fn=<MseLossBackward0>)\n",
      "2039 tensor(3.4214, grad_fn=<MseLossBackward0>)\n",
      "2040 tensor(8.0329, grad_fn=<MseLossBackward0>)\n",
      "2041 tensor(9.9116, grad_fn=<MseLossBackward0>)\n",
      "2042 tensor(3.9213, grad_fn=<MseLossBackward0>)\n",
      "2043 tensor(2.0795, grad_fn=<MseLossBackward0>)\n",
      "2044 tensor(3.5914, grad_fn=<MseLossBackward0>)\n",
      "2045 tensor(2.9369, grad_fn=<MseLossBackward0>)\n",
      "2046 tensor(3.3867, grad_fn=<MseLossBackward0>)\n",
      "2047 tensor(4.0044, grad_fn=<MseLossBackward0>)\n",
      "2048 tensor(2.1795, grad_fn=<MseLossBackward0>)\n",
      "2049 tensor(5.1405, grad_fn=<MseLossBackward0>)\n",
      "2050 tensor(2.2213, grad_fn=<MseLossBackward0>)\n",
      "2051 tensor(4.8573, grad_fn=<MseLossBackward0>)\n",
      "2052 tensor(3.6384, grad_fn=<MseLossBackward0>)\n",
      "2053 tensor(7.0297, grad_fn=<MseLossBackward0>)\n",
      "2054 tensor(6.7258, grad_fn=<MseLossBackward0>)\n",
      "2055 tensor(2.3830, grad_fn=<MseLossBackward0>)\n",
      "2056 tensor(5.5409, grad_fn=<MseLossBackward0>)\n",
      "2057 tensor(2.1534, grad_fn=<MseLossBackward0>)\n",
      "2058 tensor(2.7534, grad_fn=<MseLossBackward0>)\n",
      "2059 tensor(3.4596, grad_fn=<MseLossBackward0>)\n",
      "2060 tensor(2.9286, grad_fn=<MseLossBackward0>)\n",
      "2061 tensor(2.3902, grad_fn=<MseLossBackward0>)\n",
      "2062 tensor(2.8907, grad_fn=<MseLossBackward0>)\n",
      "2063 tensor(2.8423, grad_fn=<MseLossBackward0>)\n",
      "2064 tensor(6.9777, grad_fn=<MseLossBackward0>)\n",
      "2065 tensor(2.3497, grad_fn=<MseLossBackward0>)\n",
      "2066 tensor(2.6151, grad_fn=<MseLossBackward0>)\n",
      "2067 tensor(2.1827, grad_fn=<MseLossBackward0>)\n",
      "2068 tensor(3.5569, grad_fn=<MseLossBackward0>)\n",
      "2069 tensor(3.9929, grad_fn=<MseLossBackward0>)\n",
      "2070 tensor(6.9788, grad_fn=<MseLossBackward0>)\n",
      "2071 tensor(5.7396, grad_fn=<MseLossBackward0>)\n",
      "2072 tensor(2.6829, grad_fn=<MseLossBackward0>)\n",
      "2073 tensor(2.3378, grad_fn=<MseLossBackward0>)\n",
      "2074 tensor(3.2439, grad_fn=<MseLossBackward0>)\n",
      "2075 tensor(3.5512, grad_fn=<MseLossBackward0>)\n",
      "2076 tensor(6.0922, grad_fn=<MseLossBackward0>)\n",
      "2077 tensor(4.4175, grad_fn=<MseLossBackward0>)\n",
      "2078 tensor(3.8350, grad_fn=<MseLossBackward0>)\n",
      "2079 tensor(3.8261, grad_fn=<MseLossBackward0>)\n",
      "2080 tensor(6.0402, grad_fn=<MseLossBackward0>)\n",
      "2081 tensor(1.9570, grad_fn=<MseLossBackward0>)\n",
      "2082 tensor(6.2385, grad_fn=<MseLossBackward0>)\n",
      "2083 tensor(2.7409, grad_fn=<MseLossBackward0>)\n",
      "2084 tensor(4.9878, grad_fn=<MseLossBackward0>)\n",
      "2085 tensor(4.5282, grad_fn=<MseLossBackward0>)\n",
      "2086 tensor(1.9417, grad_fn=<MseLossBackward0>)\n",
      "2087 tensor(6.5081, grad_fn=<MseLossBackward0>)\n",
      "2088 tensor(5.9034, grad_fn=<MseLossBackward0>)\n",
      "2089 tensor(1.9276, grad_fn=<MseLossBackward0>)\n",
      "2090 tensor(2.3437, grad_fn=<MseLossBackward0>)\n",
      "2091 tensor(2.0238, grad_fn=<MseLossBackward0>)\n",
      "2092 tensor(2.2811, grad_fn=<MseLossBackward0>)\n",
      "2093 tensor(2.5085, grad_fn=<MseLossBackward0>)\n",
      "2094 tensor(2.1298, grad_fn=<MseLossBackward0>)\n",
      "2095 tensor(7.0068, grad_fn=<MseLossBackward0>)\n",
      "2096 tensor(3.0804, grad_fn=<MseLossBackward0>)\n",
      "2097 tensor(2.5607, grad_fn=<MseLossBackward0>)\n",
      "2098 tensor(3.7131, grad_fn=<MseLossBackward0>)\n",
      "2099 tensor(1.8142, grad_fn=<MseLossBackward0>)\n",
      "2100 tensor(3.3871, grad_fn=<MseLossBackward0>)\n",
      "2101 tensor(2.4375, grad_fn=<MseLossBackward0>)\n",
      "2102 tensor(1.9729, grad_fn=<MseLossBackward0>)\n",
      "2103 tensor(6.0225, grad_fn=<MseLossBackward0>)\n",
      "2104 tensor(2.5321, grad_fn=<MseLossBackward0>)\n",
      "2105 tensor(5.6230, grad_fn=<MseLossBackward0>)\n",
      "2106 tensor(1.6459, grad_fn=<MseLossBackward0>)\n",
      "2107 tensor(4.9762, grad_fn=<MseLossBackward0>)\n",
      "2108 tensor(3.6738, grad_fn=<MseLossBackward0>)\n",
      "2109 tensor(2.2865, grad_fn=<MseLossBackward0>)\n",
      "2110 tensor(6.6028, grad_fn=<MseLossBackward0>)\n",
      "2111 tensor(6.3474, grad_fn=<MseLossBackward0>)\n",
      "2112 tensor(4.2032, grad_fn=<MseLossBackward0>)\n",
      "2113 tensor(3.4406, grad_fn=<MseLossBackward0>)\n",
      "2114 tensor(2.2444, grad_fn=<MseLossBackward0>)\n",
      "2115 tensor(4.7226, grad_fn=<MseLossBackward0>)\n",
      "2116 tensor(5.4347, grad_fn=<MseLossBackward0>)\n",
      "2117 tensor(1.7334, grad_fn=<MseLossBackward0>)\n",
      "2118 tensor(1.8083, grad_fn=<MseLossBackward0>)\n",
      "2119 tensor(3.8108, grad_fn=<MseLossBackward0>)\n",
      "2120 tensor(3.3585, grad_fn=<MseLossBackward0>)\n",
      "2121 tensor(1.6768, grad_fn=<MseLossBackward0>)\n",
      "2122 tensor(5.9287, grad_fn=<MseLossBackward0>)\n",
      "2123 tensor(5.2913, grad_fn=<MseLossBackward0>)\n",
      "2124 tensor(6.3341, grad_fn=<MseLossBackward0>)\n",
      "2125 tensor(2.9171, grad_fn=<MseLossBackward0>)\n",
      "2126 tensor(2.4409, grad_fn=<MseLossBackward0>)\n",
      "2127 tensor(2.2586, grad_fn=<MseLossBackward0>)\n",
      "2128 tensor(2.2682, grad_fn=<MseLossBackward0>)\n",
      "2129 tensor(5.4736, grad_fn=<MseLossBackward0>)\n",
      "2130 tensor(5.7648, grad_fn=<MseLossBackward0>)\n",
      "2131 tensor(2.5490, grad_fn=<MseLossBackward0>)\n",
      "2132 tensor(6.8118, grad_fn=<MseLossBackward0>)\n",
      "2133 tensor(1.7545, grad_fn=<MseLossBackward0>)\n",
      "2134 tensor(4.2855, grad_fn=<MseLossBackward0>)\n",
      "2135 tensor(6.4416, grad_fn=<MseLossBackward0>)\n",
      "2136 tensor(1.4797, grad_fn=<MseLossBackward0>)\n",
      "2137 tensor(2.7797, grad_fn=<MseLossBackward0>)\n",
      "2138 tensor(2.4725, grad_fn=<MseLossBackward0>)\n",
      "2139 tensor(3.7988, grad_fn=<MseLossBackward0>)\n",
      "2140 tensor(2.0450, grad_fn=<MseLossBackward0>)\n",
      "2141 tensor(2.9066, grad_fn=<MseLossBackward0>)\n",
      "2142 tensor(2.6455, grad_fn=<MseLossBackward0>)\n",
      "2143 tensor(2.5976, grad_fn=<MseLossBackward0>)\n",
      "2144 tensor(6.5378, grad_fn=<MseLossBackward0>)\n",
      "2145 tensor(1.4057, grad_fn=<MseLossBackward0>)\n",
      "2146 tensor(1.4391, grad_fn=<MseLossBackward0>)\n",
      "2147 tensor(5.9912, grad_fn=<MseLossBackward0>)\n",
      "2148 tensor(1.9343, grad_fn=<MseLossBackward0>)\n",
      "2149 tensor(3.0975, grad_fn=<MseLossBackward0>)\n",
      "2150 tensor(3.1817, grad_fn=<MseLossBackward0>)\n",
      "2151 tensor(3.9713, grad_fn=<MseLossBackward0>)\n",
      "2152 tensor(3.5689, grad_fn=<MseLossBackward0>)\n",
      "2153 tensor(2.3745, grad_fn=<MseLossBackward0>)\n",
      "2154 tensor(3.5493, grad_fn=<MseLossBackward0>)\n",
      "2155 tensor(3.3635, grad_fn=<MseLossBackward0>)\n",
      "2156 tensor(4.0952, grad_fn=<MseLossBackward0>)\n",
      "2157 tensor(2.5994, grad_fn=<MseLossBackward0>)\n",
      "2158 tensor(1.8499, grad_fn=<MseLossBackward0>)\n",
      "2159 tensor(5.1805, grad_fn=<MseLossBackward0>)\n",
      "2160 tensor(9.2210, grad_fn=<MseLossBackward0>)\n",
      "2161 tensor(2.7338, grad_fn=<MseLossBackward0>)\n",
      "2162 tensor(4.3516, grad_fn=<MseLossBackward0>)\n",
      "2163 tensor(3.0480, grad_fn=<MseLossBackward0>)\n",
      "2164 tensor(2.1907, grad_fn=<MseLossBackward0>)\n",
      "2165 tensor(6.9199, grad_fn=<MseLossBackward0>)\n",
      "2166 tensor(3.1332, grad_fn=<MseLossBackward0>)\n",
      "2167 tensor(3.0899, grad_fn=<MseLossBackward0>)\n",
      "2168 tensor(2.1748, grad_fn=<MseLossBackward0>)\n",
      "2169 tensor(2.9980, grad_fn=<MseLossBackward0>)\n",
      "2170 tensor(3.2422, grad_fn=<MseLossBackward0>)\n",
      "2171 tensor(3.5923, grad_fn=<MseLossBackward0>)\n",
      "2172 tensor(2.0436, grad_fn=<MseLossBackward0>)\n",
      "2173 tensor(3.8742, grad_fn=<MseLossBackward0>)\n",
      "2174 tensor(4.8093, grad_fn=<MseLossBackward0>)\n",
      "2175 tensor(3.6365, grad_fn=<MseLossBackward0>)\n",
      "2176 tensor(2.9160, grad_fn=<MseLossBackward0>)\n",
      "2177 tensor(6.3170, grad_fn=<MseLossBackward0>)\n",
      "2178 tensor(7.9261, grad_fn=<MseLossBackward0>)\n",
      "2179 tensor(1.8965, grad_fn=<MseLossBackward0>)\n",
      "2180 tensor(3.4570, grad_fn=<MseLossBackward0>)\n",
      "2181 tensor(1.4797, grad_fn=<MseLossBackward0>)\n",
      "2182 tensor(4.5025, grad_fn=<MseLossBackward0>)\n",
      "2183 tensor(3.7764, grad_fn=<MseLossBackward0>)\n",
      "2184 tensor(3.1353, grad_fn=<MseLossBackward0>)\n",
      "2185 tensor(3.3517, grad_fn=<MseLossBackward0>)\n",
      "2186 tensor(1.9643, grad_fn=<MseLossBackward0>)\n",
      "2187 tensor(2.8714, grad_fn=<MseLossBackward0>)\n",
      "2188 tensor(3.8123, grad_fn=<MseLossBackward0>)\n",
      "2189 tensor(4.9084, grad_fn=<MseLossBackward0>)\n",
      "2190 tensor(5.4483, grad_fn=<MseLossBackward0>)\n",
      "2191 tensor(2.7215, grad_fn=<MseLossBackward0>)\n",
      "2192 tensor(4.7445, grad_fn=<MseLossBackward0>)\n",
      "2193 tensor(3.3789, grad_fn=<MseLossBackward0>)\n",
      "2194 tensor(2.7404, grad_fn=<MseLossBackward0>)\n",
      "2195 tensor(2.3651, grad_fn=<MseLossBackward0>)\n",
      "2196 tensor(2.5477, grad_fn=<MseLossBackward0>)\n",
      "2197 tensor(8.3014, grad_fn=<MseLossBackward0>)\n",
      "2198 tensor(3.7951, grad_fn=<MseLossBackward0>)\n",
      "2199 tensor(7.0718, grad_fn=<MseLossBackward0>)\n",
      "2200 tensor(1.8583, grad_fn=<MseLossBackward0>)\n",
      "2201 tensor(4.4092, grad_fn=<MseLossBackward0>)\n",
      "2202 tensor(3.3595, grad_fn=<MseLossBackward0>)\n",
      "2203 tensor(2.2334, grad_fn=<MseLossBackward0>)\n",
      "2204 tensor(2.8239, grad_fn=<MseLossBackward0>)\n",
      "2205 tensor(3.1408, grad_fn=<MseLossBackward0>)\n",
      "2206 tensor(3.7013, grad_fn=<MseLossBackward0>)\n",
      "2207 tensor(5.1866, grad_fn=<MseLossBackward0>)\n",
      "2208 tensor(2.3526, grad_fn=<MseLossBackward0>)\n",
      "2209 tensor(6.4895, grad_fn=<MseLossBackward0>)\n",
      "2210 tensor(2.5682, grad_fn=<MseLossBackward0>)\n",
      "2211 tensor(3.5832, grad_fn=<MseLossBackward0>)\n",
      "2212 tensor(2.3319, grad_fn=<MseLossBackward0>)\n",
      "2213 tensor(4.4451, grad_fn=<MseLossBackward0>)\n",
      "2214 tensor(3.3366, grad_fn=<MseLossBackward0>)\n",
      "2215 tensor(8.1012, grad_fn=<MseLossBackward0>)\n",
      "2216 tensor(4.2769, grad_fn=<MseLossBackward0>)\n",
      "2217 tensor(2.4577, grad_fn=<MseLossBackward0>)\n",
      "2218 tensor(8.1261, grad_fn=<MseLossBackward0>)\n",
      "2219 tensor(8.5372, grad_fn=<MseLossBackward0>)\n",
      "2220 tensor(3.3759, grad_fn=<MseLossBackward0>)\n",
      "2221 tensor(2.7468, grad_fn=<MseLossBackward0>)\n",
      "2222 tensor(2.1393, grad_fn=<MseLossBackward0>)\n",
      "2223 tensor(2.2161, grad_fn=<MseLossBackward0>)\n",
      "2224 tensor(2.4946, grad_fn=<MseLossBackward0>)\n",
      "2225 tensor(2.9429, grad_fn=<MseLossBackward0>)\n",
      "2226 tensor(3.4977, grad_fn=<MseLossBackward0>)\n",
      "2227 tensor(2.1571, grad_fn=<MseLossBackward0>)\n",
      "2228 tensor(1.5206, grad_fn=<MseLossBackward0>)\n",
      "2229 tensor(2.7753, grad_fn=<MseLossBackward0>)\n",
      "2230 tensor(3.7570, grad_fn=<MseLossBackward0>)\n",
      "2231 tensor(3.4778, grad_fn=<MseLossBackward0>)\n",
      "2232 tensor(2.5403, grad_fn=<MseLossBackward0>)\n",
      "2233 tensor(3.3563, grad_fn=<MseLossBackward0>)\n",
      "2234 tensor(2.5431, grad_fn=<MseLossBackward0>)\n",
      "2235 tensor(3.8707, grad_fn=<MseLossBackward0>)\n",
      "2236 tensor(2.2041, grad_fn=<MseLossBackward0>)\n",
      "2237 tensor(4.3852, grad_fn=<MseLossBackward0>)\n",
      "2238 tensor(3.7228, grad_fn=<MseLossBackward0>)\n",
      "2239 tensor(1.8749, grad_fn=<MseLossBackward0>)\n",
      "2240 tensor(1.8057, grad_fn=<MseLossBackward0>)\n",
      "2241 tensor(3.8889, grad_fn=<MseLossBackward0>)\n",
      "2242 tensor(4.7315, grad_fn=<MseLossBackward0>)\n",
      "2243 tensor(2.3630, grad_fn=<MseLossBackward0>)\n",
      "2244 tensor(6.5261, grad_fn=<MseLossBackward0>)\n",
      "2245 tensor(5.0003, grad_fn=<MseLossBackward0>)\n",
      "2246 tensor(3.9412, grad_fn=<MseLossBackward0>)\n",
      "2247 tensor(1.4960, grad_fn=<MseLossBackward0>)\n",
      "2248 tensor(2.8386, grad_fn=<MseLossBackward0>)\n",
      "2249 tensor(2.3528, grad_fn=<MseLossBackward0>)\n",
      "2250 tensor(10.7681, grad_fn=<MseLossBackward0>)\n",
      "2251 tensor(4.2352, grad_fn=<MseLossBackward0>)\n",
      "2252 tensor(1.2768, grad_fn=<MseLossBackward0>)\n",
      "2253 tensor(3.2670, grad_fn=<MseLossBackward0>)\n",
      "2254 tensor(2.4564, grad_fn=<MseLossBackward0>)\n",
      "2255 tensor(2.6545, grad_fn=<MseLossBackward0>)\n",
      "2256 tensor(3.5953, grad_fn=<MseLossBackward0>)\n",
      "2257 tensor(1.9383, grad_fn=<MseLossBackward0>)\n",
      "2258 tensor(2.8971, grad_fn=<MseLossBackward0>)\n",
      "2259 tensor(5.2416, grad_fn=<MseLossBackward0>)\n",
      "2260 tensor(3.3987, grad_fn=<MseLossBackward0>)\n",
      "2261 tensor(6.6890, grad_fn=<MseLossBackward0>)\n",
      "2262 tensor(5.0069, grad_fn=<MseLossBackward0>)\n",
      "2263 tensor(2.3184, grad_fn=<MseLossBackward0>)\n",
      "2264 tensor(4.3181, grad_fn=<MseLossBackward0>)\n",
      "2265 tensor(2.6041, grad_fn=<MseLossBackward0>)\n",
      "2266 tensor(4.2916, grad_fn=<MseLossBackward0>)\n",
      "2267 tensor(6.5592, grad_fn=<MseLossBackward0>)\n",
      "2268 tensor(3.6922, grad_fn=<MseLossBackward0>)\n",
      "2269 tensor(3.7812, grad_fn=<MseLossBackward0>)\n",
      "2270 tensor(4.8208, grad_fn=<MseLossBackward0>)\n",
      "2271 tensor(4.4416, grad_fn=<MseLossBackward0>)\n",
      "2272 tensor(2.2479, grad_fn=<MseLossBackward0>)\n",
      "2273 tensor(2.3168, grad_fn=<MseLossBackward0>)\n",
      "2274 tensor(2.3494, grad_fn=<MseLossBackward0>)\n",
      "2275 tensor(8.2420, grad_fn=<MseLossBackward0>)\n",
      "2276 tensor(2.3193, grad_fn=<MseLossBackward0>)\n",
      "2277 tensor(4.7080, grad_fn=<MseLossBackward0>)\n",
      "2278 tensor(3.0270, grad_fn=<MseLossBackward0>)\n",
      "2279 tensor(2.8294, grad_fn=<MseLossBackward0>)\n",
      "2280 tensor(1.6548, grad_fn=<MseLossBackward0>)\n",
      "2281 tensor(1.8609, grad_fn=<MseLossBackward0>)\n",
      "2282 tensor(7.4541, grad_fn=<MseLossBackward0>)\n",
      "2283 tensor(2.1789, grad_fn=<MseLossBackward0>)\n",
      "2284 tensor(2.2136, grad_fn=<MseLossBackward0>)\n",
      "2285 tensor(2.5285, grad_fn=<MseLossBackward0>)\n",
      "2286 tensor(2.0204, grad_fn=<MseLossBackward0>)\n",
      "2287 tensor(2.9648, grad_fn=<MseLossBackward0>)\n",
      "2288 tensor(6.8959, grad_fn=<MseLossBackward0>)\n",
      "2289 tensor(3.5375, grad_fn=<MseLossBackward0>)\n",
      "2290 tensor(1.9067, grad_fn=<MseLossBackward0>)\n",
      "2291 tensor(1.4752, grad_fn=<MseLossBackward0>)\n",
      "2292 tensor(4.3226, grad_fn=<MseLossBackward0>)\n",
      "2293 tensor(3.9364, grad_fn=<MseLossBackward0>)\n",
      "2294 tensor(2.4527, grad_fn=<MseLossBackward0>)\n",
      "2295 tensor(3.3277, grad_fn=<MseLossBackward0>)\n",
      "2296 tensor(2.7585, grad_fn=<MseLossBackward0>)\n",
      "2297 tensor(4.6464, grad_fn=<MseLossBackward0>)\n",
      "2298 tensor(2.0565, grad_fn=<MseLossBackward0>)\n",
      "2299 tensor(6.6505, grad_fn=<MseLossBackward0>)\n",
      "2300 tensor(2.0164, grad_fn=<MseLossBackward0>)\n",
      "2301 tensor(6.3963, grad_fn=<MseLossBackward0>)\n",
      "2302 tensor(4.2529, grad_fn=<MseLossBackward0>)\n",
      "2303 tensor(2.0501, grad_fn=<MseLossBackward0>)\n",
      "2304 tensor(2.1191, grad_fn=<MseLossBackward0>)\n",
      "2305 tensor(2.8754, grad_fn=<MseLossBackward0>)\n",
      "2306 tensor(3.2334, grad_fn=<MseLossBackward0>)\n",
      "2307 tensor(2.2572, grad_fn=<MseLossBackward0>)\n",
      "2308 tensor(2.3550, grad_fn=<MseLossBackward0>)\n",
      "2309 tensor(4.9011, grad_fn=<MseLossBackward0>)\n",
      "2310 tensor(2.4533, grad_fn=<MseLossBackward0>)\n",
      "2311 tensor(2.9793, grad_fn=<MseLossBackward0>)\n",
      "2312 tensor(3.0918, grad_fn=<MseLossBackward0>)\n",
      "2313 tensor(2.5230, grad_fn=<MseLossBackward0>)\n",
      "2314 tensor(2.9191, grad_fn=<MseLossBackward0>)\n",
      "2315 tensor(3.9583, grad_fn=<MseLossBackward0>)\n",
      "2316 tensor(1.7579, grad_fn=<MseLossBackward0>)\n",
      "2317 tensor(5.8975, grad_fn=<MseLossBackward0>)\n",
      "2318 tensor(2.4339, grad_fn=<MseLossBackward0>)\n",
      "2319 tensor(7.7844, grad_fn=<MseLossBackward0>)\n",
      "2320 tensor(5.5034, grad_fn=<MseLossBackward0>)\n",
      "2321 tensor(3.8552, grad_fn=<MseLossBackward0>)\n",
      "2322 tensor(3.8115, grad_fn=<MseLossBackward0>)\n",
      "2323 tensor(4.9704, grad_fn=<MseLossBackward0>)\n",
      "2324 tensor(2.3700, grad_fn=<MseLossBackward0>)\n",
      "2325 tensor(4.1184, grad_fn=<MseLossBackward0>)\n",
      "2326 tensor(4.9572, grad_fn=<MseLossBackward0>)\n",
      "2327 tensor(2.2968, grad_fn=<MseLossBackward0>)\n",
      "2328 tensor(2.9655, grad_fn=<MseLossBackward0>)\n",
      "2329 tensor(3.6393, grad_fn=<MseLossBackward0>)\n",
      "2330 tensor(3.7007, grad_fn=<MseLossBackward0>)\n",
      "2331 tensor(2.8047, grad_fn=<MseLossBackward0>)\n",
      "2332 tensor(2.9145, grad_fn=<MseLossBackward0>)\n",
      "2333 tensor(4.2292, grad_fn=<MseLossBackward0>)\n",
      "2334 tensor(2.5558, grad_fn=<MseLossBackward0>)\n",
      "2335 tensor(2.7996, grad_fn=<MseLossBackward0>)\n",
      "2336 tensor(4.8723, grad_fn=<MseLossBackward0>)\n",
      "2337 tensor(4.2668, grad_fn=<MseLossBackward0>)\n",
      "2338 tensor(1.8263, grad_fn=<MseLossBackward0>)\n",
      "2339 tensor(3.6709, grad_fn=<MseLossBackward0>)\n",
      "2340 tensor(4.8932, grad_fn=<MseLossBackward0>)\n",
      "2341 tensor(5.0383, grad_fn=<MseLossBackward0>)\n",
      "2342 tensor(4.6750, grad_fn=<MseLossBackward0>)\n",
      "2343 tensor(2.4609, grad_fn=<MseLossBackward0>)\n",
      "2344 tensor(2.8568, grad_fn=<MseLossBackward0>)\n",
      "2345 tensor(1.8869, grad_fn=<MseLossBackward0>)\n",
      "2346 tensor(2.9218, grad_fn=<MseLossBackward0>)\n",
      "2347 tensor(2.3019, grad_fn=<MseLossBackward0>)\n",
      "2348 tensor(3.3913, grad_fn=<MseLossBackward0>)\n",
      "2349 tensor(3.2038, grad_fn=<MseLossBackward0>)\n",
      "2350 tensor(2.9842, grad_fn=<MseLossBackward0>)\n",
      "2351 tensor(2.0595, grad_fn=<MseLossBackward0>)\n",
      "2352 tensor(3.7741, grad_fn=<MseLossBackward0>)\n",
      "2353 tensor(2.5150, grad_fn=<MseLossBackward0>)\n",
      "2354 tensor(1.8792, grad_fn=<MseLossBackward0>)\n",
      "2355 tensor(2.7921, grad_fn=<MseLossBackward0>)\n",
      "2356 tensor(2.6017, grad_fn=<MseLossBackward0>)\n",
      "2357 tensor(4.3267, grad_fn=<MseLossBackward0>)\n",
      "2358 tensor(3.4378, grad_fn=<MseLossBackward0>)\n",
      "2359 tensor(1.9271, grad_fn=<MseLossBackward0>)\n",
      "2360 tensor(3.0708, grad_fn=<MseLossBackward0>)\n",
      "2361 tensor(1.8806, grad_fn=<MseLossBackward0>)\n",
      "2362 tensor(2.6125, grad_fn=<MseLossBackward0>)\n",
      "2363 tensor(4.3618, grad_fn=<MseLossBackward0>)\n",
      "2364 tensor(2.1967, grad_fn=<MseLossBackward0>)\n",
      "2365 tensor(2.1826, grad_fn=<MseLossBackward0>)\n",
      "2366 tensor(2.7056, grad_fn=<MseLossBackward0>)\n",
      "2367 tensor(2.8407, grad_fn=<MseLossBackward0>)\n",
      "2368 tensor(2.8515, grad_fn=<MseLossBackward0>)\n",
      "2369 tensor(5.6986, grad_fn=<MseLossBackward0>)\n",
      "2370 tensor(3.9751, grad_fn=<MseLossBackward0>)\n",
      "2371 tensor(5.3674, grad_fn=<MseLossBackward0>)\n",
      "2372 tensor(2.3008, grad_fn=<MseLossBackward0>)\n",
      "2373 tensor(5.1957, grad_fn=<MseLossBackward0>)\n",
      "2374 tensor(1.7683, grad_fn=<MseLossBackward0>)\n",
      "2375 tensor(4.8059, grad_fn=<MseLossBackward0>)\n",
      "2376 tensor(2.0513, grad_fn=<MseLossBackward0>)\n",
      "2377 tensor(2.9437, grad_fn=<MseLossBackward0>)\n",
      "2378 tensor(1.8196, grad_fn=<MseLossBackward0>)\n",
      "2379 tensor(4.6696, grad_fn=<MseLossBackward0>)\n",
      "2380 tensor(4.4704, grad_fn=<MseLossBackward0>)\n",
      "2381 tensor(2.6023, grad_fn=<MseLossBackward0>)\n",
      "2382 tensor(5.7805, grad_fn=<MseLossBackward0>)\n",
      "2383 tensor(4.3281, grad_fn=<MseLossBackward0>)\n",
      "2384 tensor(3.4527, grad_fn=<MseLossBackward0>)\n",
      "2385 tensor(2.7093, grad_fn=<MseLossBackward0>)\n",
      "2386 tensor(3.8057, grad_fn=<MseLossBackward0>)\n",
      "2387 tensor(1.9938, grad_fn=<MseLossBackward0>)\n",
      "2388 tensor(4.4051, grad_fn=<MseLossBackward0>)\n",
      "2389 tensor(3.2733, grad_fn=<MseLossBackward0>)\n",
      "2390 tensor(3.8898, grad_fn=<MseLossBackward0>)\n",
      "2391 tensor(2.4382, grad_fn=<MseLossBackward0>)\n",
      "2392 tensor(3.4583, grad_fn=<MseLossBackward0>)\n",
      "2393 tensor(3.2101, grad_fn=<MseLossBackward0>)\n",
      "2394 tensor(5.7356, grad_fn=<MseLossBackward0>)\n",
      "2395 tensor(2.2661, grad_fn=<MseLossBackward0>)\n",
      "2396 tensor(3.0637, grad_fn=<MseLossBackward0>)\n",
      "2397 tensor(2.3511, grad_fn=<MseLossBackward0>)\n",
      "2398 tensor(4.7132, grad_fn=<MseLossBackward0>)\n",
      "2399 tensor(6.0449, grad_fn=<MseLossBackward0>)\n",
      "2400 tensor(2.0233, grad_fn=<MseLossBackward0>)\n",
      "2401 tensor(2.2657, grad_fn=<MseLossBackward0>)\n",
      "2402 tensor(4.3928, grad_fn=<MseLossBackward0>)\n",
      "2403 tensor(3.3233, grad_fn=<MseLossBackward0>)\n",
      "2404 tensor(1.8269, grad_fn=<MseLossBackward0>)\n",
      "2405 tensor(1.1940, grad_fn=<MseLossBackward0>)\n",
      "2406 tensor(2.0691, grad_fn=<MseLossBackward0>)\n",
      "2407 tensor(7.2062, grad_fn=<MseLossBackward0>)\n",
      "2408 tensor(2.2903, grad_fn=<MseLossBackward0>)\n",
      "2409 tensor(2.2339, grad_fn=<MseLossBackward0>)\n",
      "2410 tensor(1.8665, grad_fn=<MseLossBackward0>)\n",
      "2411 tensor(1.7242, grad_fn=<MseLossBackward0>)\n",
      "2412 tensor(1.5621, grad_fn=<MseLossBackward0>)\n",
      "2413 tensor(5.9324, grad_fn=<MseLossBackward0>)\n",
      "2414 tensor(1.6552, grad_fn=<MseLossBackward0>)\n",
      "2415 tensor(3.0095, grad_fn=<MseLossBackward0>)\n",
      "2416 tensor(3.0448, grad_fn=<MseLossBackward0>)\n",
      "2417 tensor(4.7873, grad_fn=<MseLossBackward0>)\n",
      "2418 tensor(2.1602, grad_fn=<MseLossBackward0>)\n",
      "2419 tensor(3.7888, grad_fn=<MseLossBackward0>)\n",
      "2420 tensor(1.6868, grad_fn=<MseLossBackward0>)\n",
      "2421 tensor(2.4564, grad_fn=<MseLossBackward0>)\n",
      "2422 tensor(7.5498, grad_fn=<MseLossBackward0>)\n",
      "2423 tensor(1.5311, grad_fn=<MseLossBackward0>)\n",
      "2424 tensor(5.5376, grad_fn=<MseLossBackward0>)\n",
      "2425 tensor(2.2576, grad_fn=<MseLossBackward0>)\n",
      "2426 tensor(2.2641, grad_fn=<MseLossBackward0>)\n",
      "2427 tensor(2.1126, grad_fn=<MseLossBackward0>)\n",
      "2428 tensor(4.0462, grad_fn=<MseLossBackward0>)\n",
      "2429 tensor(2.3844, grad_fn=<MseLossBackward0>)\n",
      "2430 tensor(2.5105, grad_fn=<MseLossBackward0>)\n",
      "2431 tensor(2.3728, grad_fn=<MseLossBackward0>)\n",
      "2432 tensor(4.5427, grad_fn=<MseLossBackward0>)\n",
      "2433 tensor(2.2975, grad_fn=<MseLossBackward0>)\n",
      "2434 tensor(1.6992, grad_fn=<MseLossBackward0>)\n",
      "2435 tensor(2.5964, grad_fn=<MseLossBackward0>)\n",
      "2436 tensor(2.8465, grad_fn=<MseLossBackward0>)\n",
      "2437 tensor(4.8106, grad_fn=<MseLossBackward0>)\n",
      "2438 tensor(1.9194, grad_fn=<MseLossBackward0>)\n",
      "2439 tensor(6.0633, grad_fn=<MseLossBackward0>)\n",
      "2440 tensor(0.8921, grad_fn=<MseLossBackward0>)\n",
      "2441 tensor(3.0299, grad_fn=<MseLossBackward0>)\n",
      "2442 tensor(2.0504, grad_fn=<MseLossBackward0>)\n",
      "2443 tensor(1.8308, grad_fn=<MseLossBackward0>)\n",
      "2444 tensor(4.9350, grad_fn=<MseLossBackward0>)\n",
      "2445 tensor(3.4078, grad_fn=<MseLossBackward0>)\n",
      "2446 tensor(2.4634, grad_fn=<MseLossBackward0>)\n",
      "2447 tensor(3.5972, grad_fn=<MseLossBackward0>)\n",
      "2448 tensor(2.1091, grad_fn=<MseLossBackward0>)\n",
      "2449 tensor(4.8106, grad_fn=<MseLossBackward0>)\n",
      "2450 tensor(1.8207, grad_fn=<MseLossBackward0>)\n",
      "2451 tensor(2.1661, grad_fn=<MseLossBackward0>)\n",
      "2452 tensor(2.6208, grad_fn=<MseLossBackward0>)\n",
      "2453 tensor(3.6966, grad_fn=<MseLossBackward0>)\n",
      "2454 tensor(2.9491, grad_fn=<MseLossBackward0>)\n",
      "2455 tensor(2.3581, grad_fn=<MseLossBackward0>)\n",
      "2456 tensor(2.2508, grad_fn=<MseLossBackward0>)\n",
      "2457 tensor(2.3252, grad_fn=<MseLossBackward0>)\n",
      "2458 tensor(1.5786, grad_fn=<MseLossBackward0>)\n",
      "2459 tensor(2.3281, grad_fn=<MseLossBackward0>)\n",
      "2460 tensor(2.6408, grad_fn=<MseLossBackward0>)\n",
      "2461 tensor(2.4165, grad_fn=<MseLossBackward0>)\n",
      "2462 tensor(4.7964, grad_fn=<MseLossBackward0>)\n",
      "2463 tensor(2.3044, grad_fn=<MseLossBackward0>)\n",
      "2464 tensor(4.2484, grad_fn=<MseLossBackward0>)\n",
      "2465 tensor(2.6473, grad_fn=<MseLossBackward0>)\n",
      "2466 tensor(2.2397, grad_fn=<MseLossBackward0>)\n",
      "2467 tensor(3.7414, grad_fn=<MseLossBackward0>)\n",
      "2468 tensor(5.1189, grad_fn=<MseLossBackward0>)\n",
      "2469 tensor(4.2304, grad_fn=<MseLossBackward0>)\n",
      "2470 tensor(1.6256, grad_fn=<MseLossBackward0>)\n",
      "2471 tensor(2.9032, grad_fn=<MseLossBackward0>)\n",
      "2472 tensor(2.9770, grad_fn=<MseLossBackward0>)\n",
      "2473 tensor(2.8692, grad_fn=<MseLossBackward0>)\n",
      "2474 tensor(2.5511, grad_fn=<MseLossBackward0>)\n",
      "2475 tensor(3.6361, grad_fn=<MseLossBackward0>)\n",
      "2476 tensor(1.8428, grad_fn=<MseLossBackward0>)\n",
      "2477 tensor(5.3933, grad_fn=<MseLossBackward0>)\n",
      "2478 tensor(1.9621, grad_fn=<MseLossBackward0>)\n",
      "2479 tensor(2.6165, grad_fn=<MseLossBackward0>)\n",
      "2480 tensor(2.8754, grad_fn=<MseLossBackward0>)\n",
      "2481 tensor(2.5761, grad_fn=<MseLossBackward0>)\n",
      "2482 tensor(2.1985, grad_fn=<MseLossBackward0>)\n",
      "2483 tensor(7.3714, grad_fn=<MseLossBackward0>)\n",
      "2484 tensor(2.3870, grad_fn=<MseLossBackward0>)\n",
      "2485 tensor(3.9010, grad_fn=<MseLossBackward0>)\n",
      "2486 tensor(3.1417, grad_fn=<MseLossBackward0>)\n",
      "2487 tensor(2.2727, grad_fn=<MseLossBackward0>)\n",
      "2488 tensor(1.8584, grad_fn=<MseLossBackward0>)\n",
      "2489 tensor(2.9002, grad_fn=<MseLossBackward0>)\n",
      "2490 tensor(6.0650, grad_fn=<MseLossBackward0>)\n",
      "2491 tensor(1.5979, grad_fn=<MseLossBackward0>)\n",
      "2492 tensor(3.4437, grad_fn=<MseLossBackward0>)\n",
      "2493 tensor(4.2596, grad_fn=<MseLossBackward0>)\n",
      "2494 tensor(2.3326, grad_fn=<MseLossBackward0>)\n",
      "2495 tensor(4.8660, grad_fn=<MseLossBackward0>)\n",
      "2496 tensor(2.7760, grad_fn=<MseLossBackward0>)\n",
      "2497 tensor(2.7867, grad_fn=<MseLossBackward0>)\n",
      "2498 tensor(2.4790, grad_fn=<MseLossBackward0>)\n",
      "2499 tensor(5.4168, grad_fn=<MseLossBackward0>)\n",
      "2500 tensor(2.6637, grad_fn=<MseLossBackward0>)\n",
      "2501 tensor(2.0866, grad_fn=<MseLossBackward0>)\n",
      "2502 tensor(2.6532, grad_fn=<MseLossBackward0>)\n",
      "2503 tensor(3.2302, grad_fn=<MseLossBackward0>)\n",
      "2504 tensor(6.3367, grad_fn=<MseLossBackward0>)\n",
      "2505 tensor(3.1629, grad_fn=<MseLossBackward0>)\n",
      "2506 tensor(1.9411, grad_fn=<MseLossBackward0>)\n",
      "2507 tensor(1.0591, grad_fn=<MseLossBackward0>)\n",
      "2508 tensor(2.1208, grad_fn=<MseLossBackward0>)\n",
      "2509 tensor(3.6288, grad_fn=<MseLossBackward0>)\n",
      "2510 tensor(2.2200, grad_fn=<MseLossBackward0>)\n",
      "2511 tensor(3.2412, grad_fn=<MseLossBackward0>)\n",
      "2512 tensor(1.9139, grad_fn=<MseLossBackward0>)\n",
      "2513 tensor(2.2115, grad_fn=<MseLossBackward0>)\n",
      "2514 tensor(1.9437, grad_fn=<MseLossBackward0>)\n",
      "2515 tensor(3.9890, grad_fn=<MseLossBackward0>)\n",
      "2516 tensor(3.6322, grad_fn=<MseLossBackward0>)\n",
      "2517 tensor(2.2910, grad_fn=<MseLossBackward0>)\n",
      "2518 tensor(5.6975, grad_fn=<MseLossBackward0>)\n",
      "2519 tensor(3.2918, grad_fn=<MseLossBackward0>)\n",
      "2520 tensor(3.0416, grad_fn=<MseLossBackward0>)\n",
      "2521 tensor(5.1076, grad_fn=<MseLossBackward0>)\n",
      "2522 tensor(2.8764, grad_fn=<MseLossBackward0>)\n",
      "2523 tensor(2.5780, grad_fn=<MseLossBackward0>)\n",
      "2524 tensor(4.0185, grad_fn=<MseLossBackward0>)\n",
      "2525 tensor(2.0389, grad_fn=<MseLossBackward0>)\n",
      "2526 tensor(1.5277, grad_fn=<MseLossBackward0>)\n",
      "2527 tensor(2.8881, grad_fn=<MseLossBackward0>)\n",
      "2528 tensor(2.3440, grad_fn=<MseLossBackward0>)\n",
      "2529 tensor(1.6935, grad_fn=<MseLossBackward0>)\n",
      "2530 tensor(2.0887, grad_fn=<MseLossBackward0>)\n",
      "2531 tensor(2.8407, grad_fn=<MseLossBackward0>)\n",
      "2532 tensor(5.4567, grad_fn=<MseLossBackward0>)\n",
      "2533 tensor(1.6304, grad_fn=<MseLossBackward0>)\n",
      "2534 tensor(2.3482, grad_fn=<MseLossBackward0>)\n",
      "2535 tensor(4.3039, grad_fn=<MseLossBackward0>)\n",
      "2536 tensor(3.8082, grad_fn=<MseLossBackward0>)\n",
      "2537 tensor(2.4098, grad_fn=<MseLossBackward0>)\n",
      "2538 tensor(1.4398, grad_fn=<MseLossBackward0>)\n",
      "2539 tensor(4.3177, grad_fn=<MseLossBackward0>)\n",
      "2540 tensor(2.3454, grad_fn=<MseLossBackward0>)\n",
      "2541 tensor(3.0377, grad_fn=<MseLossBackward0>)\n",
      "2542 tensor(3.4829, grad_fn=<MseLossBackward0>)\n",
      "2543 tensor(2.9898, grad_fn=<MseLossBackward0>)\n",
      "2544 tensor(2.5005, grad_fn=<MseLossBackward0>)\n",
      "2545 tensor(5.9970, grad_fn=<MseLossBackward0>)\n",
      "2546 tensor(6.6656, grad_fn=<MseLossBackward0>)\n",
      "2547 tensor(1.4382, grad_fn=<MseLossBackward0>)\n",
      "2548 tensor(1.4439, grad_fn=<MseLossBackward0>)\n",
      "2549 tensor(1.3829, grad_fn=<MseLossBackward0>)\n",
      "2550 tensor(3.8303, grad_fn=<MseLossBackward0>)\n",
      "2551 tensor(2.7198, grad_fn=<MseLossBackward0>)\n",
      "2552 tensor(3.0636, grad_fn=<MseLossBackward0>)\n",
      "2553 tensor(2.0390, grad_fn=<MseLossBackward0>)\n",
      "2554 tensor(2.0118, grad_fn=<MseLossBackward0>)\n",
      "2555 tensor(3.1148, grad_fn=<MseLossBackward0>)\n",
      "2556 tensor(1.8094, grad_fn=<MseLossBackward0>)\n",
      "2557 tensor(3.1991, grad_fn=<MseLossBackward0>)\n",
      "2558 tensor(1.3270, grad_fn=<MseLossBackward0>)\n",
      "2559 tensor(1.6573, grad_fn=<MseLossBackward0>)\n",
      "2560 tensor(1.8624, grad_fn=<MseLossBackward0>)\n",
      "2561 tensor(1.4017, grad_fn=<MseLossBackward0>)\n",
      "2562 tensor(2.0420, grad_fn=<MseLossBackward0>)\n",
      "2563 tensor(2.7377, grad_fn=<MseLossBackward0>)\n",
      "2564 tensor(4.8049, grad_fn=<MseLossBackward0>)\n",
      "2565 tensor(2.3786, grad_fn=<MseLossBackward0>)\n",
      "2566 tensor(2.3149, grad_fn=<MseLossBackward0>)\n",
      "2567 tensor(2.2433, grad_fn=<MseLossBackward0>)\n",
      "2568 tensor(3.1337, grad_fn=<MseLossBackward0>)\n",
      "2569 tensor(1.7779, grad_fn=<MseLossBackward0>)\n",
      "2570 tensor(2.0782, grad_fn=<MseLossBackward0>)\n",
      "2571 tensor(5.7458, grad_fn=<MseLossBackward0>)\n",
      "2572 tensor(1.7323, grad_fn=<MseLossBackward0>)\n",
      "2573 tensor(4.6740, grad_fn=<MseLossBackward0>)\n",
      "2574 tensor(4.1012, grad_fn=<MseLossBackward0>)\n",
      "2575 tensor(2.7700, grad_fn=<MseLossBackward0>)\n",
      "2576 tensor(4.3396, grad_fn=<MseLossBackward0>)\n",
      "2577 tensor(2.2841, grad_fn=<MseLossBackward0>)\n",
      "2578 tensor(2.2059, grad_fn=<MseLossBackward0>)\n",
      "2579 tensor(2.6941, grad_fn=<MseLossBackward0>)\n",
      "2580 tensor(2.2326, grad_fn=<MseLossBackward0>)\n",
      "2581 tensor(1.8531, grad_fn=<MseLossBackward0>)\n",
      "2582 tensor(2.0527, grad_fn=<MseLossBackward0>)\n",
      "2583 tensor(2.4721, grad_fn=<MseLossBackward0>)\n",
      "2584 tensor(2.7463, grad_fn=<MseLossBackward0>)\n",
      "2585 tensor(4.5278, grad_fn=<MseLossBackward0>)\n",
      "2586 tensor(2.3921, grad_fn=<MseLossBackward0>)\n",
      "2587 tensor(3.3239, grad_fn=<MseLossBackward0>)\n",
      "2588 tensor(1.2824, grad_fn=<MseLossBackward0>)\n",
      "2589 tensor(3.1471, grad_fn=<MseLossBackward0>)\n",
      "2590 tensor(2.6540, grad_fn=<MseLossBackward0>)\n",
      "2591 tensor(2.5315, grad_fn=<MseLossBackward0>)\n",
      "2592 tensor(3.3038, grad_fn=<MseLossBackward0>)\n",
      "2593 tensor(1.4749, grad_fn=<MseLossBackward0>)\n",
      "2594 tensor(2.3943, grad_fn=<MseLossBackward0>)\n",
      "2595 tensor(1.6820, grad_fn=<MseLossBackward0>)\n",
      "2596 tensor(2.0594, grad_fn=<MseLossBackward0>)\n",
      "2597 tensor(3.3247, grad_fn=<MseLossBackward0>)\n",
      "2598 tensor(3.1631, grad_fn=<MseLossBackward0>)\n",
      "2599 tensor(2.6073, grad_fn=<MseLossBackward0>)\n",
      "2600 tensor(5.6687, grad_fn=<MseLossBackward0>)\n",
      "2601 tensor(3.5917, grad_fn=<MseLossBackward0>)\n",
      "2602 tensor(3.8016, grad_fn=<MseLossBackward0>)\n",
      "2603 tensor(5.4096, grad_fn=<MseLossBackward0>)\n",
      "2604 tensor(1.7396, grad_fn=<MseLossBackward0>)\n",
      "2605 tensor(2.2714, grad_fn=<MseLossBackward0>)\n",
      "2606 tensor(1.7131, grad_fn=<MseLossBackward0>)\n",
      "2607 tensor(2.5278, grad_fn=<MseLossBackward0>)\n",
      "2608 tensor(4.3706, grad_fn=<MseLossBackward0>)\n",
      "2609 tensor(1.8971, grad_fn=<MseLossBackward0>)\n",
      "2610 tensor(3.1747, grad_fn=<MseLossBackward0>)\n",
      "2611 tensor(1.6503, grad_fn=<MseLossBackward0>)\n",
      "2612 tensor(1.8410, grad_fn=<MseLossBackward0>)\n",
      "2613 tensor(2.1100, grad_fn=<MseLossBackward0>)\n",
      "2614 tensor(1.5117, grad_fn=<MseLossBackward0>)\n",
      "2615 tensor(1.3156, grad_fn=<MseLossBackward0>)\n",
      "2616 tensor(2.7720, grad_fn=<MseLossBackward0>)\n",
      "2617 tensor(1.8492, grad_fn=<MseLossBackward0>)\n",
      "2618 tensor(2.1944, grad_fn=<MseLossBackward0>)\n",
      "2619 tensor(1.3533, grad_fn=<MseLossBackward0>)\n",
      "2620 tensor(3.3116, grad_fn=<MseLossBackward0>)\n",
      "2621 tensor(2.4309, grad_fn=<MseLossBackward0>)\n",
      "2622 tensor(2.7395, grad_fn=<MseLossBackward0>)\n",
      "2623 tensor(2.3489, grad_fn=<MseLossBackward0>)\n",
      "2624 tensor(2.3177, grad_fn=<MseLossBackward0>)\n",
      "2625 tensor(1.2803, grad_fn=<MseLossBackward0>)\n",
      "2626 tensor(2.2981, grad_fn=<MseLossBackward0>)\n",
      "2627 tensor(2.8524, grad_fn=<MseLossBackward0>)\n",
      "2628 tensor(1.8646, grad_fn=<MseLossBackward0>)\n",
      "2629 tensor(2.0172, grad_fn=<MseLossBackward0>)\n",
      "2630 tensor(1.6835, grad_fn=<MseLossBackward0>)\n",
      "2631 tensor(3.2440, grad_fn=<MseLossBackward0>)\n",
      "2632 tensor(2.8762, grad_fn=<MseLossBackward0>)\n",
      "2633 tensor(4.0334, grad_fn=<MseLossBackward0>)\n",
      "2634 tensor(3.6701, grad_fn=<MseLossBackward0>)\n",
      "2635 tensor(2.3934, grad_fn=<MseLossBackward0>)\n",
      "2636 tensor(2.7740, grad_fn=<MseLossBackward0>)\n",
      "2637 tensor(3.3451, grad_fn=<MseLossBackward0>)\n",
      "2638 tensor(2.1956, grad_fn=<MseLossBackward0>)\n",
      "2639 tensor(3.4336, grad_fn=<MseLossBackward0>)\n",
      "2640 tensor(3.0057, grad_fn=<MseLossBackward0>)\n",
      "2641 tensor(1.5541, grad_fn=<MseLossBackward0>)\n",
      "2642 tensor(1.9325, grad_fn=<MseLossBackward0>)\n",
      "2643 tensor(3.1858, grad_fn=<MseLossBackward0>)\n",
      "2644 tensor(1.7021, grad_fn=<MseLossBackward0>)\n",
      "2645 tensor(2.7022, grad_fn=<MseLossBackward0>)\n",
      "2646 tensor(2.3727, grad_fn=<MseLossBackward0>)\n",
      "2647 tensor(1.0990, grad_fn=<MseLossBackward0>)\n",
      "2648 tensor(1.2209, grad_fn=<MseLossBackward0>)\n",
      "2649 tensor(2.6359, grad_fn=<MseLossBackward0>)\n",
      "2650 tensor(3.8436, grad_fn=<MseLossBackward0>)\n",
      "2651 tensor(1.1723, grad_fn=<MseLossBackward0>)\n",
      "2652 tensor(6.5023, grad_fn=<MseLossBackward0>)\n",
      "2653 tensor(2.3137, grad_fn=<MseLossBackward0>)\n",
      "2654 tensor(1.5388, grad_fn=<MseLossBackward0>)\n",
      "2655 tensor(2.1300, grad_fn=<MseLossBackward0>)\n",
      "2656 tensor(1.4660, grad_fn=<MseLossBackward0>)\n",
      "2657 tensor(2.0562, grad_fn=<MseLossBackward0>)\n",
      "2658 tensor(1.8868, grad_fn=<MseLossBackward0>)\n",
      "2659 tensor(2.6327, grad_fn=<MseLossBackward0>)\n",
      "2660 tensor(3.3177, grad_fn=<MseLossBackward0>)\n",
      "2661 tensor(2.9870, grad_fn=<MseLossBackward0>)\n",
      "2662 tensor(4.6170, grad_fn=<MseLossBackward0>)\n",
      "2663 tensor(2.7111, grad_fn=<MseLossBackward0>)\n",
      "2664 tensor(6.2100, grad_fn=<MseLossBackward0>)\n",
      "2665 tensor(1.2625, grad_fn=<MseLossBackward0>)\n",
      "2666 tensor(2.7125, grad_fn=<MseLossBackward0>)\n",
      "2667 tensor(4.1712, grad_fn=<MseLossBackward0>)\n",
      "2668 tensor(3.7006, grad_fn=<MseLossBackward0>)\n",
      "2669 tensor(1.9490, grad_fn=<MseLossBackward0>)\n",
      "2670 tensor(1.5563, grad_fn=<MseLossBackward0>)\n",
      "2671 tensor(3.2008, grad_fn=<MseLossBackward0>)\n",
      "2672 tensor(1.9675, grad_fn=<MseLossBackward0>)\n",
      "2673 tensor(2.1610, grad_fn=<MseLossBackward0>)\n",
      "2674 tensor(1.3250, grad_fn=<MseLossBackward0>)\n",
      "2675 tensor(1.6420, grad_fn=<MseLossBackward0>)\n",
      "2676 tensor(2.7795, grad_fn=<MseLossBackward0>)\n",
      "2677 tensor(4.4809, grad_fn=<MseLossBackward0>)\n",
      "2678 tensor(3.0573, grad_fn=<MseLossBackward0>)\n",
      "2679 tensor(1.0104, grad_fn=<MseLossBackward0>)\n",
      "2680 tensor(0.9989, grad_fn=<MseLossBackward0>)\n",
      "2681 tensor(4.1563, grad_fn=<MseLossBackward0>)\n",
      "2682 tensor(2.1184, grad_fn=<MseLossBackward0>)\n",
      "2683 tensor(1.2334, grad_fn=<MseLossBackward0>)\n",
      "2684 tensor(4.0727, grad_fn=<MseLossBackward0>)\n",
      "2685 tensor(1.9655, grad_fn=<MseLossBackward0>)\n",
      "2686 tensor(3.6889, grad_fn=<MseLossBackward0>)\n",
      "2687 tensor(2.0562, grad_fn=<MseLossBackward0>)\n",
      "2688 tensor(2.2159, grad_fn=<MseLossBackward0>)\n",
      "2689 tensor(3.4599, grad_fn=<MseLossBackward0>)\n",
      "2690 tensor(1.9459, grad_fn=<MseLossBackward0>)\n",
      "2691 tensor(2.2980, grad_fn=<MseLossBackward0>)\n",
      "2692 tensor(2.0394, grad_fn=<MseLossBackward0>)\n",
      "2693 tensor(5.7802, grad_fn=<MseLossBackward0>)\n",
      "2694 tensor(2.1952, grad_fn=<MseLossBackward0>)\n",
      "2695 tensor(3.3810, grad_fn=<MseLossBackward0>)\n",
      "2696 tensor(2.1295, grad_fn=<MseLossBackward0>)\n",
      "2697 tensor(2.3234, grad_fn=<MseLossBackward0>)\n",
      "2698 tensor(2.7878, grad_fn=<MseLossBackward0>)\n",
      "2699 tensor(0.8585, grad_fn=<MseLossBackward0>)\n",
      "2700 tensor(2.6114, grad_fn=<MseLossBackward0>)\n",
      "2701 tensor(2.4333, grad_fn=<MseLossBackward0>)\n",
      "2702 tensor(3.7784, grad_fn=<MseLossBackward0>)\n",
      "2703 tensor(1.6968, grad_fn=<MseLossBackward0>)\n",
      "2704 tensor(5.7063, grad_fn=<MseLossBackward0>)\n",
      "2705 tensor(1.8261, grad_fn=<MseLossBackward0>)\n",
      "2706 tensor(2.0297, grad_fn=<MseLossBackward0>)\n",
      "2707 tensor(2.0738, grad_fn=<MseLossBackward0>)\n",
      "2708 tensor(2.5042, grad_fn=<MseLossBackward0>)\n",
      "2709 tensor(4.5394, grad_fn=<MseLossBackward0>)\n",
      "2710 tensor(0.8254, grad_fn=<MseLossBackward0>)\n",
      "2711 tensor(1.2546, grad_fn=<MseLossBackward0>)\n",
      "2712 tensor(3.3765, grad_fn=<MseLossBackward0>)\n",
      "2713 tensor(3.1102, grad_fn=<MseLossBackward0>)\n",
      "2714 tensor(1.7549, grad_fn=<MseLossBackward0>)\n",
      "2715 tensor(3.6766, grad_fn=<MseLossBackward0>)\n",
      "2716 tensor(4.0352, grad_fn=<MseLossBackward0>)\n",
      "2717 tensor(1.0613, grad_fn=<MseLossBackward0>)\n",
      "2718 tensor(2.0453, grad_fn=<MseLossBackward0>)\n",
      "2719 tensor(2.4650, grad_fn=<MseLossBackward0>)\n",
      "2720 tensor(1.8016, grad_fn=<MseLossBackward0>)\n",
      "2721 tensor(2.1521, grad_fn=<MseLossBackward0>)\n",
      "2722 tensor(4.2176, grad_fn=<MseLossBackward0>)\n",
      "2723 tensor(2.6429, grad_fn=<MseLossBackward0>)\n",
      "2724 tensor(1.6401, grad_fn=<MseLossBackward0>)\n",
      "2725 tensor(1.9448, grad_fn=<MseLossBackward0>)\n",
      "2726 tensor(1.8665, grad_fn=<MseLossBackward0>)\n",
      "2727 tensor(4.8828, grad_fn=<MseLossBackward0>)\n",
      "2728 tensor(4.1176, grad_fn=<MseLossBackward0>)\n",
      "2729 tensor(4.4129, grad_fn=<MseLossBackward0>)\n",
      "2730 tensor(2.0917, grad_fn=<MseLossBackward0>)\n",
      "2731 tensor(2.0040, grad_fn=<MseLossBackward0>)\n",
      "2732 tensor(1.8664, grad_fn=<MseLossBackward0>)\n",
      "2733 tensor(1.5909, grad_fn=<MseLossBackward0>)\n",
      "2734 tensor(1.6165, grad_fn=<MseLossBackward0>)\n",
      "2735 tensor(0.8315, grad_fn=<MseLossBackward0>)\n",
      "2736 tensor(1.9311, grad_fn=<MseLossBackward0>)\n",
      "2737 tensor(4.6300, grad_fn=<MseLossBackward0>)\n",
      "2738 tensor(2.4936, grad_fn=<MseLossBackward0>)\n",
      "2739 tensor(1.3074, grad_fn=<MseLossBackward0>)\n",
      "2740 tensor(1.5123, grad_fn=<MseLossBackward0>)\n",
      "2741 tensor(1.2137, grad_fn=<MseLossBackward0>)\n",
      "2742 tensor(1.1940, grad_fn=<MseLossBackward0>)\n",
      "2743 tensor(3.6497, grad_fn=<MseLossBackward0>)\n",
      "2744 tensor(1.3086, grad_fn=<MseLossBackward0>)\n",
      "2745 tensor(2.0022, grad_fn=<MseLossBackward0>)\n",
      "2746 tensor(1.5983, grad_fn=<MseLossBackward0>)\n",
      "2747 tensor(2.9903, grad_fn=<MseLossBackward0>)\n",
      "2748 tensor(2.7645, grad_fn=<MseLossBackward0>)\n",
      "2749 tensor(1.4930, grad_fn=<MseLossBackward0>)\n",
      "2750 tensor(1.7084, grad_fn=<MseLossBackward0>)\n",
      "2751 tensor(2.2004, grad_fn=<MseLossBackward0>)\n",
      "2752 tensor(2.2587, grad_fn=<MseLossBackward0>)\n",
      "2753 tensor(2.1949, grad_fn=<MseLossBackward0>)\n",
      "2754 tensor(1.3434, grad_fn=<MseLossBackward0>)\n",
      "2755 tensor(2.3252, grad_fn=<MseLossBackward0>)\n",
      "2756 tensor(3.4644, grad_fn=<MseLossBackward0>)\n",
      "2757 tensor(3.2522, grad_fn=<MseLossBackward0>)\n",
      "2758 tensor(2.9084, grad_fn=<MseLossBackward0>)\n",
      "2759 tensor(2.7974, grad_fn=<MseLossBackward0>)\n",
      "2760 tensor(1.4286, grad_fn=<MseLossBackward0>)\n",
      "2761 tensor(3.7709, grad_fn=<MseLossBackward0>)\n",
      "2762 tensor(4.1105, grad_fn=<MseLossBackward0>)\n",
      "2763 tensor(2.9248, grad_fn=<MseLossBackward0>)\n",
      "2764 tensor(3.6776, grad_fn=<MseLossBackward0>)\n",
      "2765 tensor(3.5887, grad_fn=<MseLossBackward0>)\n",
      "2766 tensor(2.6913, grad_fn=<MseLossBackward0>)\n",
      "2767 tensor(3.2270, grad_fn=<MseLossBackward0>)\n",
      "2768 tensor(2.1858, grad_fn=<MseLossBackward0>)\n",
      "2769 tensor(6.1282, grad_fn=<MseLossBackward0>)\n",
      "2770 tensor(2.3315, grad_fn=<MseLossBackward0>)\n",
      "2771 tensor(2.5593, grad_fn=<MseLossBackward0>)\n",
      "2772 tensor(2.2207, grad_fn=<MseLossBackward0>)\n",
      "2773 tensor(2.0131, grad_fn=<MseLossBackward0>)\n",
      "2774 tensor(1.1575, grad_fn=<MseLossBackward0>)\n",
      "2775 tensor(1.1127, grad_fn=<MseLossBackward0>)\n",
      "2776 tensor(2.1482, grad_fn=<MseLossBackward0>)\n",
      "2777 tensor(3.7768, grad_fn=<MseLossBackward0>)\n",
      "2778 tensor(1.4888, grad_fn=<MseLossBackward0>)\n",
      "2779 tensor(1.7655, grad_fn=<MseLossBackward0>)\n",
      "2780 tensor(1.8131, grad_fn=<MseLossBackward0>)\n",
      "2781 tensor(2.3148, grad_fn=<MseLossBackward0>)\n",
      "2782 tensor(3.3753, grad_fn=<MseLossBackward0>)\n",
      "2783 tensor(4.5160, grad_fn=<MseLossBackward0>)\n",
      "2784 tensor(1.8394, grad_fn=<MseLossBackward0>)\n",
      "2785 tensor(2.2764, grad_fn=<MseLossBackward0>)\n",
      "2786 tensor(2.6325, grad_fn=<MseLossBackward0>)\n",
      "2787 tensor(1.7271, grad_fn=<MseLossBackward0>)\n",
      "2788 tensor(1.6617, grad_fn=<MseLossBackward0>)\n",
      "2789 tensor(1.7573, grad_fn=<MseLossBackward0>)\n",
      "2790 tensor(4.0836, grad_fn=<MseLossBackward0>)\n",
      "2791 tensor(1.2438, grad_fn=<MseLossBackward0>)\n",
      "2792 tensor(3.2215, grad_fn=<MseLossBackward0>)\n",
      "2793 tensor(5.9099, grad_fn=<MseLossBackward0>)\n",
      "2794 tensor(1.9137, grad_fn=<MseLossBackward0>)\n",
      "2795 tensor(2.3690, grad_fn=<MseLossBackward0>)\n",
      "2796 tensor(2.2774, grad_fn=<MseLossBackward0>)\n",
      "2797 tensor(5.1969, grad_fn=<MseLossBackward0>)\n",
      "2798 tensor(1.2787, grad_fn=<MseLossBackward0>)\n",
      "2799 tensor(2.4200, grad_fn=<MseLossBackward0>)\n",
      "2800 tensor(2.2172, grad_fn=<MseLossBackward0>)\n",
      "2801 tensor(2.4074, grad_fn=<MseLossBackward0>)\n",
      "2802 tensor(1.8156, grad_fn=<MseLossBackward0>)\n",
      "2803 tensor(2.3004, grad_fn=<MseLossBackward0>)\n",
      "2804 tensor(1.8055, grad_fn=<MseLossBackward0>)\n",
      "2805 tensor(3.4270, grad_fn=<MseLossBackward0>)\n",
      "2806 tensor(2.4830, grad_fn=<MseLossBackward0>)\n",
      "2807 tensor(1.8309, grad_fn=<MseLossBackward0>)\n",
      "2808 tensor(3.9891, grad_fn=<MseLossBackward0>)\n",
      "2809 tensor(2.5393, grad_fn=<MseLossBackward0>)\n",
      "2810 tensor(2.2445, grad_fn=<MseLossBackward0>)\n",
      "2811 tensor(4.2447, grad_fn=<MseLossBackward0>)\n",
      "2812 tensor(3.2619, grad_fn=<MseLossBackward0>)\n",
      "2813 tensor(4.6485, grad_fn=<MseLossBackward0>)\n",
      "2814 tensor(2.3897, grad_fn=<MseLossBackward0>)\n",
      "2815 tensor(1.6848, grad_fn=<MseLossBackward0>)\n",
      "2816 tensor(3.8870, grad_fn=<MseLossBackward0>)\n",
      "2817 tensor(2.7612, grad_fn=<MseLossBackward0>)\n",
      "2818 tensor(2.5050, grad_fn=<MseLossBackward0>)\n",
      "2819 tensor(3.9004, grad_fn=<MseLossBackward0>)\n",
      "2820 tensor(1.9738, grad_fn=<MseLossBackward0>)\n",
      "2821 tensor(4.0537, grad_fn=<MseLossBackward0>)\n",
      "2822 tensor(1.4795, grad_fn=<MseLossBackward0>)\n",
      "2823 tensor(2.0019, grad_fn=<MseLossBackward0>)\n",
      "2824 tensor(1.8937, grad_fn=<MseLossBackward0>)\n",
      "2825 tensor(4.4834, grad_fn=<MseLossBackward0>)\n",
      "2826 tensor(2.4195, grad_fn=<MseLossBackward0>)\n",
      "2827 tensor(2.5034, grad_fn=<MseLossBackward0>)\n",
      "2828 tensor(2.4132, grad_fn=<MseLossBackward0>)\n",
      "2829 tensor(1.6370, grad_fn=<MseLossBackward0>)\n",
      "2830 tensor(2.2496, grad_fn=<MseLossBackward0>)\n",
      "2831 tensor(3.9522, grad_fn=<MseLossBackward0>)\n",
      "2832 tensor(4.2202, grad_fn=<MseLossBackward0>)\n",
      "2833 tensor(1.5305, grad_fn=<MseLossBackward0>)\n",
      "2834 tensor(2.7485, grad_fn=<MseLossBackward0>)\n",
      "2835 tensor(1.5212, grad_fn=<MseLossBackward0>)\n",
      "2836 tensor(1.1632, grad_fn=<MseLossBackward0>)\n",
      "2837 tensor(1.8853, grad_fn=<MseLossBackward0>)\n",
      "2838 tensor(2.7393, grad_fn=<MseLossBackward0>)\n",
      "2839 tensor(1.8349, grad_fn=<MseLossBackward0>)\n",
      "2840 tensor(1.5637, grad_fn=<MseLossBackward0>)\n",
      "2841 tensor(1.6832, grad_fn=<MseLossBackward0>)\n",
      "2842 tensor(1.7107, grad_fn=<MseLossBackward0>)\n",
      "2843 tensor(2.7269, grad_fn=<MseLossBackward0>)\n",
      "2844 tensor(1.6555, grad_fn=<MseLossBackward0>)\n",
      "2845 tensor(1.2916, grad_fn=<MseLossBackward0>)\n",
      "2846 tensor(2.3902, grad_fn=<MseLossBackward0>)\n",
      "2847 tensor(2.5641, grad_fn=<MseLossBackward0>)\n",
      "2848 tensor(1.5403, grad_fn=<MseLossBackward0>)\n",
      "2849 tensor(2.7348, grad_fn=<MseLossBackward0>)\n",
      "2850 tensor(1.5209, grad_fn=<MseLossBackward0>)\n",
      "2851 tensor(5.0474, grad_fn=<MseLossBackward0>)\n",
      "2852 tensor(5.6414, grad_fn=<MseLossBackward0>)\n",
      "2853 tensor(2.4666, grad_fn=<MseLossBackward0>)\n",
      "2854 tensor(2.7366, grad_fn=<MseLossBackward0>)\n",
      "2855 tensor(2.4811, grad_fn=<MseLossBackward0>)\n",
      "2856 tensor(3.9384, grad_fn=<MseLossBackward0>)\n",
      "2857 tensor(1.6420, grad_fn=<MseLossBackward0>)\n",
      "2858 tensor(1.2610, grad_fn=<MseLossBackward0>)\n",
      "2859 tensor(1.9526, grad_fn=<MseLossBackward0>)\n",
      "2860 tensor(3.7235, grad_fn=<MseLossBackward0>)\n",
      "2861 tensor(4.2401, grad_fn=<MseLossBackward0>)\n",
      "2862 tensor(3.4637, grad_fn=<MseLossBackward0>)\n",
      "2863 tensor(2.8567, grad_fn=<MseLossBackward0>)\n",
      "2864 tensor(1.9455, grad_fn=<MseLossBackward0>)\n",
      "2865 tensor(3.2182, grad_fn=<MseLossBackward0>)\n",
      "2866 tensor(2.6297, grad_fn=<MseLossBackward0>)\n",
      "2867 tensor(4.6257, grad_fn=<MseLossBackward0>)\n",
      "2868 tensor(2.0203, grad_fn=<MseLossBackward0>)\n",
      "2869 tensor(2.6780, grad_fn=<MseLossBackward0>)\n",
      "2870 tensor(2.8835, grad_fn=<MseLossBackward0>)\n",
      "2871 tensor(3.6144, grad_fn=<MseLossBackward0>)\n",
      "2872 tensor(1.5622, grad_fn=<MseLossBackward0>)\n",
      "2873 tensor(1.1779, grad_fn=<MseLossBackward0>)\n",
      "2874 tensor(2.5783, grad_fn=<MseLossBackward0>)\n",
      "2875 tensor(2.4220, grad_fn=<MseLossBackward0>)\n",
      "2876 tensor(4.9020, grad_fn=<MseLossBackward0>)\n",
      "2877 tensor(2.1309, grad_fn=<MseLossBackward0>)\n",
      "2878 tensor(1.6767, grad_fn=<MseLossBackward0>)\n",
      "2879 tensor(1.6605, grad_fn=<MseLossBackward0>)\n",
      "2880 tensor(2.8872, grad_fn=<MseLossBackward0>)\n",
      "2881 tensor(2.1255, grad_fn=<MseLossBackward0>)\n",
      "2882 tensor(3.2687, grad_fn=<MseLossBackward0>)\n",
      "2883 tensor(1.7690, grad_fn=<MseLossBackward0>)\n",
      "2884 tensor(2.5581, grad_fn=<MseLossBackward0>)\n",
      "2885 tensor(2.8003, grad_fn=<MseLossBackward0>)\n",
      "2886 tensor(4.6017, grad_fn=<MseLossBackward0>)\n",
      "2887 tensor(3.5089, grad_fn=<MseLossBackward0>)\n",
      "2888 tensor(3.1514, grad_fn=<MseLossBackward0>)\n",
      "2889 tensor(1.3509, grad_fn=<MseLossBackward0>)\n",
      "2890 tensor(4.6253, grad_fn=<MseLossBackward0>)\n",
      "2891 tensor(1.5857, grad_fn=<MseLossBackward0>)\n",
      "2892 tensor(1.1847, grad_fn=<MseLossBackward0>)\n",
      "2893 tensor(4.8961, grad_fn=<MseLossBackward0>)\n",
      "2894 tensor(1.8909, grad_fn=<MseLossBackward0>)\n",
      "2895 tensor(2.4249, grad_fn=<MseLossBackward0>)\n",
      "2896 tensor(2.6555, grad_fn=<MseLossBackward0>)\n",
      "2897 tensor(3.9609, grad_fn=<MseLossBackward0>)\n",
      "2898 tensor(2.6149, grad_fn=<MseLossBackward0>)\n",
      "2899 tensor(3.7163, grad_fn=<MseLossBackward0>)\n",
      "2900 tensor(4.8386, grad_fn=<MseLossBackward0>)\n",
      "2901 tensor(4.4329, grad_fn=<MseLossBackward0>)\n",
      "2902 tensor(2.6437, grad_fn=<MseLossBackward0>)\n",
      "2903 tensor(1.3681, grad_fn=<MseLossBackward0>)\n",
      "2904 tensor(2.2474, grad_fn=<MseLossBackward0>)\n",
      "2905 tensor(1.7848, grad_fn=<MseLossBackward0>)\n",
      "2906 tensor(3.3769, grad_fn=<MseLossBackward0>)\n",
      "2907 tensor(1.5547, grad_fn=<MseLossBackward0>)\n",
      "2908 tensor(1.2761, grad_fn=<MseLossBackward0>)\n",
      "2909 tensor(2.7502, grad_fn=<MseLossBackward0>)\n",
      "2910 tensor(1.2335, grad_fn=<MseLossBackward0>)\n",
      "2911 tensor(2.5199, grad_fn=<MseLossBackward0>)\n",
      "2912 tensor(2.3376, grad_fn=<MseLossBackward0>)\n",
      "2913 tensor(1.7045, grad_fn=<MseLossBackward0>)\n",
      "2914 tensor(1.7018, grad_fn=<MseLossBackward0>)\n",
      "2915 tensor(1.7858, grad_fn=<MseLossBackward0>)\n",
      "2916 tensor(2.7969, grad_fn=<MseLossBackward0>)\n",
      "2917 tensor(1.6070, grad_fn=<MseLossBackward0>)\n",
      "2918 tensor(1.6715, grad_fn=<MseLossBackward0>)\n",
      "2919 tensor(2.8801, grad_fn=<MseLossBackward0>)\n",
      "2920 tensor(1.9961, grad_fn=<MseLossBackward0>)\n",
      "2921 tensor(1.9432, grad_fn=<MseLossBackward0>)\n",
      "2922 tensor(3.4206, grad_fn=<MseLossBackward0>)\n",
      "2923 tensor(1.6888, grad_fn=<MseLossBackward0>)\n",
      "2924 tensor(2.2377, grad_fn=<MseLossBackward0>)\n",
      "2925 tensor(2.9628, grad_fn=<MseLossBackward0>)\n",
      "2926 tensor(1.6824, grad_fn=<MseLossBackward0>)\n",
      "2927 tensor(2.1841, grad_fn=<MseLossBackward0>)\n",
      "2928 tensor(2.5356, grad_fn=<MseLossBackward0>)\n",
      "2929 tensor(2.0799, grad_fn=<MseLossBackward0>)\n",
      "2930 tensor(1.7223, grad_fn=<MseLossBackward0>)\n",
      "2931 tensor(2.3030, grad_fn=<MseLossBackward0>)\n",
      "2932 tensor(2.2216, grad_fn=<MseLossBackward0>)\n",
      "2933 tensor(1.4089, grad_fn=<MseLossBackward0>)\n",
      "2934 tensor(2.3748, grad_fn=<MseLossBackward0>)\n",
      "2935 tensor(1.8064, grad_fn=<MseLossBackward0>)\n",
      "2936 tensor(2.5083, grad_fn=<MseLossBackward0>)\n",
      "2937 tensor(1.4147, grad_fn=<MseLossBackward0>)\n",
      "2938 tensor(3.6770, grad_fn=<MseLossBackward0>)\n",
      "2939 tensor(2.1761, grad_fn=<MseLossBackward0>)\n",
      "2940 tensor(1.3384, grad_fn=<MseLossBackward0>)\n",
      "2941 tensor(2.4139, grad_fn=<MseLossBackward0>)\n",
      "2942 tensor(1.8866, grad_fn=<MseLossBackward0>)\n",
      "2943 tensor(3.2443, grad_fn=<MseLossBackward0>)\n",
      "2944 tensor(1.8090, grad_fn=<MseLossBackward0>)\n",
      "2945 tensor(1.7121, grad_fn=<MseLossBackward0>)\n",
      "2946 tensor(4.2884, grad_fn=<MseLossBackward0>)\n",
      "2947 tensor(1.2036, grad_fn=<MseLossBackward0>)\n",
      "2948 tensor(2.3077, grad_fn=<MseLossBackward0>)\n",
      "2949 tensor(1.0285, grad_fn=<MseLossBackward0>)\n",
      "2950 tensor(0.8714, grad_fn=<MseLossBackward0>)\n",
      "2951 tensor(1.3025, grad_fn=<MseLossBackward0>)\n",
      "2952 tensor(1.7706, grad_fn=<MseLossBackward0>)\n",
      "2953 tensor(2.9629, grad_fn=<MseLossBackward0>)\n",
      "2954 tensor(2.2363, grad_fn=<MseLossBackward0>)\n",
      "2955 tensor(1.6676, grad_fn=<MseLossBackward0>)\n",
      "2956 tensor(1.3005, grad_fn=<MseLossBackward0>)\n",
      "2957 tensor(3.0725, grad_fn=<MseLossBackward0>)\n",
      "2958 tensor(1.5668, grad_fn=<MseLossBackward0>)\n",
      "2959 tensor(3.1012, grad_fn=<MseLossBackward0>)\n",
      "2960 tensor(1.4488, grad_fn=<MseLossBackward0>)\n",
      "2961 tensor(4.0460, grad_fn=<MseLossBackward0>)\n",
      "2962 tensor(2.4731, grad_fn=<MseLossBackward0>)\n",
      "2963 tensor(1.4680, grad_fn=<MseLossBackward0>)\n",
      "2964 tensor(1.6520, grad_fn=<MseLossBackward0>)\n",
      "2965 tensor(1.3653, grad_fn=<MseLossBackward0>)\n",
      "2966 tensor(1.4240, grad_fn=<MseLossBackward0>)\n",
      "2967 tensor(1.7186, grad_fn=<MseLossBackward0>)\n",
      "2968 tensor(1.4924, grad_fn=<MseLossBackward0>)\n",
      "2969 tensor(5.1019, grad_fn=<MseLossBackward0>)\n",
      "2970 tensor(2.7432, grad_fn=<MseLossBackward0>)\n",
      "2971 tensor(2.6300, grad_fn=<MseLossBackward0>)\n",
      "2972 tensor(2.9548, grad_fn=<MseLossBackward0>)\n",
      "2973 tensor(2.3625, grad_fn=<MseLossBackward0>)\n",
      "2974 tensor(3.9171, grad_fn=<MseLossBackward0>)\n",
      "2975 tensor(3.0698, grad_fn=<MseLossBackward0>)\n",
      "2976 tensor(2.8102, grad_fn=<MseLossBackward0>)\n",
      "2977 tensor(2.3986, grad_fn=<MseLossBackward0>)\n",
      "2978 tensor(4.0955, grad_fn=<MseLossBackward0>)\n",
      "2979 tensor(6.3060, grad_fn=<MseLossBackward0>)\n",
      "2980 tensor(3.3122, grad_fn=<MseLossBackward0>)\n",
      "2981 tensor(1.3292, grad_fn=<MseLossBackward0>)\n",
      "2982 tensor(1.7819, grad_fn=<MseLossBackward0>)\n",
      "2983 tensor(1.8605, grad_fn=<MseLossBackward0>)\n",
      "2984 tensor(3.8553, grad_fn=<MseLossBackward0>)\n",
      "2985 tensor(0.6806, grad_fn=<MseLossBackward0>)\n",
      "2986 tensor(1.3679, grad_fn=<MseLossBackward0>)\n",
      "2987 tensor(3.6691, grad_fn=<MseLossBackward0>)\n",
      "2988 tensor(2.3237, grad_fn=<MseLossBackward0>)\n",
      "2989 tensor(3.5814, grad_fn=<MseLossBackward0>)\n",
      "2990 tensor(2.1163, grad_fn=<MseLossBackward0>)\n",
      "2991 tensor(2.6171, grad_fn=<MseLossBackward0>)\n",
      "2992 tensor(2.0298, grad_fn=<MseLossBackward0>)\n",
      "2993 tensor(2.4976, grad_fn=<MseLossBackward0>)\n",
      "2994 tensor(1.5585, grad_fn=<MseLossBackward0>)\n",
      "2995 tensor(2.2181, grad_fn=<MseLossBackward0>)\n",
      "2996 tensor(1.2827, grad_fn=<MseLossBackward0>)\n",
      "2997 tensor(1.3313, grad_fn=<MseLossBackward0>)\n",
      "2998 tensor(1.8921, grad_fn=<MseLossBackward0>)\n",
      "2999 tensor(3.5847, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# grad\n",
    "for epoch in range(3000):\n",
    "    for bx, by in loader:\n",
    "        # 현재 파라미터를 이용해서 예측갑을 구하고 f(x)=ax^2+bx+c\n",
    "        pred = model(bx)\n",
    "        loss = loss_fn(pred, by)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        opt.step()\n",
    "    print(epoch, loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f8c1a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZfhJREFUeJzt3Qd8k9XeB/DfkzRp0r0XdAFl711AEUUBBQe48b3IxXGvE3Bc8aqIXsXrVcGJCyfixoEDFBQQKKvsvWmhe890JM/7OadNSDqAQkfa/L737fv0SdP0aajNr//zP+coqqqqICIiImommub6QkREREQMH0RERNTsWPkgIiKiZsXwQURERM2K4YOIiIiaFcMHERERNSuGDyIiImpWDB9ERETUrNzgZCwWC1JSUuDt7Q1FUVr6coiIiOgciDVLCwsLERERAY1G07rChwgekZGRLX0ZREREdB6Sk5PRvn371hU+RMXDevE+Pj4tfTlERER0DgoKCmTxwPo63qrCh3WoRQQPhg8iIqLW5VxaJthwSkRERM2K4YOIiIiaFcMHERERNSuGDyIiImpWDB9ERETUrBg+iIiIqFkxfBAREVGzYvggIiKiZsXwQURERM2K4YOIiIiaFcMHERGRC0krTsOm1E3y2CrCh9lsxpNPPonY2FgYjUZ07NgRzz77rNxG10q8/9RTTyE8PFzeZ/To0Th06FBTXDsRERE1wJJDSzDm2zGY9ts0eRTnTh8+/vvf/2LBggV44403sG/fPnn+4osv4vXXX7fdR5y/9tprePvtt7Fx40Z4enpizJgxMJlMTXH9REREdA5EpWNOwhxYVIs8F0dx3hIVkAbtart+/Xpcc801uOqqq+R5TEwMPv/8c2zatMlW9Zg/fz6eeOIJeT/hk08+QWhoKL7//nvcfPPNTfE9EBER0VkkFSTZgoeVOE8uTEaYZxictvIxbNgwrFy5EgcPHpTnO3bswNq1azFu3Dh5fuzYMaSlpcmhFitfX18MGTIECQkJdT5mWVkZCgoKHN6IiIiocUX5REGjOL7si/NI70g0twaFj8cee0xWL7p27QqdTod+/fph+vTpmDx5svy4CB6CqHTYE+fWj9U0d+5cGVCsb5GRzf8kEBERtXVhnmGYHT/bFkDEUZw3d9WjwcMuX331FT777DMsXrwYPXr0wPbt22X4iIiIwJQpU87rAmbNmoWZM2fazkXlgwGEiIio8U2Mm4hhEcPkUIuoeLRE8Ghw+HjkkUds1Q+hV69eOHHihKxeiPARFlb1TaSnp8vZLlbivG/fvnU+pru7u3wjIiKipicCR0uFjvMadikpKYFG4/gpWq0WFktVA4uYgisCiOgLsa9kiFkv8fHxjXXNRERE1Io1qPIxYcIEPPfcc4iKipLDLtu2bcMrr7yCv//97/LjiqLIYZj//Oc/iIuLk2FErAsihmWuvfbapvoeiIiI6AzEdFox20U0nbZ01aPB4UOs5yHCxD333IOMjAwZKu6++265qJjVo48+iuLiYtx1113Iy8vDiBEjsGzZMhgMhqa4fiIiIjoDsZCYdX0Pa5Op6P1oSYpqvzypExDDNGLWS35+Pnx8fFr6coiIiFp1xWPMt2Mc1vcQAWT5pOWNXgFpyOs393YhIiJywYXFWhLDBxERURsV5UQLizlcQ4t+dSIiInKJhcXOu+GUiIiIWpeJTrKwmD2GDyIiojYuzAkWFrPHYRciIiJqVgwfRERE1KwYPoiIiFxIUa4JJw/kymNLYc8HERGRi9i7LgV/LtoPiOVFFWDUbV3RfXhEs18HKx9EREQuoCjXhD8X7UOexoQjhnx5FOctUQFh5YOIiMgFHE06hUSPHPypM0KFHorBglEVObg46RR6+Xds1mth5YOIiKiNK8o1YevhY9XBQ5G3ieOfOgP2F51o9uth5YOIiKiN93msWrQfJ71ToWoCHD6mQoOM7NJmvyZWPoiIiNpwxWPVov0ogIpyz3wosCAM2YjX7JFHDSzw1qY2+3Wx8kFERNRG5WWUYoeuEr8Zy6GWxuJG7R+Y67YQWkWFWVXwU3BPKNobmv26WPkgIiJqo04hB795lENVFFnpsAYPQRzHZ+1CePmxZr8uhg8iIqI2at3hnbYG01hNmi14WGlFdSS7uNmvi+GDiIiojTKWnrD1eQSgQA612KuEBofLgpr9utjzQURE1EYFGyowv/1/MT5zt6x6WFTAAgUaqDJ4PNL5YXhrfJr9uhg+iIiI2qgoyykMzNoFTXXBQxwrAdzV7Wkk+vZAmi4IcwoON/t1cdiFiIiojUrLFNNpHblBRY7eTwaPK9b8gGEdBjT7dbHyQURE1EYdKguAGRpoYbHdJoZbOm3agt7pK+CvDoa50L3Zr4uVDyIiolYirTgNm1I3yeO5sAT3wcOdH5aBQ7D2eXi5XYr2upvgpu+BkuJyNDdWPoiIiFqBJYeWYE7CHFhUCzSKBrPjZ2Ni3MQzfk6EIRyvBPTFqoBBiC09hWPGdkjXB2PpgWKE+FmQfnQdQjSeADqgObHyQURE5OTSitNswUMQR3F+tgpIz6AOsq9D9Hes9+sng8fje8oQWqZCURSExsbD4/gRNDdWPoiIiJxcUkGSLXhYifPkwmSEeYbV/4l5OvRPCkDsZy8DwV0xwTBKBg8rRaOFPjQCzY3hg4iIyMlF+UTJoRb7ACLOI70jz/h5BT5usq+jPWLhV1CGENF4qtgtNKaa4RGrQ3PjsAsREZGTC/MMkz0eInAI1p6PM1Y9AMQo6Ricsxlj/Pww1LfqvqpaVfkwQ8XB4L/gFtW8/R4CKx9EREStwMS4iRgWMUwOtYiKx9mCh2DcvR9hHYbJ/g5BHFWLGQtCcrG0ezSyDeOx2T0YzT3wwvBBRETUSoR5hp1T6LDSh7WD6ajjIEeJpgJpHiqKlTKYYcSx0jJEGPRoThx2ISIiakNre9jziHUD7PpEDmhT8IX7OkSm78TkDcvRPfU4Yo1cZIyIiIjs1vYY8+0YTPttmjyK84Zwi+qIg8FrZH9HMUxY67YP1o1tRfXh4kM74FlWiubWoMpHTEyMHC+q+XbvvffKj5tMJvl+YGAgvLy8MGnSJKSnpzfVtRMREbVZaee5toe9FPdg3DZgPCaM9MLsbmZb8LBRVeTk5MCpw8fmzZuRmppqe/v999/l7TfccIM8zpgxA0uXLsXXX3+N1atXIyUlBRMnnnn1NSIiImrY2h7n6mhpGSxQZH9Hknul3Q4vVUQWCQgIAJy54TQ4ONjh/IUXXkDHjh0xcuRI5OfnY+HChVi8eDEuvfRS+fEPP/wQ3bp1w4YNGzB06NDGvXIiIiIXXNvDLcMHRe4mePkbzvoYHYzu6JZ6HBcd3C6rDeKR1OrQoVgsGJiYCI/SUsDXF62i4bS8vByLFi3C3//+dzn0kpiYiIqKCowePdp2n65duyIqKgoJCQn1Pk5ZWRkKCgoc3oiIiFxdWM21PaDBxYdvxPq3TuGTx9dj77qUsz6G6OcYeWiH7cVeHBVVRfzadRi/9Cd0OHIU5SeSgNYy1fb7779HXl4ebr/9dnmelpYGvV4PPz8/h/uFhobKj9Vn7ty5mDNnzvleBhERUZtf2+NgyhFseCMditmANI98+JS7Y9Vn+xHVPeCMFRDZz1G9qJiNosBQXlZV8dBooI+OQnM778qHGGIZN24cIiIubGmSWbNmySEb61ty8rmPZREREblCBaSDpTuS9Wa85WXBp3q9PO7zzEZ+xplnqoh+jpo9pmK4xauwSAaP8GfmQBd27uuGtGjl48SJE1ixYgWWLDk95ScsLEwOxYhqiH31Q8x2ER+rj7u7u3wjIiKi2opyTfhrx378pDVArY4S4ijOR6ceRbsuA1AfUd0YuHkztgwYAFWjqerz2JKITs8/D49+fVskeJx3+BCNpCEhIbjqqqtstw0YMAA6nQ4rV66UU2yFAwcOICkpCfHx8Y13xURERC5i77oUrFq0H1lBh6CiPcKQjVhNGo5ZwpCGQOw+uR3XoP7wUX78hOzrCEtJRZG3l6x4iEDiFhDQYsHjvMKHxWKR4WPKlClwczv96b6+vpg2bRpmzpwpyzw+Pj64//77ZfDgTBciIqKGVzxWLdqPAqjI8yjATUV/4Hm3hdAqKsyqgn9XTkNkkP8ZH0MfEy2HV0TgkD0eQgv1eVxQz4cYbhHVDDHLpaZ58+Zh/PjxsvJx8cUXy+EW+6EZIiIiOjd5GaXYoavEOz6lWJ8fagsegjg+r1sI//KiMz6GqG6Ivg4ROKQW7PO4oMrHFVdcYduOtyaDwYA333xTvhEREdH5O4Uc/OZRLvs7xFCLNXhYaaDCcvLgWR/H7/rr4TlihJxSKyoeLR08BO5qS0RE5ITWHd4JFVW7zYoeDzHUYh9AKqHBEbPj4p/1EYHDGUKHFXe1JSIickKGksNQYLE1mb5QeQsqVY0teDzS+WHkBg5Ba8TKBxERkRMK1Jdgfvv/YnzmbluT6XOBU7E9sjeOGdshTReE98oy0Rqx8kFEROSEQgvSMT5rl0OT6eM5H9mCxxVrfkCw5sIW+mwprHwQERE5oQOZWbikxm1usOCu9T8gP7UCxopusLRLBdALrQ3DBxERkRM64t0V5oLfoZV70cLW6xHrcQV6BPgj/eg6BJYcRWvEYRciIiInNCC4Dx6JmykDhyCO/4p7CMGlgXI3+dDYeAT3ikNrxMoHERGRE4px74xftu3E4CFfIKYsBScMEZh62BehZRXy44pGCzeDFq0RwwcREZETsiSnov8Jf3RL+RbmwE54XN8RYeVVwUNQYYFbVAxaI4YPIiIiJxRWfBTDi0sR2uE2KFCglqkoQikKtKXwsnjgtR6+eCo4HK1xvgvDBxERkRMK7h0HtzQ32d8hHHRLxVq3fVAVUfUA9mv74lhpN0QYqlZBbU3YcEpEROSE3Nx1UJSql+limGzBQxCHiw/uQHBFGVojhg8iIiIn5BYdLfs6hHxNiS142G8spysuRGvE8EFEROSEMoLD8VwPIwpgQinKq8Za7IgsEhAQgNaIPR9ERERO6GhpGfZrU/ClYbsMGtbsId5XLBYMTEyER2kp4OuL1obhg4iIyAmFVJTh4oNVwUMQRzEI03PbDnRIOiGDR/mJJOjCwtDacNiFiIjICemKC2u9SIvzkNzsqoqHRgN9dBRaI4YPIiIiJxQQEGCreliJ4RavwiIZPMKfmdMqqx4Ch12IiIickEdpKQZu3owtAwZA1Wiq+jy2JKLT88/Do1/fVhs8BIYPIiIiJ1R+/AQ6HDmKsJRUFHl7yYqHCCRuAQGtOngIDB9EREROSB8TLYdXROCQPR5CK+7zsMeeDyIiIiekCwuTfR0icEitvM/DHisfRERETsrv+uvhOWKEnFIrKh5tIXgIDB9EREROTBcW1mZChxWHXYiIiKhZMXwQERFRs2L4ICIiombF8EFERETNiuGDiIiImhXDBxERETUrhg8iIiJqVgwfRERE5Nzh49SpU7jtttsQGBgIo9GIXr16YcuWLbaPq6qKp556CuHh4fLjo0ePxqFDhxr7uomIiMgVwkdubi6GDx8OnU6HX3/9FXv37sXLL78Mf39/231efPFFvPbaa3j77bexceNGeHp6YsyYMTCZTE1x/URERNTKKKooVZyjxx57DOvWrcNff/1V58fFQ0VEROChhx7Cww8/LG/Lz89HaGgoPvroI9x8881n/RoFBQXw9fWVn+fj49OQ74WIiIhaSENevxtU+fjxxx8xcOBA3HDDDQgJCUG/fv3w3nvv2T5+7NgxpKWlyaEWK3EhQ4YMQUJCwvl8L0RERNTGNCh8HD16FAsWLEBcXByWL1+Of/7zn3jggQfw8ccfy4+L4CGISoc9cW79WE1lZWUyLdm/ERERUdvVoF1tLRaLrHw8//zz8lxUPnbv3i37O6ZMmXJeFzB37lzMmTPnvD6XiIiI2njlQ8xg6d69u8Nt3bp1Q1JSknw/rHrL3/T0dIf7iHPrx2qaNWuWHB+yviUnJzf0eyAiIqK2Gj7ETJcDBw443Hbw4EFER0fL92NjY2XIWLlype3jYhhFzHqJj4+v8zHd3d1lY4r9GxEREbVdDRp2mTFjBoYNGyaHXW688UZs2rQJ7777rnwTFEXB9OnT8Z///Ef2hYgw8uSTT8oZMNdee21TfQ9ERETUVsPHoEGD8N1338mhkmeeeUaGi/nz52Py5Mm2+zz66KMoLi7GXXfdhby8PIwYMQLLli2DwWBoiusnIiJqVkW5JuRllMIvxAgvf762Nfk6H82B63wQEZGz2rsuBcu+X41iz2x4Fgdi7LUj0X14REtfVqt7/W5Q5YOIiMiVKx4frF6Mb7VdoJp8oGgtSFm9GE93v48VkAbixnJERETn4Je/1uDbgi4IRS7iNXvkcUlBF/yydg2fvwZi5YOIiOgcqh67Tu7CDdpUzHV7H1pFhVlVMKvyDuzJEMMuV/A5bABWPoiIiM7S5/Hm07+gpGgrprt/KIOHII7Puy2Eb+U+Pn8NxMoHERHRGSoeby3/EKv7fSmmaGCFGo7ZWTmYWFRc9SKqWOBVcJLPXwOx8kFERFSPzYd3YHWHL2TwECyKgjlBAUjTauV5JTRI1fXm89dADB9ERET1+O34UkBxvE0EkGSdmwwej8bNRHd9Dz5/DcRhFyIionpoM1Kg8VRl4LDSqCpeD/4n9kQMw6W7KhGeeZjPXwMxfBAREdUjsjwPs0tz5FCLCCAieEzPyUOmpgT/t7kcXYp9UdHZj89fAzF8EBER1eOILgZ35P+BYaUmOdSyW6/H/AA/WJRlWOS3HA+m3orJQ67n89dA7PkgIiKqR4eicDwS9xCCzCoiKyqrg0fVEIyqqHg1fDH2+9ZoCqGzYvggIiKqR1yaGcqJMAwZ8gVu7/qgQ++HNYBsyDvI56+BGD6IiIjqEdMjHNPzu+DdDZ4Ym9QFqLEVq6Iq6KCw56OhGD6IiIjqYegfD0VR4FVWiv7ZWvTP6i8DhyCO/bL6YYBHMJ+/BmLDKRERUT2SggJRpN2G9W77IDJHbGEsQktCUawvhme5B0Zu2APDRTlAJz6FDcHwQUREVI9Ao4plun2n1xlTAKPFA4MSDqFD0gl4lJVBHx3F56+BOOxCRERUD11xYc0FTuV5SG62DB7hz8yBLiyMz18DsfJBRERw9c3j8jJK4RdihJe/weFjPmYzoKqA3SwXxWJBx5kPIXToEAaP88TwQURELmvvuhR8+9NSZPgeR0h+DCaNn4DuwyNsHzdkZWPQ5s3YMnAgVI1GBo+BW7bA/6rxDB4XgOGDiIhctuLxzOY52NMloWrX2jAFezZvxfvdX7VVQPQx0ehw/ATCUtNQ5O0Fr8Ii9nk0AvZ8EBGRS1r855fY47e+KngIioq9fuvx+Z9f2u4j+jlEX4fo7wjJyGSfRyNh5YOIiFzS3tRlgJfjbWI67d7U5QCm2G7zu/56eI4YgfITSXJmCxtMLxzDBxERuaRQUx40nqrDkuli19pgU26t+4rAwdDReDjsQkRELqm8pCuezMqVgUMQxyeyclFR0rWlL63NY/ggIiKX1CE7BOvD/4lfktPwQWq6PG4I/4e8nZoWh12IiMglDesRgfzszrh64CBEm1JwwhCB2w/5YFgP7lLb1Bg+iIjIZTeNu/brdPTJdsNBr3B0LnJDbFkllBvjW/rS2jyGDyIicu1N47APajFwUlRDtN3gHdgd7Vr64to49nwQEZHLbhq3Vle1W60gjuI8wFi97gc1GYYPIiJySUryiTo3jdMkn2ihK3IdDB9EROSSvAsLqzaNsyP2bhFLqFPTYvggIiKXFNi1KwZt2SIDhyA3jUtMRGDXLi19aW0eG06JiMgliRVLh0+dirC5L6DI0wNexSXoOOsxrmTqbJWPp59+GoqiOLx17Xp6JTiTyYR7770XgYGB8PLywqRJk5Cent4U101ERHTBxL4tvX/+CQP/95I8inNywspHjx49sGLFitMP4Hb6IWbMmIGff/4ZX3/9NXx9fXHfffdh4sSJWLduXeNdMRERUSPivi2tIHyIsBEWFlbr9vz8fCxcuBCLFy/GpZdeKm/78MMP0a1bN2zYsAFDhw5tnCsmIiIi12o4PXToECIiItChQwdMnjwZSUlJ8vbExERUVFRg9OjRtvuKIZmoqCgkJCTU+3hlZWUoKChweCMiIqK2q0HhY8iQIfjoo4+wbNkyLFiwAMeOHcNFF12EwsJCpKWlQa/Xw8/Pz+FzQkND5cfqM3fuXDlEY32LjIw8/++GiIiI2tawy7hx42zv9+7dW4aR6OhofPXVVzAajed1AbNmzcLMmTNt56LywQBCRETUdl3QOh+iytG5c2ccPnxY9oGUl5cjLy/P4T5itktdPSJW7u7u8PHxcXgjIiKituuCwkdRURGOHDmC8PBwDBgwADqdDitXrrR9/MCBA7InJD6eOwQSERHReQy7PPzww5gwYYIcaklJScHs2bOh1Wpxyy23yH6NadOmySGUgIAAWcG4//77ZfDgTBciIiI6r/Bx8uRJGTSys7MRHByMESNGyGm04n1h3rx50Gg0cnExMYtlzJgxeOuttxryJYiIiBrs6KlkHD2RjA7RkejQjhMXnJ2iqjV21WlhouFUVFHEuiHs/yAiorNZtOQnJO9ZC9UrA0pRCCJ7jMBtE8fziXPi12/u7UJERK2WqHhkHl+MRyt/gTZfhVlV8NrxJBw91YcVECfGXW2JiKjV+uqHV/FA/i/QKlVFfHG8P/9XeTs5L4YPIiJqtdTiDFvwsHJTLEBxRotdE50dwwcREbVaPll6OdRir1LVwCtL32LXRGfH8EFERK1Wd+NgvO91iwwcgjgu9LoZ3Y2DWvrS6AzYcEpERK1WxaUDkbYyBc8Y74K7rhRlFUZoi4zocBnDhzNj+CAiolZl2XcvQU3aAiVqIPwjRkKMumhMRlSYjLKcL859DOaWvkw6A4YPIiJqNRJeGoUrirbKkGHJ/RUbj/wEqCMBxa7vQ1URXVLSkpdJZ8GeDyIiajUVjyFF22wvXOI4uGg7OhftR4mmGBmGDHkctGULArt2aeGrpTNh5YOIiFqF9P1/QQPHabVaqEiMzMTvvnshek7FLNvecROgO8Nu6tTyWPkgIiKnV3JkM0xlPjDX2BDklEaH3/wyZfAQxPF/FT8jrTitRa6Tzg3DBxERObXcRQ/B8OnluBs/QoFiW9dDTKt9SjMBcFzmAxbVguTC5Ja5WDonHHYhIiKnrnj4Hl5oG27RKFX7t9xT/gASLXEweJdCQSJUu+EYjaJBpDd3tnVmrHwQEZHT+mbx/2r3eSgqvHwqMMArA6OKUvG38CkycAjiODt+NsI82fPhzFj5ICIip7XbrT3MlYrD/i1iuMVkMCAkr0hOq71NPxC3TZosh1pExYPBw/mx8kFERE4roDICsyrvcFg+/fHKaVDK3GTwsE6rFYFjUNggBo9WgpUPIiJyWn1HjsdrO1dj+MlXEatJxzFLKALbl2LijsPot/VPdJz1GKfVtkIMH0RE5LT83QsxOCsVuf5uyDZ6YGhpEvyzKtF59GD0fvYhBo9WiuGDiIicVnRpsRxe8S+thH9pQdWNqorOoSEMHq0Yez6IiMhpBXbtKvs6ROCwBY89G3A0uJQLibViiqpa/0WdQ0FBAXx9fZGfnw8fH5+WvhwiImphed98gyNzX0CRpwcSowvx8WgzLFBt02onxk1s6UskNOz1m+GDiIic3snju5B4aDWeTHm31oJiyyct5yyXVhY+2PNBREQtruLwdlQe2Ai3LkOg69TX4WNLDi3BnIQ5ctn0mqxLqXNtj9aF4YOIiFpUybsPwnjqI+gUQN0ElLS7HR53vSo/JjaIqy94CFxKvXViwykREbVoxcOQ8jGU6s3hxFGci9uFpIKkMwYPLqXeOjF8EBFRi/l58RO19m4R5z8tfkK+H+UTZdu3xfZxRYOXRr4kez3YbNo6MXwQEVGL2WWIkrvU2hNLqO8xRMn3RS+HqG7U3DhuTMwY9nm0Yuz5ICKiFpNl6i73bnnebSHcFItt75ZyU4ztPqK6MSxiGDeOa0MYPoiIqMX0DAjGcydjsMbcGzGadBy3hCINAfh3QInD/UQFhDNa2g4OuxARUYuZOHYAhrkdRzoCsMHSXR7F+cSx/fmv0oax8kFERC0msFMn3BOlR7uT21GgGuGjlOKa9iHydmq7GD6IiKhFjbjrLnQ7fBgZBw8hpHMcg4cLuKBhlxdeeAGKomD69Om220wmE+69914EBgbCy8sLkyZNQnp6emNcKxERtVGi0tHtynEMHi7ivMPH5s2b8c4776B3794Ot8+YMQNLly7F119/jdWrVyMlJQUTJ3LTHyIiIrqA8FFUVITJkyfjvffeg7+/v+12sZnMwoUL8corr+DSSy/FgAED8OGHH2L9+vXYsGHD+XwpIiIiamPOK3yIYZWrrroKo0ePdrg9MTERFRUVDrd37doVUVFRSEhIqPOxysrK5E549m9ERETUdjW44fSLL77A1q1b5bBLTWlpadDr9fDz83O4PTQ0VH6sLnPnzsWcOXMaehlERETkCpWP5ORkPPjgg/jss89gMBga5QJmzZolh2usb+JrEBERUdvVoPAhhlUyMjLQv39/uLm5yTfRVPraa6/J90WFo7y8HHl5eQ6fJ2a7hIWF1fmY7u7u8PHxcXgjIiLXklachk2pm+SR2r4GDbtcdtll2LVrl8NtU6dOlX0d//rXvxAZGQmdToeVK1fKKbbCgQMHkJSUhPj4+Ma9ciIicloVaWkoP34C+pho6Or549NqyaElmJMwBxbVYts4jrvVtm0NCh/e3t7o2bOnw22enp5yTQ/r7dOmTcPMmTMREBAgqxj333+/DB5Dhw5t3CsnIiKnlPfNN8h6+QW4B3miLKsYQQ89Br/rr6/zvqLSYQ0egjiKc7GRHPdyabsafYXTefPmQaPRyMqHmMkyZswYvPXWW439ZYiIyEkrHmU/LEDHKw5DUVSoqoKMHxagYsSIOisgSQVJtuBhJc6TC5MZPtowRVVVFU5ETLX19fWVzafs/yAial1Ovfcawk89BQ1Ov7RYoEFquzlod+cDdVY+xnw7xiGAiKGX5ZOWM3y0Mg15/eautkRE1Gi2HdvmEDyqXmgs2HZse533F0MrosdDBA553+qeDw65tG3cWI6IiBrNFv9gjCtWoLULIJXQYIt/IMbX8zmiuVT0eIihlkjvSAYPF8DKBxERNZpCdwtmVdyBSrXq5UUcH6+YhkL3M4/wi0rHoLBBDB4ugpUPIiJqNOER8XgjxYA15t6I0aTjuCUUaQjEfREmPstkw8oHERE1msmXXwkFqgwcGyzd5VGBRd5OZMXwQUREjSbc14gXJvWWAUQQxxcm9ZG3E1lx2IWIiBrVTYOicHHnYBzPKkFMkAeDB9XC8EFEROeldP1KlCSsgkf8JTAOu8zhY6LSwWoH1Yfhg4iIGizlzhuRuisDJcHt4fH1Xwjv9Q4i3vuKzySdE4YPIiJqcMVjhykMiRM6waA3wVQehQH7i+C/fmWtCghRXRg+iIioQXYsW4WkfmWYVfAptBUWmKHB+/1uxI7fVmNodfgQy6aLfVuifKK4dgfVwtkuRETUIJt1WtxR8BW0qNqPRRynFXyFzdqql5Qlh5bI/Vqm/TZNHsU5kT2GDyIiahhDii14WLnBAtWQKisecxLm2DaKE0dxLm4nsmL4ICKiBjG7VcCsKg63iWXULW4VcqjFfodaQZyLfVuIrNjzQUREDZpWm6HvgVmVgXjebSHcFEvV/i2V06DTh8oeD7EzrX0AEediwzgiK4YPIiJq0LTai3tG459B1zvs35IOf7w6MEg2l86On20behHBQ5yL24msGD6IiKjB02qHexzH2pJYpFkCAagYGJKBq4dMkJ8zMW4ihkUMk0MtouLB4EE1MXwQEVGDp9XeV5CJMeM7YvWRHRjZsQ/+b8TfHT5PBA6GDqoPwwcREdVrp869zmm1D/lch1VHZ8ECCzYf/QKeoSZZ8SA6F5ztQkRE9arQptWaVpulVfBnwGZOp6XzxvBBRET1KjFkwwzHabXHdHrUmGnL6bTUIAwfRERUL5N3AGZV3CGn0wri+HnpJNFj6vhiwum01ADs+SAionrpDKPwldnoMK02DYEYrfHFZvzM6bR0XhRVVWvk15ZVUFAAX19f5Ofnw8fHp6Uvh4jIpR0+kYfLF6yFajf0osCC3/95EbyCTJxOS+f1+s3KBxER1atTtB8e6BON13ackAFEgYoH+sTI2wVOp6XzwfBBRERnNOOWXpgwLBL7juWhW6yfLXgQnS+GDyIiOisROBg6qLG49GwXscXzptRN3OqZiIioGbls5WPJoSW1Nj7i6nxERERNT+OqFQ9r8BDEUZyL24mIqO7fm6wUU2NxycpHUkGSLXhYiXOxAyM7t4mIHLFSTI3NJSsfUT5RcqjFHlfnIyKXkn8KOLam6ngGrBRTi4ePBQsWoHfv3nLxEPEWHx+PX3/91fZxk8mEe++9F4GBgfDy8sKkSZOQnp4OZyOqG6LHwxpArD0frHoQkUvY+gl2/+cq/DDvJXkU5+dTKSZqlmGX9u3b44UXXkBcXBzEwqgff/wxrrnmGmzbtg09evTAjBkz8PPPP+Prr7+Wq5zdd999mDhxItatWwdncDTlBPadPIxu7TvJ5tJhEcO4Oh8RuZb8U/hg4QokBV0DeCrYpvbDpoUr8PeOlwG+7eqtFNsHEFaKqcWXVw8ICMD//vc/XH/99QgODsbixYvl+8L+/fvRrVs3JCQkYOjQoS26vPobv36Ad9PnQ1VUKKqCu0Kn475xf2+0xyciag12LnoTSw5lAIrdcumqiuviQtH7tnvq/Bz2fJDTLK9uNptlhaO4uFgOvyQmJqKiogKjR4+23adr166Iioo6Y/goKyuTb/YX3xQVD2vwEMRRnF+Zchk6REQ3+tcjInJWBw7kAloFWl05jHoTSssNMFfocfBQDnrX8zmsFFNja3D42LVrlwwbor9D9HV899136N69O7Zv3w69Xg8/P8dld0NDQ5GWVv8U1rlz52LOnDloSmKoxRo8rMT5/pNHGD6IyCVUpKWh/PgJBPh2RLhmFe4o+AraCgvM0OB9nxvhrxt7xs8XPXHsi6MWCx9dunSRQUOUVb755htMmTIFq1evPu8LmDVrFmbOnOlQ+YiMjERjCg4MEGlD1BZtt4mhFy93L5w8kAu/ECO8/A2N+jWJiJxF3jffYOvbryMjOBgl/kG4I+QraFHVwyGO0wq+wjch/Vr6MsmFNDh8iOpGp06d5PsDBgzA5s2b8eqrr+Kmm25CeXk58vLyHKofYrZLWFhYvY/n7u4u35rS4ZwTKFZuhae6uCqAqAqKlFuR+NkR7CooEvtDY9RtXdF9eESTXgcRUUtUPL5a9jNShl8i+zy8PPOhLXacveIGC9LztvEfh1rPOh8Wi0X2bIggotPpsHLlStvHDhw4gKSkJDlM05J2Hz2F6w9mYXnSKXyQmi6P4jwzMAkhHgkwalPw56J9KMo1teh1EhE1tt0/f4eUdlG2BtMKFXKoxV4lNNB4lPDJJ+esfIghknHjxskm0sLCQjmzZdWqVVi+fLnscJ02bZocQhEzYESn6/333y+Dx7nOdGkqXjlH8S+3hdBaVESYKuVtz7stxGuacTgVo8fQku+RlzMIx5Ji0Mu/Y4teKxFRY0pOzZHBwweFCEAeKrQVeD/ocozP+QOn9Bq0K7fgp4BL4RsSxCeenDN8ZGRk4G9/+xtSU1Nl2BALjongcfnll8uPz5s3DxqNRi4uJqohY8aMwVtvvYWW1rcoGdoaDaduigUbc/tho6U7VNyAsVGb8H/FouzI8EFEbUd4TG/0P/QxxisroIEKS6GC2yrH4I2oCNswdFlqN9zm69/Sl0oupEHhY+HChWf8uMFgwJtvvinfnImpsiss+E3+h2dVqWpQYtEjXrMHxyxhWHZyMOLb7cOIc3g8sdywWPVPLL7D7m8icmZF3jpb8BAytBrsCd1zep0PRYU+/Dso+ida9kLJpbjExnL5hZ74STfa9h+gGQq+Mw/H9+6zZUXErCqYVXkHNpR1we1nCRgf7v4Q8xLnQYVqW5ZdzIEnInJGxw/8hFF2f3gl6dxgsVtgTFAUFb26BLfA1ZGrconw4ZuejGz/6zDfMxoBSh6y9QZMVz+zDcWIo+gBebfsX6dX81s/BxZYRBsWZg+rChgieLyS+IrtccVyw3MS5shl2lkBISJnkb0jAZl7NyO4+yDsUzWywdQ6tTaqohIaVXUIIAo0GBrXrQWvmFyNS4SPysHDEfnFCpR3uw5m9wp0sWyE1r92D0jPkwdkxePp9XPEyKi8XQQQEUTi/OJkxaMm6wZLDB9E5Aw2zHsEy/M8oCoKlP3LEOZjwqNB9+G/mW/I33NBlSq6lfXDXvftcvkj6x9Y/B1GzcklwofhqiuwODMN9349F2XGQBTGtYfFX2T90wHEAgWZJeFQUw7bgsfpj1mw8cRmOdRSkwIFkd6NuygaEdH5VjyW5XnY+jlEAClSjfj25BCsQxxiNOk4bglF+jF/3NU9GKOvuEX+/mLwoObmEuFjYJA37h4+Cpu690a7zHR0S01CmXq6B0QEj5/U0SjML0ZAnq7WaqjiPLA8vNbOjsKMATP4Hy4ROYXdv34PKEaH2zJKQuSfSWkIRJol0HZ7ha4fBoUNaoGrJGqERcZagwiDHi93iUSOfyB2dO6OP7p1QkVqX8xXp+EjXC+P4rw42B97slNQmjqxKoAIqiLPUwpMsrlUBBBBlCpnDpiJqT2ntuw3R0QEoOTdB5F97CREgVas6RGDZHn0qBTRw7Fqq8CCi2IYPKjluETlQ7g1IhCXBHjjWGkZduwHloeuwO2rUlAY2gGd0w/ho4sjMGrMP7DuwCZMLCrC9KRTOKXXol25GfPLirAvPQv3jr1TNpeKHg+WKonIWVQc3o4vduQiKawT+im7MQGnq7r7Lf+HclNv/GAQuUQMFKu4okSPQV1jWvqyyYW5TPiwVkDEW2y/7ni29G5s6HUUkekHkRx6ETKDOuDpvt2Ru/N93O22qNZqqO+W3SbfL8mrQP6JCgRGVwCeLfwNEZHLK925Czs+fQNJQbHwUYpswUMQx65ui3Bdz3Hw2V2KFGM6IkpDcfNN/bmZJrUolwofDsMwXaPw8AEFGcEd5diTGJYRt3fKTqlzNVRx+6IlPyE9oRAmv3zszDsBz1463P2361vs+yAi15by2CzkbPgJB7oNBTwVGN1KoKl0/P2lgQU/ln6EFf0O2JYP6BsyG93B9Ymo5bhk+Kg5DBNrdJfBQ8jKj4XF4NgMI2fC5MViz8kj+HhUPJAnKiIqrs7ehnc/+RZ3/W1Si30fROS6FY+jhQlYMXJi1ewWFUj1CoQ5T4HWrsfjlFaH37z32c7l8gFcn4hamEs0nNZHBI7h/t624CGU52vwk3q5DByCdSZMeoU7PvXrj8g1+zFy91pE7TyCX0/FYllaPo6eSm7B74KIXFHKxy9hRdBI27TaMN9UzM57UwYP1W632idibhEdpnWuT0TUUly28lGfkgGD4L3JF/PDq1ZDzVH9EJ3qjyM9dbj1wA+Y6/6+HJaxqArmVt6Cd3PGY+mmLXjwOq71QUTNoyItDV+4B9uCh1ZXjjsLvrKtYipuFdtGXFf2NPSWdHluPxgjZu1xfSJqSS5d+ajLsGvG4udIDwxddRjhe73lUZz7qpmY61YVPASNouJxt8W4S7sUBzLSaj2OWCl1U+omeSQiakz7tm9Ahc7Ldm7Um2zBw0r8rvLUlGPbvngMUEacXiagek8qLixGLYmVjxr6BvvA57IJuKtbb7TLzsCpwPEYHRGN3iufrNWIKv7oeMztC7xhcqxpyr1hEubI0iY3nyOixnYsKc02lOJuKES4NhVmETjs7iN27harmYoW017G6zD3ytlcJoCcBsNHHeYPi8P2zFBszijEoBBvGUiW/BANi7F2qUgEki4lBbZzUemwBg+Bm88RUWMLzwG2q8DF3mtxSdFmKKaqYRWLKiobVcHj8cppclVTsa7H1YMHI8zTj9UOchoMH/UQgUO8WZXkabFfdy26uX3v0LslGlLTisJs50kFSbWWYOfmc0TUmLTRXTD+0F+IKN2KzQZ3uVNtmFnUPoDHy6fiD0t/GTzEb6gH+sSgU7Qf/wHIqTB8nCNlxCAU/ByI/Z2ALm7fywqICB7rTbeisFRnu1+UT1StPWDY3EVEjdFkWn78BPQx0dAEVOKIx5/4e1AELIoCjapidlYOJhYVw6BUVAcP4Ma4PMy4pReffHI6DB/naNQNV+GFPS9gQmIIDnZ+AwZdMjJN7RC6Zw36/mOw7X6iiUs0c81ZP8e2oA+bu4joQuR98w0S338DJ6Mi0D4pBUdGxuHN8AAZPARxnBMUgPgSEzabO0Nxy4dWn4Ghcdy/hZwTw8c5EmuBDH3gXix9/U1M+WUByoxBCCrNwsb+vXHPTY4rBZbs0uOqY7cjyT8NUblhKPHWA3FN8c9HRG292lGybRsWbl2DgqGXyCHfoxGdkWQ8ZQseVuL8a21P7PfNhWf4QiiKijnHPgTCZ2NiHFczJefC8NHQVVFnP4Zf+/VF2bZt6NW3V63gIRYcW5p8HMdNgeiQXoGVlkAcST6OYaeS0aEd1wIhonOvdqQ+NRspkUEoiB9l6zUTR0/T6T4zKzEZb6nXEBh8v7XNhOFqpuSsGD7OowIy7carAfFWh+/++gXRGcexuHoxMrHQz6yMOzD312W4ZdgIXNq9W2P8uxFRG694WIPHln79ai5QigiLCQHlw5CjS6hKHaqC0oxxOGqKgEeN3lI2vJMz4iJjjSwr85jDYmTiKHbF3WYJwuTjRZj2yVIU5Zoa+8sSURuS88kn+HXipVgTPwqlBj/H5UnF6qX6Cpw4Mh6m9LFQxccUFe4hv0DrcaTWfdnwTs6I4aORdSxIq3NX3KtPrEDUmn1YXuyFZ9/4HHvXpTT2lyaiNlL1OLZnCfK1Yo2OagrgoxYiBsnyGFDYF4pbIdxDf7WusC6P+qA/HfZx4Wqm5Kw47NLI9AVhsBgVaOz+/BB/mTylW4R/uy3C+6euwgeVY3F8yxIs7H4HvPwNjX0JRNSKlW5YgY3BA2yhQuiH3ZigrJC/V1QoOFFxL+Z5ejrcR7A/FzPtFo1bhF7BnGpLzoeVj0ZWWuGO/ZW32XbFFcHD+gtBqwB3u/2M9e4PyL6QT//4srG/PBG1YtkLF2Lz5+8j1zPIdpsPCjEBVcFDELNYovRvIb685IyPJZpNTWYO8ZJzYvhoZL0uvwjFB9vjSOkCJGJ8rb9M7PtAkk/ub+wvT0StOHhsWvst/uo60vYXiwgeHTXHZPBI02qxyeAujxpYMNy9sFZ/hz32epAzY/hoZMPGjcaWgBQUrfoEFQeCbRUQwf6Xh+gD6Zyf1dhfnohaodKdu/DXrz9gY9QgW/AQQy3TsRDXWFbiWy9PjImMwLTwUHn82ssLKW7umNX3CTm8IijV/xPY60HOjj0fTeCfb7+G9b+uwOafv4Mu9wpEBK3AYh8vfOLrY1sK+cmsXJQnuzfFlyeiVraex5Hn52L7hPEOFQ/rUIv4Y+WZIMfVTJ8JDsSNFXm4te9NuDRupG23WsH6vlhtmchZMXw0YQVEvP39iUuxObKdQwe6+OXxbFAA/uEe01RfnohaScUj9cmncLxnnEO3aD/NLmjEFrVis0qdW63VTMV4i2941f4tImTYBw2GDmoNOOzShNKK05DYOdsheFhZFGDgDSOb8ssTkZNXPI7fdBOOxsZgV/e+ttsjkIaLLRtt52LHWlEttaeqCvKL9M16vUSNieGjCSUVJDnsbuvwxCsaW5mUiFxzBdNsPz9sHuTY53EHPofWrkdMEDvWWpcPEsGjLHUiKsqiWvJbILogHHZpQlE+UTJk1AwgbAYjct1hlpLERCgGdxyNiXYIHqf7PIAlXp5yl1ox3KKoKh7MyYPh8D+QrTfDUh4EVHpj6NB2Lf3tEDVP5WPu3LkYNGgQvL29ERISgmuvvRYHDhxwuI/JZMK9996LwMBAeHl5YdKkSUhPT4crEmOvs+Nny7AhiE7023vcjuWTlnOXSSIXk/LYLBy/8UZk/Pe/OPbi/xyChzAE22wNptbgIaiKgvkB/sj1OQFzSUeg0gejKky4uG/3FvxuiJqx8rF69WoZLEQAqaysxOOPP44rrrgCe/fuhaenp7zPjBkz8PPPP+Prr7+Gr68v7rvvPkycOBHr1q2DKxJbWQ+LGMYOdCIXr3jkf/+97TwlOswheIiqRzwS628wVQD3kGXoWxyOftldcO0tw7k6MrlO+Fi2bJnD+UcffSQrIImJibj44ouRn5+PhQsXYvHixbj00kvlfT788EN069YNGzZswNChQ+GKanajE5FrEUMtQra/PzKDg5Hvb3RY/jhImwmN+XSDqRhqERUPe2Jl04FdAnDvuEsZPMi1ez5E2BACAgLkUYSQiooKjB492nafrl27IioqCgkJCXWGj7KyMvlmVVBQcCGX1GY9tvANbC0woL+PCS9Mu6+lL4eIzrGxtPz4CehjYrBx8CAcj42tChyqihCLLzI1Beir7MYE8++2zwkzmzEjJw+vBPg5btaiKrjhkksYPMi1Z7tYLBZMnz4dw4cPR8+ePeVtaWlp0Ov18PPzc7hvaGio/Fh9fSRieMb6FhnJGSA19X3pa+w4VoAuxX/IY88Xl5zvPxsRNeNU2sOXXoak22/HjieewLEOHU6HCUVBhqYA15Z5YQJ+d/hFLHo+upVVoENORzmzRRDHGz3vQYeIaP77kWtXPkTvx+7du7F27doLuoBZs2Zh5syZDpUPBhDHiseVlq/xc6dsnJSro+7EpAw/DHnZgo0PXX9Bzz0RNe1UWliqZrod6Nmx1nI//ZTd6G1Y4XC7/SwXVT2GsvSxsJS1x21RPfDkDZfxn4tcu/Ihmkh/+ukn/Pnnn2jfvr3t9rCwMJSXlyMvL8/h/mK2i/hYXdzd3eHj4+PwRqdl5ibh55Bsh6WVvw/JwzUF3+DrxCN8qoickBhqEcGjxGhEekgICt0NDpvAWafVKnY31pzlIno83EOX4f86hGLO7Qwe5MLhQ1VVGTy+++47/PHHH4gV45d2BgwYAJ1Oh5UrV9puE1Nxk5KSEB8f33hX7UJ6YEetzndxfqnhD6z9/fMWuy4iqrviUbxhIzQeHjjasQOWXj0Bqy4dhTz/MIRYfGwBxOhWIqfV2qtrlosIIH17hvCpJtcedhFDLWImyw8//CDX+rD2cYheDaPRKI/Tpk2TwyiiCVVUMe6//34ZPFx1psuFCs7ygiZIdfilJJZajqmsxBBlX4teGxGdljF/PrLffke+X+LpiS3jr3Lo8RDNpRebFWR77AQ0PWCu1CBTq8jQYbRYkKVoq8KJXf5QVA2GxnXj00yuHT4WLFggj5dcconD7WI67e233y7fnzdvHjQajVxcTMxiGTNmDN56663GvGaXMnb8dJRvuRuv+mlsU+/E76e1RiOUArEIMxG1tJMPTkfh8uW280JPz1pTZX29f8KMIFP1ztYHsKI0GhuMlVV/WFRPu5VbuFTPwBXB4+nhszlNn9okRRVjKU5ENJyKCoqYxsv+jyrL7r4Zj8TvcfiLSKMCS0Z/hY7tq/4q+mrtOvx5KBWj4sJx44jhLfOPR+SCCv9chZP//KfDOh7QmrG990Bb4cNHewIfR252HFaxW+ejLjMHzMTUnlOb/PqJWuL1m3u7tAJ77hgE7N1Ta1fcHLcidBTVkVe/RG5qGTpo0vDygRJ8kHgSyx68qcWul8iVptOmPvGkfH/1iOFIa9fOto6HV6U3StwKxfIcMBvT61i1tP7gIczfOh/jYsex8kFtEne1dXJpxWn4ZO8ntW4X+8SIXXFFxaN3xmasd38An+ufk0dxLm4noiaeTlsdPNZcNOJ08JD/gSoodivE5WW9cWV5f2j1Y2SvloOzFJ3FhpTJhclNdv1ELYnhw8klFSTBAsddcYUpPabIv4jW/rUSc93eh7Z6v21xfN5tobydiJpO9qeL5PFodBRSIyJqVzIUoEybglgkIyS3Pa7KCLQFEHHsU+Blt4hY7ccXG1KKPzCI2iIOuzi5KJ8o+UtI/BVkJc4nd5ss3x9mOSADh1gjQHTNi30hxPLMQ80HW/Cqidp4xePfT6B43TrHJdNr6KfuRlf3P3FMr8VlFWbszJ+KkBIDQj13Ib24F9aa4jEC69Ap1Bfdo+KQrDsmq5ziv3Xx37jYEZt7QlFbxfDh5MQvH/FLaE7CnDp/KblV6PGNlxeeDfKv7qJX8WRWLvRFupa+dKI2J3vhQmT876Wq9/396w0ePmohKrwTMDYo3Pbf5VVF3yLNyxtpiipXL9WlatHPpxAP3fS87fPEHxViqEVUPBg8qC3jbJdW1PtR1y8lcfsVX18um9rsZ8Isv+F3/vIiaqLgIWwcOADHO3VCibYERboieFV4wcPsIafKttdn4/V2f555douq4JUOj+Lyi2/jvxO1CZzt0gaJwFHXX0KiJ8Q+eFhnwoigUvP+KaZyHC0tQwejOyIM+qa+ZKI2NdRiDR5iyfRNgwYhPTwMx7yOYWvQ1qpp8CrQM6cnBhR3RbJHyNlntygq/Dp3acbvgsh5cNiljfaE1GxUW5ySjU+//xPt0/NwMtQPE666CPdEh7bAFRO1PhkvvyyPO3v2xL4e3WWQEBUPW/AQFGB3wG7sDtwNRfxFUGO10prnbCglV8bZLm2kJ0T8IhPqalQTFY8/X1uC3KRiFFXkyOOmN5firRPpLXjlRK1n2fSCpT9hXfxQW/AQst2zHcOFUH2uir4OcWKdxaIqqMjvL6fIC2woJVfHykcbMDFuIoZFDKu3Ue3zl9+GvzYJS9w/xEm9FpHlZnxQdgM2vQnE//sW9PP1bLFrJ3L2Pg+xX4toLj0ZFeUwdFKuKT/j54pN4UpP3gLV7AW1PAD/1/lXXDPodZj9PdhQSi6P4aON94QIJaYUDPb/HOPsOu+fyvoRyAUWvGHBJVMn4daIwGa/ZiJnVrpzl63PY49dxUMQvR7bg7af+QFUFVMjEuDnXY5gYxa8K69G/24jm/qyiVoFhg8XoPM7hWcDq6biCuL4TFAAfi35Cq9rvJD12gf4YcYdGOTryUZUcnliBtnenz6F/uUPEVjd55EqVi+tVqvX4wzcsozoH3sL2kX1RlSHni7/3BJZMXy4gJJAj1qd9+L8lN4Nz5W+j9eCJmH3gs/wVodYTBk9lFUQcllLDi3BnPVz5KrCyj81uHpbCNz8HaseYlrtuQQP8TmZ3gbEX3Jrk14zUWvEhlMXcM8Nc8SsPgdi6CWyohIaRcV9hd9C57kTQ0+tQcrbn8sGVSJXrHjIxfyqtzNQNQp+7J+FErdSh/uJ9TxqzW+vg/hvrk+Ie5NdL1FrxvDhAkQvyGN9n7AFEBE8ZmflyGXYhSw3DS4xf4+7K96Hm882rFj4dcteMFFL7aNkN2XdOmslxz0HGYYMOdxSrOqQXRmIIQVuZ94YTgVuDChDv47XNf2FE7VCHHZxEbf2vQmR+V3x49YXMLPwN4RbKuXtS7w8MScowNaI+kTWMiRniJHuqr1jiFxFRJkRikWVFQ8bFdgYstG2iFiH/FA8W7Qd/+cTVnvRMFXF9QWFyDJ0w8jow9CVxKJnnyub/fsgag1Y+XAhF43sgzsufgBLjFfilEaHZR5GW/AQxPE/Qf7I98xq6Uslanb+aSW4+1fL6SFK69FuEbGjfumY3C6s9uqlAP6XnoV+RV1xTcdDcK/ohGuu+73Zrp2otWHlw8X4do3FkR25GKcJr3PYWvxSLdU5jnFzWXZyBfqYaESUd8GYpE4o0ZfApDFhU+im2ncUwaPGPi2iarjXdyB6x+2Dp8e9GH3ZzOa9eKJWhuHDxTr5n17/NFRt/WPV4q++q4fd5LAs+1M7D8KnpAgFHl54pndnzoahNkns2bKjbx94WgBPk6fs8ai1RLpV9TCldbhyYpEPenfbh4rygYhn8CA6K4YPF+vkF4s+nyl43FzaESOGXGureLz751+YfGi7dcgb72an45JJV3I9EGqV/w2IplKxH1JdC/IdSnPcbqC4IgimjHEwhPxaO4CoKoaXlGJ4oTfyYvzQJTIZ7SLmolu3G5v4uyBqGxg+XLiT30rsNzFVexEGxV1uCx7C5xt345KD26HT5yIEmchAsDz/fGMUHhrZvxmvnqgR1u8Q02hVi21fFbEtgb1snJ4Wuw2e2O2mx83FWeiYk4t5AX6ODaaKgr88jBjWrgBdvYvQretcREQweBCdK4YPF979VqjvF7FgWboUg3zX48ryjbZh7l/chmDHL1q8ExSCISE+6Bvs04zfBdEFrN9R/bMvjuJc7IdkXwGJSlegLe+K53wTkBm6BB6Kip/ktHQL/pGXh7f9/R0fWFFwUOmG64e9D4MhnP80RA3A2S4uuvutqHbc3uN2LJ+0vM7gIXga8nFl5Uaku2mxyeAuj+Lcy5CPdw7sw6Qtu3HL2v1clIycWl1VP3EuNmK0d3TzDvylcUNm6J9yUzh5P0WRM8J6iIX36ljXI6j9GAYPovPAyocLOdvutzXpDWn4zuC4DohYnExfkYard66TPSA72nfExUW5mNmtA+6JDm2274XoQqp+GosK3zW7gBsH2W7bcTwRi4IDZMXDnvjZF/s+X15cgt89PRyGXxbuXogbu9x41v+WiMgRKx8uRvySHBQ26Jx+WR7wNtRaB0ScHzUaofPOhmooRd+TRzB5w3J8tTYBb51wbNgjcgbiZ/2B411k4BDE8a5fLTA//TIq0tJs98vRFeBm83YZsuvaiuDGQrGni3LWCgoRnR0rH1QvL/eqqYT2xPnE0h8xtMwEswp8pR+FI+VdZSPqywEhiNPpcHlEAJ9VchqlO3dh2OJd6OINHA9zh2eFN6LSiwFLKcpPJEEXVhXEfWIL8ETWEgzI8qhV7Qsxm3E8/mFoUr50rKAoGllFJKKGYfigel3e9wZ8e2CLw+Rc8cs4prJCvq9VgFsq/oQFq/C5ZQz+sd6IfwAYfDQALw/swOm45BRTaEsSE+XxVHRP7OwdgyJ9MbzKPTFyw150io6y3a+fZz402WLNjmIMKzVhh7te/uz3KSvH4ehBGDr6ccw+1LXWrBkOuRA1HMMH1WvY0Ctx6+ZP8bn3LvlXoBgKt9+QzkrsjHurZhl+ggm3bSzF6s590b/ChOkRQXisS3s+w9SiU2g9BgzAuvih+KuHBVuDltn2aUn36I8RRiN8q+/Xq++tsCT9Jsei1xtPDzmKn/unh9+DzufRN0VEdVNU9UxbMza/goIC+Pr6Ij8/Hz4+nMbpDNZv+AX7D65DQW4K7s/5DtoaDXlWIpK8of4fchGExHYdsTeyIy7zCcS7w7s0+zWTa1Q8xnw7ptYwiJjBZR8KTp48iVc/fA3LIh0XC1NUBf/t/ibGDb7Idtu2d8chLH0jxkZGOAw51vW4RHT+r99sOKWz6tCrP3pefg38o/rjM+/LUGm3KUyatmoarjhqAdyPRYiwHMeAlCO4bcNvOHh8H7ZnFvBZpmabQrt5y1Isf/Mx7P/sHdlQ+lviXhTrRLOo4+erioqNx4463Nbvrl+xeeS/a/c6sbGUqFFx2IXOuawt1ga5vrwTvvbvhSOV3ojQ7MSrAX4OjXlivHya5jt8g0txSukgG1HXRURB6ROMLdlHMDCwI/oEsEGPmmYKrSjK/fvoq1C9FCjlKu5+4FUkjnwWWaWxUNW1tvU7BFVVUHm89sYtgwdMhub4J2wsJWpCHHahBpW1q35oxF+N1Zu92P3uFgFkeXKKrSdEfNZSXI7vA0Ow2WctlOqdZeI73IX3Lrqfzzw1cjiu2nNFtataKBZg6J7b8ZtbF+h8t8A9fIkMICJ4lKVOxHVFEXjxubvO+LhnWgWYiJpp2GXNmjWYMGECIiIioCgKvv/+e4ePixaSp556CuHh4TAajRg9ejQOHTrU0C9DTrwfjG3UpcYfjaICkqxzc/jhGqT9A1u8/5LBw/opCUffxZ1/vd60F08uQQSCReMW4T7dFfj7r2aH4CGoGuBU2Hb5k1eRPwgeh+9GTNLl8liePwj+3gfqfVzR4/HBmA/OuAowEZ2fBoeP4uJi9OnTB2+++WadH3/xxRfx2muv4e2338bGjRvh6emJMWPGwGQyneclUkuXtc+VdTEmeydFGKkRUqoCyHtYmXassS6VXNSHuz/E5F8m442K3/DBWG1VNa6GpKAdUNzyMVP7JTZoH8dS80IkaP+NG4LWAd45jbIgHxE1cfgYN24c/vOf/+C6666r9TFR9Zg/fz6eeOIJXHPNNejduzc++eQTpKSk1KqQUOvcD0b8rz4P5uTVmobbrtxc5wuCqITcvm0DFqdkN/6Fk8sEj1e2vFI9mGdXkatJUTHD+DZucP8JW4zVzdGKiheKFqBzWUmzXjMRNcFsl2PHjiEtLU0OtViJ8Z8hQ4YgISGhzs8pKyuT40T2b+Q87MvPv13/G54e9nS9AeQv9/GwqIptBkyKxg2vlk3FqLyQWptyiWmO3U/k4KH9SbaN6USPyabUTfJIdCbiZ0QEj1o/inX9aKqAl/EoxkZFYFp4KMZERmCJlyfcYMHIPsP4RBO19tkuIngIoaGOG4yJc+vHapo7dy7mzJnTmJdBTVABsZaeRRiJ84uTpW7rX5zWMBHhdgL9I/8Os9vv1Qs5KYgrzsI3mYn42OKF+XYzYy7O94Ex/zg6ZxRhWYAXMrEZi3f8FxawwY/OvoLpoe2r6g4aNYjM2z2nPV4NUGvtURRfYkL4wOv5dBO1gBZf52PWrFmyM9b6lpzMTZqcXa/gXrICYj8cI/7vR8NJmN1+O/2ioKg47JmIDK0GUwsK5UyY2/PyZWRZ5VeAZVHLUOCxFwv2bcanO16QwUMQTa5ipgErIK5N/Pu/vOVlXPHNFZj22zQ580rMQlm7eS3eWb+uzuG8msGjLH0ccop617lH0YGOlwK+7Zr2myCipq98hFVv0JSeni5nu1iJ8759+9b5Oe7u7vKNWhfrMtM7MnbgkTWPnK6C1JxtoADH3dxsvSCf+PrYZiSIj20L2opuKT7YG6TWuagTm/1ck5zqun6OLZBafyZmr5tddRJSNZwiAkaNHzmbyJSLkFvQE4fd3OCj/uLQEyKqb12vfaOpvw0iao7KR2xsrAwgK1eutN0mejjErJf4+PjG/FLkBEQw8Df4Owy/1GWp6Sr5IpGkc6v1F6h4QfCtyK/1V6wKDfR6zjJw1YrH0+ufdggeNuLHp46p3jU3iRDh4kPzF1jn/gAmhe9H78p+0FR/gjjOHv4Mgy1Ra6p8FBUV4fDhww5Nptu3b0dAQACioqIwffp0ORsmLi5OhpEnn3xSrgly7bXXNva1k5OuMmlvtF88DgZ2xnPpBkwr/0q+KDjsmaGqCFP16J/VH9uCtsklr8WrSlHAVCSVeKBPQDN+M+QUPtv72VkDrZX1R0keq8sg1tV2ZbVNAf6b+TpWaUeg58zfuCEcUWsNH1u2bMGoUaNs5zNnzpTHKVOm4KOPPsKjjz4q1wK56667kJeXhxEjRmDZsmUwGAyNe+XkVNNxxV+qNV8wRCi5OmIasP4NLGjXG3elPIUns/6LZ6t3CxUvEk9k5WKv0YghJRWIThmGbI0WnuUeKD1ugiUkC2gf2GLfGzVfA6n19u0Z2/Hx3o/r/Dz503WGYRbxgX/m5mJiYYnDlG8xqyVaV+bQOE1ELYvLq1OjEC8cn+37DJ/s/cRhSeorgq7Ex7PWobvfR8gPT8Na82DMKHoHKXoNIsotWOc2HJPKVst1F8wqsELXD3sqB6BQ9cZFBl9cOmuGnIq7Ob9Yfp1Bvp6IMOj5r9ZK1bdseVWPhxhqqbviUV7UB2WpV0LnedC2RHpdXkrPxJiSUofbREEkfdy7CBt6U5N8T0TU8OXVGT6o0UOIaBSN9I60/ZW59bcTSFhyGP7uO5AzZDs+9LgJMfmnUKLV46dD02XwsCd26ViK0SjJjIH671mYt2YDYtNTcCy0HZJDI/DPyGDc2T7YFkLq+0uazu3fq7meu7r2CtJAg2e73YEn9r1b70CL2Iel+PBjVffXZyHAUoru+u3YGrHLoQyiqCp+s9tbqOpzgaTSSES/uLsJvzMiamj44K621KjqKm33vyJatvqtX6Kg0/bD+Cp4Bg77RSI3z6NW8BA0UDEeK/Ct7z/w13vvYZSSjSJ9MWIOHoRumx57ImJwbWgErogIRbT7Dryx/7Wq9UGgwexh3ADsXF3o5mkNDS517RUk/t2WpnxzxuAhNoBzkxWPb2XWMKkqrsnKwTVZkOt12O+qLIKHdQaMOBaaIhg8iJwQKx/UbNKP5+ObF7bA6JYKb30S9gRr8KTphToDiPCpMgErPLXYGrS1etEyyMbU2KJY+cpSXJaHX7v94bDWlFhzRKzEygrIeVQhFI1czfZcnrvzCS5rtt+P+3b8CdXuX0zMairxGQ+Pgh8d/h1FcDCduhWWCn9odLkwtFvs2Ouhqvg9OUW+KzYzFHsKWYNHyql28O01GNo+42C8hEMtRG1iV1ui8xUa44tR/9cNJnMEMkqGIqJQh38F349TGje5HLtYlt2qUtUg0c14OngI1euCBGqPwEcpQo5/Ua1FLkXT69ZDq1G8YSMq6llVl+qpQlSvrXIuwcUaPGouCldzifz8/B04cPBZrFrdG5vT98Ld0MtW5RBBobT8cnjkL60VPMTiYIqmDJ4xb8HYvkbwkD8LClYbDTJwDDKV2YJHQZIBAQ98Ca8HP2LwIHJiHHahZtV9eASiugcgP6MUB5OycOjY7xgbLVaZVG2l86sLS/F45TRk+JXVWkJbrAsyVLccA8xleF3pgU11fI1ln3yD7GNBCM45jqH3T4PniBEo/ONPVGZlwvuSUTD27gVXV9cUaXEuenXONpxSX3B5d+e7+PbQt7ZqyGCvXjiaNQxuig8yjFeiovJnKEpGVRGremjEqFteK1jIIZNKPxjbfX7GJdSztNrTQywWIHOvJ8rb3Yj2/PclcnocdqEW8+fmjXhwzx2Ou5GqCgyH70Zq59544tRiLAj/y3FhMlXF9QVFuDu/agPCyyMjHOdeqsDt22fBXzGgoswdSt4GDNqzAlkBPVBiDIFHaQZCOgfDc+wEBHaPhH+XyEb/vlpLA2xdQydCfcMp1u/L6GbEbb/eVu/aLvb9GiXH76l72OQsn3dZajT+iDh+pjvhs1NpCN1khClXh/JCN/hOuQch06c34BkgosbE2S7UKqw+tBb3rf9nrdsLfB/Bc6nLoJgUlKur8UqAX63FHUSVZHpOHuYF+NmWaxdE+8jYpHHwsHjI96OK42ApCkWATiuDSY5Zhcly+s/lPpG5COvRrtGCiNjmfV7iPDn8cz5NnE3hTGHIfnaSUFcfyKJxi/Ddoe/wzaFvbN/X+A7jsfTIT1DrWoXUzpmWP6/7E4C4ggC8mbdL7kJbM3haO0mvLirGPUvNKEj2RsTLL8OjX1/oqrd3IKKWwfBBrYJ44bvimzEOL2AaFfgozYT0nMuxqkMMrs19CXdGOO6SfPq+jqulWt2S2gkRpgDkwE+uFzLBNACZ2kKYlHJEVgYix+SFbDNQVCOIDOlegoEPXo2iXBNSj+TLxwrv6Asvf8M5B49XEl9xvMYGNHE2hYY0hopeDbGB2zlRFZRmXQRj8Joz3KeeLe7PQvy7ik0IX/f3xY9enrbAcVFJKeJNJvQpLUOnFODEHyEIf/YZ+F3PnWmJnAGn2lKrIF6Qnx4227aBmKIquOrkIGxLG4VSczgivLbhs9QboVFX1RkyxG1ibQf7yod44ZpasRrhMFetF6KMxo8GINs9G9mGbASWBqKPLg69LdHwMRux3VyEE7pMuJcEY+Neb+Db3dj4e4bt8Yp0OfAIPY4R7XzRvswCtbyszr4REaRExaPWNaoWufjaQwMfQnOrrzFUbAhYVwUkx5QjpyvXuadKTYoKY1DjBw95nYqCHe56/GQNHvLrKVjnYcQjBwug3e6FE2lGhD3/HPwmXnd+X4SIWhQbTskpdsf9a/02HPupFJ5lfjApwKjbuuKEWzISU81wSx2L8vDltV7MrEMv8wP8HNZ6CK9eZMq6XsjikEzs9cyxTdc9WXQAJ7OG4bjXMWyt3k9GBJ/rTtyMjb8NgUGjQGvIQ0LYaqwP/UN+3nuqgiv3x+OifV4I/PRhtB/WDf633AJ9TLQs94thjfr2I/l4z8eY3G1ys1c/zjSjxf5a7KsjQvXTdPYAoTTWWIsj8e8ovnTNwCnOE/cGo0eaCmPfvvBn8CBqtRg+qMWJF8IbLh+HooEmOQvGN8QohzoMpy7D7VlP4qOgblh9eBb0AWvlm1xaW1XQMb0vppT8iHHFJQ5rPdjbq3fDXs9su7+ggcNeKbii6HcsCSqwNbuKAPJ99JeYVdgVRYEH8Ur44upN7k5//Je+CfC3PA2vsiuRlLwV7ac/B6MpG1H3/x3BnSPqrRqIl9KaL/gtOaMluzTbNhV2XcIczDm5tkbTb/X3fQEB4nxZA2QfU3ntTQgtKmKi+6D9rLvhPeqSZr82Imo8DB/kNETgsO+v0Bd5YW/+33BH5Td40P9rJFZ0BdIj8aW+Hw5nd8fWSl88pg3F83hPho5KtWpp9gw3DZJ0boiqqMRWd/faL6KKgs1+6VAVo8PNFsUCi98JvFYjeFiJ2yJ8s6Fk+yEzuD8yQwbIXpFTXy+HrjIRt7Xzxaejch1fyKvtztqNQWGDHG5LzS/FsaxilJZXYvnudGQUmtA1zBvtAjzgZ9QhKsADxeVmxAZ5ItzX8VobsumftaohFmATx0fWPCLfv7OoEIMLi6GGO/bU2A9jnVdVw7q8aAPCiwgaL2ZkoU9ZOUIrzEjf4Y37ksrxxnA9LBpR1QKe6PUQBk6des6PSUTOi+GDnJZfSNUL7s7i6+FZegmC3NJQYdHjXk05DhlKkORZhu81F2N1cW/EatJxzBKKDv6/YG/obtswzLS8/DpfCLcYDLX6RUTeKFMq6gwekgrsC/wZN1mugakkFscrVBSZNTgRPVY+voc6Cf9YtR4JnROwPSLZYVhiXuJ8jIsdZ6t+fLk5CbOW7ILod7W36mDW6etxy5d7mVjKg3BZj1DcNSIWgyM7YFfKfuzMOIbeIbE4dOgrFOUkwKx6QIc8mOEFT79u8E9NQ2jWcSgBEehT4onOlSq+9Cq0PQ+iGvO+pyd6FRbX2Tej1hFCxP1GF5dghafHWQPKnXn5eN/Pt+p+dTz/4rHkXav/nR7aXYT+x7SoNHnicIoBxu5xuPGiy3GJhwFZPTogNrqPU09bJqKG4Tof5NT2rkvBn4v2Vzch1G17WBL2uhvRsTID2yLfq0oR1eQLq3yn9ovl7Xn5+MTXxxZUnszKhaZ0EGZHHj7jX+3iMR9N744x+SNRYQ7HgTIzoKQirTwMZRYVRQF/4Z3Oq2t93v+GL8DYTiNkxWPYC3/YRjfqovPdbNu91bq/SXn+IHToWYL9ATFQSixQPTS47dQChBQch141Iq6sHIU6PQKVk+hhSpa9L9Yvsdngjmk1Khy258cuIFiHPQTrviniPlPyCzC5oEhWmFK1Wiz28cLHvj71hpD3UzOQUt4DuYoR+jIvbPA2Y03oQRnsxPDJzJ3FGFZaiqOV7vA54A6/TC00Xu7w7BmDwLvuh3HYZfU/OUTklDjbhdrciqhpYuqrAngHGnA4MQPbVyTbAkmftCj0BXDSJw/balQt6ntxFC+yVxSXyBdU+36R//mL5dx9z3hN4jH/F7oXQcoa2ZtwicFiCwniotLdtHhPdVyjQjS0rvl+F8Y+PAIJJ07I1/peOIzB2gMwm1X0NOxEnq4c2WUxSEMgVoSvs1VOxGMbwpfAvzgYR3fHYCDWy8/z912D90IAS1BVQIgvNSHBaKiuJkTIEDGxqFg+htFiqVXhgP15dfBYlJKGXuUV8qb4EhNO6t3QvrwS4RYzRDfNR8ET8Hm7KzEgew/+nnMEew1HkOBhcqjyiH+C38qvhzEnHnEH90JXUYjrTdm41mJEnmchwnJVBBYaUAkDYiMi4H3l5fC76iquPEvkQjjsQk5P9IF0Gmhw2CMmPF6P/ScPo2v7TlCyPJC0OxuDO4fh5z1vnXXhK/GiK4LBbRFhDi/QKRo3fOp75s2QrMTnPxIabKsUiMeQjbCi18JsxviiYoc1KkbnaxGQVFVRyE7ZipfcFmO4+3oZfHbr9XjWNmNnH/4mVm+tGYAUFa95PgNtSSQGag7JgDMm5HTAESFivcfpvhBxu6hcDCs1Yb3RIN+3r3DUCiLVn2PSaORdVmn74K2osQgqP4osj44we4bhmCECSpmKS/duh75cDPJ0RURBV0QEJyDFM8U2TSY6Pw6+qeNQlL0XJXnbEV6chdCyQvk1or2ioB/YEd4XXyybRrkwGJFrYvigVsdh4az91Qtn3Vq1cNbTvqcbLMXsEzHoYj8Ftuq1V6n1Ah1UqWKu5gqoyu4GXYt4jKeDAuBhsaBvWbkMHmKDvKU11qhY4VuJx3VVj935xDZk+m3D2KDq8GDXEyHOxVBQXX0YMZUVCNUckncVDbV1rX1S13oZ1uET67WIx5qXloEZYSE1qjPArxiLJzqPxgGfHHjlfADFXQUqNyEkZwyG5uYhtKhq8TWrEm3J6eAhHwQ44XsYXxmScbOvBSOihsMt2BsevTtzFVIismH4oFblbAtnWdcNsS4Zvj5lvd39FVt1wkq8+N6Jqcgo6wPPjhbAsrvBi2OpNaogIojUrCqI81PBp+T76aYMPBfk7xAIal5TzX4U8bhyGrECGW5yNJp6V3i1EgEmV6Opc72MR8sfgKbSBIvbjzJ1iK3tC32m4K3oS6GpyEZg6kvW1T7kxzO1yzGgdCwAD7tvCigVgzG1Nv9T8fDlXrj2ytpL5xMRCQwf1Kqcy8JZ4mh93z6MGLSGWhuiKdCgY/xw3NOhF8Z1iMMli/cgu3zZea3Oaa2kzKpu2Kxpi3sneVxn9j5z1UJVkanRyv4LMQxiv37Jt16eeMauEbTeKa3VlZPnggJqLRYmelNS+g+BxSMIGvOl0Famw+wWKj+nXdpfiM08iAO+NXtnVBTriuFh9pCPZyxuD/eSCGTri4HI3x2afMVaIkNHXt7Qp4+IXAjDB7WZreDrYx9G7Ne9qGuvk1W3/g//Wh2CX459cvoFW7zG1yxQ1LP6pwgFuyq7Q1HTak3jhTFEvh9cqAECzrBCqKLgZx8v/OLtiaezcjCwtEze95RGhzlBgXYLo4khmzquxf5iFaVqVo1aVfVRVQ2KAqfC4hGIoUf3wK2yAtlGC8wVnyDXLRHlCnDAp/ZjioZZz3JPaMt8cLi0I45q9cj1NCOqvBBD88dik99yucCa9TnltFgiOhNOtaVWpyGbpdXFfifX+l4kxX12ZO5AeXkuYt0t+O7IBnyV9Kd8QRavyfEenigxt8N208FaVYXiw48hwHPr6SXhVaAi/1rcZ8zCfXctwLzXZuCtYl8Ywr89a4VFhJZ2h2+Br0WHI4YymKK/xPko1E+G2aM9zMZw9ErNQa9TR+BVbsIxucS8mOFT4xOshQzx/aoK+p68HGU5Q7DHzRPFiopbDdm4+7ZRiIqLOufnlIjatoKCAvj6+iI/Px8+Pmdu3mflg1qdmn0dDX2xs6+EnMt9RNj55uTq6uChYMaAGRgfOhyZmXuw2ZSKV3a9DQtU+bFh5ZGI6fYtiis9sb30GqSYDegbcBLjo37D4EHfysfzMoehMr87Si3uMLZffMbrEFWOQ0ZANbtDtXjDo7qC0VBd8k8hODsAYQWbZegQSjQldQcPQQH6HBgEraUjQtLyocs1QWdehQl9O+PiKRNtoaOu54uI6GwYPqhVaq4Xu5oNrqI1c/7W+Rg3aRy6d78O3QGM6TLRFoSMlUb8ufYHJJ/4ExMN2+FuMCErIxpK1GNo3769fAxL5GCMrfgDvxV0kZWSM4UJMWRiaPeFbR2Ryvx+cPPd1uAAEllQivYlKXYPDBRZNGesvFw/rCcuygqEfsxwaDw8oI+O4tRYImoUDB9EjdjgKlx71d8A/A0nT55EcnIyRl4caQsewpCRA5B28BBuct+PIwW9sddnZ9X4it1Qh+P+bqrtqPPdhqGnRuOkMR0nA3fWfdF19IAElAXY3hfNosc9T2FtzM/1ft+iijN01K0IZDWDiJoAwwdRIze4WonAYR86rPoG++CjS0Zh+/EjmHAwENF5Ecg2ZGNTyCbHJk/b/7OjqNBrytCruB1OBojQUscXru4zsR6Hpg9Dx9wBMFmATVHBCCrbgHUx39Rb9WDTKBE1NYYPogbsDNtYL8zzh8Vhe1woXjMakFncA91P/VVH0Khq9rTf6E6janBz8SVwt7gjOtUb74R/VecuuuJze+dchHL9UBz180XEkX3wLMvE3Rs2oOiaPvhDU/tTnhjyBDr4dWDTKBE1OYYPoiZucK2PqIB8cN0lSEo+hX//cKSqu7RG0JiacQ0+DPkBFsUiz/+5MRr5h79CWlQvRG/ehSs7j8H+Lr444vNVrf1V9AVBuGznr4jfs736RgUhDz+EipuvgubbMbWqOSMjR7JplIiaBafaEjmJr7d8iP/smSdnzmhUBXev8sPow/7I0pcgLVBBaHohYoddgvwffwQsjn0of/RW8O44DSwasSIq8LDhGow+oIfXxRfD0K0ryk8kOTSMXuh0ZSKiC5lqy/BB5ETs18sILIQMDRqjEZbSUlt4qEhLs4UJ0779KFqzBrqoKGQqRciOC0anvudWweDaHETUmBg+iIiIyGnDRx1tZ0RERERNh+GDiIiI2kb4ePPNNxETEwODwYAhQ4Zg06ZNTfWliIiIyNXDx5dffomZM2di9uzZ2Lp1K/r06YMxY8YgIyOjKb4cERERuXr4eOWVV3DnnXdi6tSp6N69O95++214eHjggw8+aIovR0RERK4cPsrLy5GYmIjRo0ef/iIajTxPSEiodf+ysjLZIWv/RkRERG1Xo4ePrKwsmM1mhIaGOtwuztPS0mrdf+7cuXJqjvUtMvLse2YQERFR69Xis11mzZol5wRb38QuoERERNR2NfreLkFBQdBqtUhPT3e4XZyHVS/tbM/d3V2+ERERkWto9MqHXq/HgAEDsHLlStttFotFnsfHxzf2lyMiIqJWpkl2tRXTbKdMmYKBAwdi8ODBmD9/PoqLi+XsFyIiInJtTRI+brrpJmRmZuKpp56STaZ9+/bFsmXLajWh1kVVq7YU56wXIiKi1sP6um19HW9Vu9qePHmSM16IiIhaKTFxpH379q0rfIj+kJSUFHh7e0NRFLSGpCemB4sn+2y7+BGf69aCP9d8rtsa/kw3PREnCgsLERERIdf3avZhlwshLvhsickZieDB8MHnuq3hzzWf67aGP9NNS6zX1SrW+SAiIiLXwvBBREREzYrh4wKJBdLE7r1cKK3p8bluPnyu+Vy3NfyZdi5O13BKREREbRsrH0RERNSsGD6IiIioWTF8EBERUbNi+CAiIqJmxfDRRMrKyuSeNmKV1u3btzfVl3FJx48fx7Rp0xAbGwuj0YiOHTvKGUfl5eUtfWltwptvvomYmBgYDAYMGTIEmzZtaulLanPmzp2LQYMGyZWcQ0JCcO211+LAgQMtfVku4YUXXpC/l6dPn97Sl+LSGD6ayKOPPiqXmKXGt3//frkM/zvvvIM9e/Zg3rx5ePvtt/H444/z6b5AX375pdyVWoS5rVu3ok+fPhgzZgwyMjL43Dai1atX495778WGDRvw+++/o6KiAldccYXc/ZuazubNm+Xvjd69e/Npbmliqi01rl9++UXt2rWrumfPHjGNWd22bRuf4ib24osvqrGxsXyeL9DgwYPVe++913ZuNpvViIgIde7cuXxum1BGRob8XbF69Wo+z02ksLBQjYuLU3///Xd15MiR6oMPPsjnugWx8tHI0tPTceedd+LTTz+Fh4dHYz881SM/Px8BAQF8fi6AGLZKTEzE6NGjHfZaEucJCQl8bpv451fgz3DTEZWmq666yuHnm1qO020s15qJ9dpuv/12/OMf/8DAgQNlbwI1vcOHD+P111/HSy+9xKf7AmRlZcFsNiM0NNThdnEuhrqoaYghRNF/MHz4cPTs2ZNPcxP44osv5DCiGHYh58DKxzl47LHHZIPSmd7EL2fxAii2E541a1bT/8u58PNs79SpUxg7dixuuOEGWXEiao1/ke/evVu+QFLjS05OxoMPPojPPvtMNlGTc+Dy6ucgMzMT2dnZZ7xPhw4dcOONN2Lp0qXyRdJK/CWp1WoxefJkfPzxxxf+L9aGnevzrNfr5fspKSm45JJLMHToUHz00UdyiIAubNhFDBV+8803cvaF1ZQpU5CXl4cffviBT28ju+++++TzumbNGjl7ixrf999/j+uuu07+Hrb/vSx+T4vfGWJmov3HqHkwfDSipKQkFBQU2M7Fi6OYKSB+mYspi+3bt2/ML+fSRMVj1KhRGDBgABYtWsRfHo1E/JwOHjxYVvGsQwJRUVHyRVJUpqjxhmjvv/9+fPfdd1i1ahXi4uL41DYRUY0+ceKEw21Tp05F165d8a9//YtDXS2EPR+NSPyStufl5SWPYh0KBo/GDR6i4hEdHS37PETFxCosLKwRv5LrEdNsRaVD9CyJEDJ//nw5/VP8sqbGHWpZvHixrHqItT7S0tLk7b6+vnLtGmo84vmt2Uvj6emJwMBABo8WxPBBrY5YF0E0mYq3mqGOmzRfmJtuukmGuaeeekq+IIqF8pYtW1arCZUuzIIFC+RRhGh7H374oWxaJ2rrOOxCREREzYodekRERNSsGD6IiIioWTF8EBERUbNi+CAiIqJmxfBBREREzYrhg4iIiJoVwwcRERE1K4YPIiIialYMH0RERNSsGD6IiIioWTF8EBERUbNi+CAiIiI0p/8HDnMva/sv/S0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.2556896209716797\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, pred, '.')\n",
    "plt.plot(x, y, '.')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\", ((pred - y) ** 2).mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88e67b",
   "metadata": {},
   "source": [
    "선형 vs 2차 비교\n",
    "\n",
    "            선형 y=3x+4      |      2차 y=2x^2+3x+5\n",
    "kernel        [x, 1]                  [x^2, x, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
